[
["index.html", "R for Psych Preface &amp; Overview", " R for Psych Glenn Williams 2018-03-27 Preface &amp; Overview In this course you’ll learn how to use R for data analysis and presentation. This course has a particular focus on using R for psychology (hence the name), but it should be applicable to most cases in the social sciences. Here, we’ll primarily take a tidyverse first approach to R. This means we’ll be relying primarily on a collection of packages designed to get things done quickly, with highly readable syntax. We will still cover some of the base R functions and the basics of programming, but our aim is to quickly and efficiently use R for managing and processing data. Along the way, we’ll look into how R encourages open and reproducible science, and how R can be useful for managing your research projects as a whole. By the time you’re finished, you will be able to tell R how (and where) to read input files (e.g. raw data from an experiment), how to perform operations on your data (e.g. data wrangling and aggregation), and how to produce and save ouputs based on your data (e.g. graphs and test statistics). You’ll also be able to produce documents that incorporate your R code with formatted text so every time you update your code, your written statistics, tables, and graphs update automatically. For this, we’ll explore R-markdown, specifically using R notebooks. We will rely on the R for Data Science (R4DS) textbook by Garrett Grolemund and Hadley Wickham as a core text. Follow the link for a free online version of the book uploaded by the authors. R4DS by Wickham &amp; Grolemund The above book assumes some familiarity with programming and/or R at the outset, but covers the basics in Chapter 4. As in this course, the aim here is to get you doing productive things (e.g. producing a graph) as quickly as possible. If, however, you feel like you’d prefer a basic grounding in R prior to doing so, you can check out R for Beginners by Emmanuel Paradis. This is a short introduction to all of the core concepts required for working with your data in R (including reading, manipulating, and saving data). As this course focuses on using R for psychologists, we’ll cover a range of traditional parameteric and non-parametric analyses, such as: Correlations t-tests ANOVA We will also cover topics such as power analysis, particularly using simulation based methods (which are scalable for any set of tests), before we move on to more advanced methods, such as hierarchical mixed effects modelling (or linear mixed effects models; LMM). Throughout, we will use examples and assignments to entrench the concepts taught in this module. Finally, we will focus on creating reproducible analyses and write-ups using R markdown. To join the rest of your class in discussing this course, please join the R @ Abertay Slack channel. We will use this channel for all communications about the course, including help, hints, and tips about the course content. To download the course content, get the lesson materials from my GitHub repo. Download all of the content from this course using the Clone or Download button, save the files as a zip, unzip them, and go to the lesson_materials folder. You can follow along with the slides using the R notebooks. "],
["introduction.html", "Chapter 1 Introduction 1.1 Installation and Setup 1.2 Working Directory and File Paths 1.3 Packages 1.4 Objects and Functions 1.5 Creating and Accessing Data 1.6 Good Practice 1.7 Exercises", " Chapter 1 Introduction To begin with, we’ll focus on getting you started in R. We’ll look into installing our two key programs, R and RStudio. After that, we’ll look into how R can communicate with files on your system. Finally, we’ll look at the core packages for this course, how to do basic data manipulation using base R (R Core Team 2017), and tips for good practice when working with R. We’ll complete some exercises at the end so you can get a handle of the key concepts introduced here. 1.1 Installation and Setup To get started, at the very least you’ll have to download R from CRAN. Select the correct distribution for your operating system and then click through to install R for the first time. For Windows users, you’ll see a new page; at the top click on “Download R [version number] for Windows.” Click on that and follow the prompts in the installer. If you have a 64 bit system, install the 64 bit version of R as you’ll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you’re working with very large data sets.) In R, you don’t want to type commands directly to the console. Instead, you should use a text editor to write your script and send it to the console as and when you want. This way, you’ll have a document of what you did, and any minor changes you have to make are very easy to implement (vs. writing it all out from scratch again). One of the most popular ways to work on your R scripts is to use an Integrated Development Environment (IDE). The most popular IDE for R is RStudio, which you can download from the RStudio website. In the navigation panel, select products and choose RStudio. Scroll down until you see Download RStudio Desktop. Finally, for the Free tier, click the Download button and select the correct installer for your operating system. For Windows users, you should select RStudio [version number] - Windows Vista/7/8/10. Follow the prompts on the installer, and your system should automatically pick up that you already have R on your system. From there, be sure to open the RStudio program to get started. This is the logo that looks like this: The RStudio logo Once open, you’ll see something that should look like this. Only, you won’t have the top pane unless you choose to start a new script (File, New File, R script) or if you’ve opened an existing script already. The RStudio environment, with added highlights RStudio can be split into 4 main panes: The editor: Type, edit, and save scripts as you would in any text editor. The console: Execute scripts by typing and pressing enter. The environment and history: View objects (e.g. values, variables, and user-defined functions etc.) stored in memory for this working session. You can also see a history of your commands in the History tab. The viewer: view any files in your working directory, see your last plot from this session, view installed packages on your machine (you can also load them here), view the help documentation for any R commands, in the viewer you can view (surprise, surprise) markdown/other documents. All of these panes are very useful when developing your R scripts, but RStudio has some other features such as syntax highlighting, code completion, and an easy interface for compiling your reproducible R-markdown notebooks. For advanced users, you can also set up projects that play nicely with Github for version control of your work. Finally, RStudio defaults to giving you the option to save your workspace (e.g. all of the data you’ve created) when you close the program and to restore this workspace once you restart RStudio. I’d advise you to change RStudio’s defaults to never do this as this could cause unforseen errors when, for example, you want to change parts of script and you accidentally delete the stage to create a variable that is crucial later on. This deletion may not be obvious if you reload your workspace, but you won’t have a record of how you created said variable. To make these changes, go to Tools, Global Options then deselect Restore .RData into workspace at startup and from the dropdown menu on Save workspace to .RData on exit: to Never. 1.2 Working Directory and File Paths When you create an R file (File, New File, R Script), where that file sits is its working directory. This is where R interprets any commands that go outside of your R session. For example, if you want to read data into R, or output a graph, R will do so in respect to the working directory. You can get your working directory like so: getwd() You should see something along the lines of “C:/Users/YOUR_NAME/FOLDER/RFILE.R” Now, you can set your working directory to any folder on your computer with an absolute file path. Often, people use absolute file paths in order read inputs and send outputs to different folders on their computer, telling R exactly where to look. However, I recommend against this. That’s because if you change computers or pass your code over to someone else R will kick out an error saying it can’t find that folder; other computers are highly unlikely to have the same folder structure or even username as you, and fixing this can be frustrating. Also, it can be a pain having to type out your full file path, so let’s avoid that. Below, I’ve outlined one method for working that will allow you to use relative filepaths (relatively) easily. If you want to keep your folder from getting cluttered, keep your R script(s) in a folder, with an Inputs and Outputs folder at that same level, like so: Potential Folder Structure for Your Projects This way, when you want to read in raw data, or output a graph (or something else) you can use a relative file path rather than an absolute file path to define where your files are taken from/should go. This saves you a lot of typing of file paths, and it means your R scripts will work on someone else’s computer if you just send the entire folder containing the R script, Inputs, and Outputs folders. To read in or save data using a relative file path, do this: # reads a CSV file from the Inputs folder my_data &lt;- read.csv(&quot;Inputs/my_data.csv&quot;) # writes a CSV file to the Outputs folder my_data &lt;- write.csv(my_data, &quot;Outputs/my_data_again.csv&quot;) You just have to type in the folder name and a slash (to indicate to go below that folder) and the name of your data. This saves you from all of the hassle of using an abosolute file path described above. 1.3 Packages Next, while you can do a lot in base R, if you can think of some task that is reasonably laborious there’s probably a package out there that can help you to achieve a certain goal. For us, the whole data processing, analysis, and presentation workflow can be made so much simpler by using a set of packages from the tidyverse library (Wickham 2017). This package is required for what we’re about to do next, so install tidyverse (using install.packages(&quot;tidyverse&quot;)). Once installed, you needn’t do it again. But on each new session you have to define which packages you want to load into R using the library() command. Make sure to uncomment the install.packages(&quot;tidyverse&quot;) line if you haven’t installed this package yet. # install.packages(&quot;tidyverse&quot;) # do this only once to install the package library(tidyverse) # do this to load your package every time you open R ## -- Attaching packages ---------------------------------- tidyverse 1.2.1 -- ## v ggplot2 2.2.1 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.2.0 ## v readr 1.1.1 v forcats 0.2.0 ## -- Conflicts ------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() By default, R installs packages from CRAN, which is essentially a centralised network for all things R. However, some useful packages may not have been submitted here, and you can install them from other places, like GitHub. We won’t install packages from github or other repositories in this course. But it’s worth knowing that you can install packages from just about anywhere! Most of the time, you won’t have any trouble using functions from a loaded package. However, there can be cases when you have two packages installed that use the same function name. To tell R exactly which version of a function to use, we can specify both the package and function name in the form package::function_name(). For example, we can use the group_by() function from the package dplyr by typing dplyr::group_by(). You won’t come across this in this course, as we’ll be using packages that have functions with unique names, but it’s worth bearing in mind if you come across problems with functions you know should work in the future. Note: You may notice that in my code chunks I have some green text following a #. These are comments in R and are not read when you execute your code. Comments are very useful for telling you what each section of your code does, and how. I tend to comment quite heavily, as there’s always a chance you’ll forget what you’ve done when you go back to it after a few months away from the code. 1.4 Objects and Functions You can use R like a calculator, e.g. 1 + 4 ## [1] 5 You can execute this directly in the console, or run it from your R script by selecting that line of code (or highlighting several lines) and pressing Ctrl + Enter (for mac, this is probably Cmd + Enter). While I said you shouldn’t use the console when writing scripts, you can use it to test out a bit of code, or to quickly use R as a calculator when you’re in a meeting and feeling like mental arithmetic is a step too far. You can see that once the code is run, the R console returns the result back to you. You see a history of what you asked, and what was returned. Top tip: use the up arrow key from the console and R will automatically fill in the last line/block of code you ran. Press up again to cycle back to older inputs, and down to back to the most recent ones. R always waits for you to finish an expression before it runs your code. So, if you ended your line with a +, it’ll wait for the next number. This is useful for complex expressions that can take up multiple lines, e.g. 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 ## [1] 45 Watch the console when you type this out. You’ll notice that if you press enter after typing 7 + on the new line you will no longer see &gt; but you’ll see +. The same happens even if you pass a different mathematical operator (e.g. -, %, ^). This is there to tell you that R is not waiting for a new statement, but is waiting for you to finish off the current statement. If you see &gt; it means that whatever you can start a new statement. R also parses text if included in quotes. The same rule applies about finishing expressions here; if you don’t close your quote, then R will wait for you to do so. This means you can spread your text over several lines (by pressing Enter) and R will parse that as one expression. Note with our output we get which indicates that a new line follows the comma. &quot;Hello, world!&quot; ## [1] &quot;Hello, world!&quot; &quot;Hello, world (except you Donald Trump, nobody want&#39;s to talk to you)&quot; ## [1] &quot;Hello, world (except you Donald Trump,\\n nobody want&#39;s to talk to you)&quot; Crucially, you can store this sort of information in a variable. Variables are useful because you may want to use the results of a calculation and do some further operations on that result. This is especially useful if you’re not sure what that first result could be. We assign values to a variable using the assignment operator &lt;- (read this as create from). summed_numbers &lt;- 5 + 8 We can then output this variable later on, like so: (This is often useful for checking that your variables store what you think they store.) summed_numbers ## [1] 13 Or we can perform operations on the variable. summed_numbers*5 ## [1] 65 Note: You cannot start variables with a number, you cannot use special characters (e.g. %!*) and you cannot include spaces in your variable name. Also, note that your capitalisation matters, so Summed_numbers is not summed_numbers. (I often get errors due to this issue…) R has the simple arithmetic operations you’d expect from any program. For example, you can: add x + y, subtract x - y, multiply x * y divide x/y, exponentiate x^y find the modulus x %% y, (e.g. 5 mod 2 = 1; i.e. the remainder from how many times 2 goes into 5) and conduct integer division x %/% y (e.g. 5 int div 2 = 2) R also has some logical operations built in. For example: less than x &lt; y less than or equal to x &lt;= y greater than x &gt; y greater than or equal to x &gt;= y exactly equal to x == y not equal to x != y not x !x x OR y x | y x AND y x &amp; y test if X is true isTRUE(x) These come in pretty handy for performing most operations on our data. If you’re unfamiliar with these, don’t worry. We’ll cover how you might use some of these in a staggered format as you progress through this course. Nicely, R also has a number of functions built in. This means you don’t need to write your own function if you want to sum a sequence of numbers. I’m sure I couldn’t provide an exhaustive list here, but as stated above we’ll cover most of the common functions as you get to grips with some data. Still, lets get an idea of how these functions work. Below, we will sum the numbers 1 to 4. This function is built into R already, so we don’t have to write a whole lot of code for this. sum(1, 2, 3, 4) ## [1] 10 What if we want to calculate the mean score from this set of numbers? That’s also built into R. mean(1, 2, 3, 4) ## [1] 1 Notice that whenever we want to run a function, these functions always have a name and are followed by parentheses (e.g. mean(), sum()). What goes in the parentheses? The argument you want to pass to the function. These can have default values, or not (requiring you to specify the argument). Above, we passed the values 1 through 4 as the arguments to the mean() function. Later, we’ll look at functions that ask for arguments from separate data types (e.g. numbers and characters). If you’re unsure what an argument does, you can always ask R what it does, how it does it, and what to pass to it by using ?, e.g. ?mean(). This will bring up a document in the Help window of RStudio. Typing out all of these numbers each time we want to perform some operation is a little tedious. To fix this, we can use what we learned above and store these numbers into a variable. In order to save these into their own variable, we use the c function. Think of this as concatenation. When we combine values into a variable, this variable is stored in our global environment. This means that we can perform operations on the variable later on, without the worry of typing our the values again. This is particularly useful if you want to store values from one function (say a statistical test) that you cannot pre-define but that you want to use later on. Let’s give that a go. First, we’ll concatenate our values into a variable using the c() function described above. Here, we simply define that we want to concatenate some values, and list each value separated by a comma. stored_values &lt;- c(1, 2, 3, 4) If we then call a function that works with a set of numbers, it should also work if we call that function on the variable storing those numbers. Let’s see how this works with the mean() function. mean(stored_values) ## [1] 2.5 Great! That’ll save is a lot of time writing and rewriting code later on. This also allows our code to be flexible, in that we can write a script that performs operations on variables that can take any range of values. This, to me, is one of the nicest things about doing your analyses in R. While you may spend more time getting your script up and running in the first place when compared to using point-and-click methods (e.g. in SPSS), if you gain new data or run a new experiment, it’s likely that your script can simply be re-run with no (or few) changes at very little cost to your time. Now, this part is pretty important but may only be obvious if you’ve programmed in other languages. R is a vectorised language, which means that, as with the sum() function above, R can perform operations on the entire variable So, if you want to increment all values in your variable by 1, you can simply tell R to do so in one line of code, without the need for loops or other complex methods. stored_values + 1 ## [1] 2 3 4 5 1.5 Creating and Accessing Data So, you’ve made it through all of the boring stuff. Now we can look at manipulating some data in R. Let’s pretend we’ve administered IQ tests to 100 participants. For some reason, we’re interested in whether dog and cat owners have different IQs. I’m sure some of you can come up with some incredibly biased hypotheses here, but we’ll leave that out for now. We’ll create this data in R, then take a look at it using some of the packages from the tidyverse library that you installed and loaded above. If the functions below don’t work, be sure to load the tidyverse library (using library(tidyverse)) before running the code below. First, we’ll create a variable containing the numbers 1 to 100, using the seq() function; this can be our participant ID variable. Next, we’ll use the sample() function to sample randomly (and with replacement) from the two labels “cat” and “dog” which acts as our factor for pet ownership. (Note that here I use the sample() function on the concatenated values of “cat” and “dog”, but this would work equally well with a variable containing these labels.) Finally, we’ll use the rnorm() function to generate some random numbers (sampled from the normal distribution) with a mean of 150, and a standard deviation of 15 to act as our IQ scores. Remember how I said we’d look at functions that take several arguments? Well, thats what all of these functions below do. We define each parameter of the argument within the function call. So, if we want to generate a sequence of numbers from 1 to 100 , in increments of 1s (e.g. seq(from = 1, to = 100, by = 1)), then we define each argument with its name (e.g. from/to/by) and tell R which values to set for these arguments using =. # create participant IDs participant &lt;- seq(from = 1, to = 100, by = 1) # create pet ownership codes set.seed(88) pet &lt;- sample(c(&quot;cat&quot;, &quot;dog&quot;), 100, replace = TRUE) # create IQ scores set.seed(88) IQ_score &lt;- rnorm(n = 100, mean = 150, sd = 15) IQ_score &lt;- round(IQ_score) # round IQ scores Note: I’ve overspecified the functions above. You can simply run seq(1: 100) and rnorm(100, 150, 15) to get the same results, but it’s a little less readable. Here, and throughout, I’ll use the most readable version so you have the best idea of what you’re doing. Because we’re using random sampling, I’ve also set a seed, so that you’ll sample the same values as me. If you want people to be able to reproduce your random sampling so they get the same data, set a seed! Finally, it’s important to note that you can call a function on the result of another function in R. Right now, each number in the participant variable is unique. We can see how many participants are in the study by asking R “how long is the variable, participant?”. We do this like so: length(participant) ## [1] 100 Cool, we have 100 participants! But what if we have several observations for one participant? I’ll define that (very simply) in the code below: participant[2] &lt;- 1 # assign the second value of participant the integer 1 head(participant) # return the first 6 values of the participant variable ## [1] 1 1 3 4 5 6 Now, using length(participant) will return 100, because there’s still 100 values in the participant variable. How do we find how many unique numbers are in the participant variable? We simply nest the unique() function within the length() function: length(unique(participant)) ## [1] 99 This asks R how many unique values are in the variable participant. That’s 99! 1.5.0.1 Data Frames and Tibbles Now, in the real world, if you tested IQs you’d typically have this data stored in a table somewhere prior to reading it into R. So lets pair the data together into a table in R. The most useful way to do this is to create a data.frame. But, since we’ve already loaded the tidyverse, we may as well use the built in tibbles. These are like data frames, but they have some different defaults that make working in the tidyverse easier. IQ_data &lt;- tibble::tibble( participant_number = participant, pet_id = pet, IQ = IQ_score ) What’s the main advantage of using a tibble over a data.frame here? Tibbles don’t automatically convert certain data types. With data.frame, character data is automatically converted to a factor. This can be useful, but not always. The thing I like most is that tibbles show the data type under the column name. This is taken from str() which tells you the structure of your data. Also, tibbles default to printing the first 10 rows of data when you type their name in the console. This will stop you from getting overwhelmed by a massive printout if you have a lot of data. Let’s see all of this in action. IQ_data ## # A tibble: 100 x 3 ## participant_number pet_id IQ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.00 cat 147 ## 2 1.00 cat 160 ## 3 3.00 dog 185 ## 4 4.00 cat 122 ## 5 5.00 dog 157 ## 6 6.00 dog 152 ## 7 7.00 cat 152 ## 8 8.00 dog 119 ## 9 9.00 dog 131 ## 10 10.0 dog 160 ## # ... with 90 more rows If you want more than 10 rows, or a specific number of columns, tell R that’s what you’d like. Here, we’ve asked for the first 12 rows, and the width of the table to be infinity so that R prints out all columns even if they don’t fit on 1 row in the console. However, with only 3 columns, that isn’t an issue here. print(IQ_data, n = 12, width = Inf) ## # A tibble: 100 x 3 ## participant_number pet_id IQ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.00 cat 147 ## 2 1.00 cat 160 ## 3 3.00 dog 185 ## 4 4.00 cat 122 ## 5 5.00 dog 157 ## 6 6.00 dog 152 ## 7 7.00 cat 152 ## 8 8.00 dog 119 ## 9 9.00 dog 131 ## 10 10.0 dog 160 ## 11 11.0 dog 138 ## 12 12.0 dog 165 ## # ... with 88 more rows Unfortunately, some older functions in R won’t allow you to use a tibble. If this is the case, simply convert your tibble to a data.frame using the as.data.frame() function. Note, we use head() to see the head of our data frame, or the first 6 values. This is necessary here to avoid printing out each row, as we’re not in using a tibble any more. Notice that We’ve assigned the data.frame version of our IQ data to a new variable, rather than overwriting the previous variable. This is good practice when testing your code, as you never know what might break, resulting in data loss. (Although this wansn’t strictly necessary here.) IQ_data_df &lt;- as.data.frame(IQ_data) head(IQ_data_df) ## participant_number pet_id IQ ## 1 1 cat 147 ## 2 1 cat 160 ## 3 3 dog 185 ## 4 4 cat 122 ## 5 5 dog 157 ## 6 6 dog 152 1.5.0.2 Accessing Tibble Information With tibbles and data.frames, you often want to access a specific column in order to perform some calculation. Let’s say we want to calculate the mean IQ score in our data set. Why doesn’t the code below work? mean(IQ_data) ## Warning in mean.default(IQ_data): argument is not numeric or logical: ## returning NA ## [1] NA R gives us a warning, saying that the argument to our function (i.e. the tibble, IQ_data) is not numeric or logical. Therefore, R cannot calculate the mean on something that isn’t a number (or set of numbers). Instead, we have to point R to the correct column within the tibble. This is often done by name using $ , or by name/position in the tibble using [[ and the name/position followed by ]]. First, let’s see that we’re accessing everything correctly here. # by name IQ_data$IQ IQ_data[[&quot;IQ&quot;]] # by position IQ_data[[3]] ## [1] 147 160 185 122 157 152 Tibbles are stricter than data frames, in that they’ll always return a tibble. For example, with a data.frame if you ask for a value in a specific row of a specific column under the format data_frame[row, column], R will return a vector of the value at that position in the data.frame. With tibbles, your single value will be returned as a tibble. # tibble output IQ_data[1, 3] # row 1, col 3 ## # A tibble: 1 x 1 ## IQ ## &lt;dbl&gt; ## 1 147 # data.frame output IQ_data_df[1, 3] ## [1] 147 Now, lets calculate the mean on the correct column in the tibble. Personally, I prefer using $ for cases like this. But, feel free to use any of the above methods. mean(IQ_data$IQ) ## [1] 150.17 We can combine what we learned above about c() to access multiple columns at once: IQ_data[, c(&quot;participant_number&quot;, &quot;IQ&quot;)] ## # A tibble: 100 x 2 ## participant_number IQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 147 ## 2 1.00 160 ## 3 3.00 185 ## 4 4.00 122 ## 5 5.00 157 ## 6 6.00 152 ## 7 7.00 152 ## 8 8.00 119 ## 9 9.00 131 ## 10 10.0 160 ## # ... with 90 more rows Or we can access a range of values by specifying the rows we’d like to access: IQ_data[c(1, 2, 3, 4, 5), c(&quot;participant_number&quot;, &quot;IQ&quot;)] ## # A tibble: 5 x 2 ## participant_number IQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 147 ## 2 1.00 160 ## 3 3.00 185 ## 4 4.00 122 ## 5 5.00 157 or, to save typing, we can just define this from a starting to an ending value for the rows: IQ_data[1:5, c(&quot;participant_number&quot;, &quot;IQ&quot;)] ## # A tibble: 5 x 2 ## participant_number IQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 147 ## 2 1.00 160 ## 3 3.00 185 ## 4 4.00 122 ## 5 5.00 157 In Session 4 we’ll look at more intuitive ways of accessing data from columns and rows in your data frames. However, for now we’ll quickly look at ways to manipulate data using the base R functionality so you have some idea how indexing works in R. 1.5.0.3 Manipulating Tibble Information 1.5.0.3.1 Changing Values We can use these same principles to edit the information within our data. Say, we’d like to change participant number in row 2 back to 2, we just need to do this: # specifying row and column by index IQ_data[2, 1] &lt;- 2 # specifying row by index and column by name IQ_data[2, &quot;participant_number&quot;] &lt;- 2 # specifying index within a column IQ_data$participant_number[2] &lt;- 2 You can see how all of this combined can result in some pretty powerful flexibility in how you access and manipulate your data in R. 1.5.0.3.2 Adding and Removing Columns To add a row to a data frame, we simply need to specify what we want to add and assign it a new name. Let’s say that we want to add a column that indicates the operating system used by each participant. We may have this because we made assumptions that people who use Windows, macOS, or the Linux families of operating systems differ in their IQ. This is a silly example for several reasons, not only because you can use more than one system; but we’ll stick with this for now. Imagine we already have a sample of operating systems to draw from. You don’t need to understand how this works, but briefly I’ve used the inbuilt sample() function to pick from the three names with replacement, skewing the probabilities to select windows most often, followed by mac, then linux. All that matters is that we’re assigning 100 names to a variable. set.seed(1000) # make the sampling procedure the same for us all operating_system &lt;- sample(c(&quot;windows&quot;, &quot;mac&quot;, &quot;linux&quot;), size = 100, replace = TRUE, prob = c(0.5, 0.3, 0.2) ) In the IQ_data set, we can add a new column for the operating systems used by the participants like so: IQ_data$operating_system &lt;- operating_system # add new column head(IQ_data) # view first 6 rows ## # A tibble: 6 x 4 ## participant_number pet_id IQ operating_system ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.00 cat 147 windows ## 2 2.00 cat 160 mac ## 3 3.00 dog 185 windows ## 4 4.00 cat 122 mac ## 5 5.00 dog 157 mac ## 6 6.00 dog 152 windows Note that you can rename the column to anything you like. But, for consistency, I like to keep the same name as the variable which acts as the data source. Finally, we can remove the new column (and any column) by setting the entire column to nothing (NULL), like so: IQ_data$operating_system &lt;- NULL # remove new column head(IQ_data) # view first 6 rows ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.00 cat 147 ## 2 2.00 cat 160 ## 3 3.00 dog 185 ## 4 4.00 cat 122 ## 5 5.00 dog 157 ## 6 6.00 dog 152 Now the data is back to its original format. We’ll look at another way to remove one or several columns from your data frame in Session 4, but for now this a quick way to get things done using base R. 1.5.0.3.3 Adding and Removing Rows What if we want to add a new row to our data? This may be less common than adding a new column for data processing purposes, but it’s good to know anyway. First, we need to know what should go in each cell. Remember that we have to keep the data square, so you can’t have missing values when you add a row. If you don’t have any data, you can just put NA (with no quotations) to keep the data square but to show that you don’t have any value for a given cell. Let’s assume we want to add a new participant, 101, who has a dog but an unknown IQ. IQ_data[101, ] &lt;- c(101, &quot;dog&quot;, NA) # add new row with values tail(IQ_data) # see last 6 rows ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 96 cat 148 ## 2 97 dog 178 ## 3 98 dog 156 ## 4 99 cat 165 ## 5 100 dog 150 ## 6 101 dog &lt;NA&gt; Here, we have to define all our values to be added in parentheses, using the c() function: participant number is 101 pet_id is &quot;dog&quot; IQ is NA (i.e. unknown) We then assign this to our data frame using the assignment operator (&lt;-). We have to tell R where these values should go in our data frame. Because we’re adding a new row, we specify the data frame, with row 101, and all columns (remember, an empty value after the comma = all columns). We can then remove this row again setting our data frame to itself, minus the 101st row: IQ_data &lt;- IQ_data[-101, ] # remove the new row tail(IQ_data) # see last 6 rows ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 95 dog 188 ## 2 96 cat 148 ## 3 97 dog 178 ## 4 98 dog 156 ## 5 99 cat 165 ## 6 100 dog 150 This is a pain as it isn’t consistent with removing the rows. But, in Session 4 we’ll look at removing rows in a consistent way to removing columns. Still, it’s useful to know how this process works. 1.5.0.4 Other Data Types For most purposes, you’ll only need vectors and data frames/tibbles when you want to work with your data. But it’s worth being aware of other data types, such as lists and matrices. With lists, you can define a vector that contains other objects. Think of this as nesting vectors within vectors (very meta). person_quality &lt;- list(glenn = c(&quot;handsome&quot;, &quot;smart&quot;, &quot;modest&quot;), not_glenn = c(&quot;less_handsome&quot;, &quot;less_smart&quot;, &quot;less_modest&quot;) ) We can then access the elements of a list similarly to how we access vectors, but the name associated with the list will also be returned: person_quality[1] ## $glenn ## [1] &quot;handsome&quot; &quot;smart&quot; &quot;modest&quot; If, instead, we just want the values associated with that entry, we can use [[your list name]]: person_quality[[1]] ## [1] &quot;handsome&quot; &quot;smart&quot; &quot;modest&quot; We can edit these values in a similar way to how we did with a data frame. This time, we just need to tell R which vector to access first (here, it’s the first vector, using double brackets so we can access the values stored there, and not the name (as would happen if we just had 1 bracket)), and then specify which location in the vector you’d like to assign a new value using a separate bracket here: person_quality[[1]][4] &lt;- &quot;liar&quot; person_quality[1] ## $glenn ## [1] &quot;handsome&quot; &quot;smart&quot; &quot;modest&quot; &quot;liar&quot; An advantage to using lists over data frames or tibbles is that the data need not be square. That is, you can have uneven lengths for your entries. Notice how glenn and not_glenn have different numbers of elements in the list. With a data frame, this is problematic. Let’s try adding another participant number to our IQ_data tibble. IQ_data[101, 1] &lt;- 101 tail(IQ_data) ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 96 cat 148 ## 2 97 dog 178 ## 3 98 dog 156 ## 4 99 cat 165 ## 5 100 dog 150 ## 6 101 &lt;NA&gt; &lt;NA&gt; Did you notice that R automatically introduced NA values in the final cells for the pet_id and IQ columns? Matrices work very similarly to data frames and tibbles, but they’re even stricter. They can only contain the same data type throughout, so we can’t mix columns containing characters and numbers without converting them all to the same data type (hint: it’s a character!). I’ll show you how to make a matrix, but we won’t linger on that as I haven’t found them all that useful in my own work. matrix_example &lt;- matrix(rep(1: 25), nrow = 5, ncol = 5 ) matrix_example ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 1.6 Good Practice 1.6.1 Data Checking Tips Finally, a few tips on checking your data before you manipulate your data: If you’re unsure what you have in your working directory, use either check your environment (the panel in light blue in the RStudio screenshot above) or type ls() in the console. This will list everything currently in the global environment. If you want to know the data class for some object, use the class() function (e.g. class(IQ_data)). If you want to know the structure (including object classes) for some object, use the str() function (e.g. str(IQ_data). Nicely, str() also tells you how many variables are in the object, and how many observations you have in total. Most importantly with anything in R, if you aren’t sure how a function works, or what it’s done to your data, check how that function works! You can do this with any function by typing ? followed by the function name into the console (e.g. ?str()). 1.6.2 Style I strongly recommend that you choose a style guide and stick to it throughout when you write your R code. This will make it easier to notice any errors in your code, and increases readability for you and others. Consistency is key here. Since we’re using a tidyverse first approach to teaching R in this course, I recommend this one by the creator of the tidyverse: R Style Guide by Hadley Wickham The important things to take home are that: Use sensible variable names: if a column shows, e.g. participant weight, call it participant_weight Use verbs to describe user-defined functions: if you write a function to make all the descriptive statistics you could ever want, call it something like make_descriptives() use a consistent style, like snake_case_here, or even camelCase, but don’t mix_snake_and_camelCase. comment your code with descriptions of why you’ve done something: you can often work out how you did it by following your code, but the why is easily lost! 1.7 Exercises Try out the exercises below, we’ll cover these in the class with the solutions uploaded at the beginning of each session in a separate downloadable link. Try to solve these questions before resorting to the solutions. I’ll be there to help out where necessary. First, I want you to figure out why some code doesn’t work, then we’ll move on to you manipulating data. For the opener, we’ll look at the basics of solving issues with your code. 1.7.1 Question 1 What’s wrong with the following code? # create a sequence from 0 to 40 in intervals of 2 sequence &lt; - seq(from = 0, to = 40, by = 2) 1.7.2 Question 2 Why doesn’t this return anything? # draw 100 times from a uniform distribution between 1 and 10 uniform_distribution &lt;- runif(n = 100, min = 1, max = 10) uniform_distrbiution Now, we’ll work with a data set. But I’d like you to produce it. We’ll break this down into steps for now. 1.7.3 Question 3 Lets pretend we have 100 participants. Create a variable that will store participant IDs. Their IDs can be anything you like, but some sort of logical numbering looks best. Hint: seq() is your friend here! 1.7.4 Question 4 Create a variable that will store their scores on some test. Let’s assume participant scores are drawn from a normal distribution with a mean of 10 and an SD of 0.75. Hint: rnorm() is a wonderful thing. 1.7.5 Question 5 Finally, create some ages for your participants. Here, you’ll have to use a new command called sample(). See if you can figure out how to use this command. If you’re lost, remember to use ? on the function. 1.7.6 Question 6 Create a data frame consisting of your participant IDs, their ages, and test scores. 1.7.7 Question 7 Take a look at the start and the end of your data frame. Do the values look reasonable? (Don’t worry about outliers or missing values etc., we’ll check those in later classes.) 1.7.8 Question 8 Access the row (and all columns) for participant 20. What do the scores look like? Are they the same as the person sitting next to you? Why or why not, might this be the case? 1.7.9 Question 9 Access just the test score for participant 73. 1.7.10 Question 10 Output some simple descriptive statistics from your sample. We want to know: Mean age Mean score SD score (Hint: Use sd()) It may be useful to store these descriptives in another data frame for further use later on. Can you do that? 1.7.11 Question 11 Access all rows and columns where the test score is greater than 11. Hint: define the column as data$column in your subsetting command, and perform a logical operation on this. 1.7.12 Question 12 Access all rows from the participant_number and IQ columns where the test score is greater than 11. Hint: use the c() function to select multiple columns. References "],
["data-visualisation-1.html", "Chapter 2 Data Visualisation 1 2.1 Getting Started 2.2 Exploring Different Geoms 2.3 Exercises", " Chapter 2 Data Visualisation 1 In this session we’ll look at data visualisation using the ggplot2 package (Wickham 2009) from the tidyverse (Wickham 2017). As with most R stats courses, we’re focusing on data visualisation early on as this allows you to get a good grasp of your data and any general patterns within those data prior running any inferential tests. We’ll follow Hadley Wickham’s approach in R for Data Science by getting you working on producing some pretty graphs from the outset to see how ggplot() works. After that, we’ll look at how ggplot can handle several data types. Along the way, we’ll add some customisation to our graphs so you can see the flexibility of this package. 2.1 Getting Started First, we’ll load the packages necessary for this class. Nicely, ggplot2 is part of the tidyverse family, so we don’t need to load this separately to the other packages in our library. library(tidyverse) Sorry to have made you create your own data frames before, but R and it’s packages often come with in-built data sets. We’ll use the starwars data set from dplyr() which loaded with the tidyverse package. Why star wars? It’s convenient, and I’m a big nerd, so indulge me. Because this is built into R, you won’t see it in your Data pane in the Global Environment. That doesn’t matter for us, but rest assured it is there. Let’s get a sense of how this data looks. How about printing the first 10 entries? 2.1.1 The Star Wars Tibble # look at first 10 entries starwars ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Sk~ 172 77.0 blond fair blue 19.0 male ## 2 C-3PO 167 75.0 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32.0 &lt;NA&gt; white, bl~ red 33.0 &lt;NA&gt; ## 4 Darth V~ 202 136 none white yellow 41.9 male ## 5 Leia Or~ 150 49.0 brown light brown 19.0 female ## 6 Owen La~ 178 120 brown, gr~ light blue 52.0 male ## 7 Beru Wh~ 165 75.0 brown light blue 47.0 female ## 8 R5-D4 97 32.0 &lt;NA&gt; white, red red NA &lt;NA&gt; ## 9 Biggs D~ 183 84.0 black light brown 24.0 male ## 10 Obi-Wan~ 182 77.0 auburn, w~ fair blue-gray 57.0 male ## # ... with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; I know that the starwars data set is saved as a tibble. That allowed me to just print its name to see the first 10 entries. But be wary of this with large data sets where you don’t know how it’s stored. You don’t want to flood your console if your data is stored as a data.frame! Let’s plot the mass and height of our characters against each other to see if there’s a trend. 2.1.2 Plotting in ggplot2 ggplot(data = starwars) + geom_point(mapping = aes(x = mass, y = height)) ## Warning: Removed 28 rows containing missing values (geom_point). So, we can see just how easy it is to create a plot of points in ggplot. Well done! There seems to be a positive relationship between mass and height in the starwars data set. We also got a couple of surprises: A warning about 28 rows that contain missing values; A really big outlier. First, we’ll explore how ggplot works, then we’ll look into these surprises. The ggplot() function always needs to take a data set. This data set should hold everything you want to plot. Crucially, ggplot builds up the plots in layers. So making the first call ggplot(data = starwars) tells ggplot where it should look for data, but it doesn’t do much else aside from making a grey background. After this, you need to add some layers to your plot in the form of geometric objects (or geoms, for short). We decided that because we want to look at the link between mass and height, two continuous variables, that adding some points to the plot will be most useful for getting an idea of how the data are related. To do this, we used the geom_point() function. There are other geoms we could add, but for now we’ll focus on points. Crucially, geom functions take as an argument the mapping in your data. That is, how the visuals of the plot are mapped to your data. This mapping is always defined in terms of the aesthetics of your plot aes(), e.g. which variables to map onto the x and y axis, in this case. You can see how this makes ggplot so flexible: Your data argument is flexible, so you can pass different data sets to the same chunk of code by changing out what you pass to the data = argument. Your aesthetics are flexible, so you can pass different columns to your x and y axis Your aesthetics are even more flexible because they can take aesthetics other than just what to plot on the x and y axes Let’s see the flexibility of your aesthetics in action. 2.1.3 Cleaning Before Plotting We saw before that R gave us a warning that we have rows containing missing values. In this instance, this just means that 28 people weren’t plotted because they didn’t have height and/or mass values. I’ll save filtering data properly (and what the %&gt;% (read pipe) symbol does) for another lesson, but we’ll get rid of them for now by running this code, below: filtered_starwars &lt;- starwars %&gt;% drop_na(height, mass) 2.1.4 Changing Your Aesthetics Let’s say we’re interested in showing the relationship between mass and height within genders. How can we do this? One way would be to add colour to our the points on the plot in order to highlight the different genders. ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ) ) We don’t get that warning now that we removed the NAs and passed the filtered data as an argument. That wasn’t really necessary as ggplot can handle that for us, but it stops the horrible warning from popping up! Warnings in R are there to tell you that the output of your code may not contain what you wanted it to contain. In the previous plot, ggplot dropped those with missing heights and masses, even though we didn’t explictly tell it to do so. Here, because we filtered those entried with missing values before plotting, we don’t get a warning. Warnings are different to errors in that your code will still work, but you need to check out whether it did what you wanted it to do. On the other hand, errors indicate you did something wrong and your code will fail to run. Now, we now have colour to see the relationship between mass and height across the genders in the data set. But it’s a little difficult to see this relationship given the outlier. Let’s see what that outlier is, and whether we should remove it from our data. Again, you’ll learn how this filtering works in later lessons. But for now, I just want to show you how it’s useful to understand your data prior to creating a final plot of your data. filtered_starwars %&gt;% filter(mass &gt; 1000) ## # A tibble: 1 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jabba D~ 175 1358 &lt;NA&gt; green-tan,~ orange 600 herma~ ## # ... with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # overwrite data to remove outlier (keep all with mass below 1000) # filtered_starwars &lt;- filtered_starwars %&gt;% filter(mass &lt; 1000) # not run Of course, it’s Jabba the Hutt. We could choose to throw Jabba out of our data by using the code above to overwrite the data (commented out), but for now, we’ll see just how powerful ggplot is without throwing away our data. ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ) ) + coord_cartesian(xlim = c(0, 180)) This time we added a function coord_cartesian() to the end of our ggplot() call. We defined the limits of the x axis to be between 0 and 180. This way, we can get a better look at the trends in our data. Why do we define limits inside this function? Well, we could have also manually defined the scale with scale_x_continuous(limits = c(0, 180)). This may seem more inuitive, but it throws out the data points outside the limits prior to plotting. Why is this a problem? ggplot has some nice functionalities such as drawing lines of best fit for you based on the data in the plot. If you throw data away while plotting, your line of best fit will shift. So, if you decide that you want to change your scale but keep your model fits for all data, use coord_cartesian(). Given this is an outlier, your choice doesn’t matter if you’re just trying to show trends like this, but your choice is important if you want to show any inferential statistics associated with the data. Next up, we’ll look at changing a few components of the points on the plot. ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ), alpha = 0.7, # opacity shape = 17, # triangles size = 4) + # bigger points coord_cartesian(xlim = c(0, 180)) We’ve added 3 variables outside of the aes() mapping. This means that all of the points within the plot are changed in the same way: They all become a little bit transparent through the alpha variable definition; They all take a triangle shape through the shape variable definition (and the number associated) They are all the same size due to the size variable definition. If we put these variables within the aes() mappings and associated them with a variable within the data set, such as gender, then each point would be affected differently depending on which level of the gender factor the individual data points belong to. It’s important to remember that everything within aes() is mapped onto variables with which to display your data. So, the x location, y location, and anything else that you define within aes() can vary by your data set. Everything outside of it will affect all levels of your data. Here, we’ll define colour both within and outside the aes() mapping, causing a clash. Try this plot below to see how clashes are resolved in ggplot: ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ), colour = &quot;red&quot;) + coord_cartesian(xlim = c(0, 180)) The different colours for each level of gender are now gone, along with the legend! Our variable definition for colour outside of the aes() mappings overrides that within aes(). This is because we’ve manually set the aesthetic properties of our plot by defining colour as an argument of the geom function, rather than of the aethetic mapping. R has some built in shapes that we can define within our plots. For shapes, these are divided into three categories: Colour border with hollow fill (0 - 14) Colourless border with colour fill (15 - 18) Colour border with colour fill (21 - 24) Bear in mind that colour and fill are different properties that we can control within our plots. Below, I’ve used hex values to specify the exact colours that I’d like for the colour (around the border) and the fill for our points. You can find a nice hex selector at htmlcolorcodes.com which will allow you to customise your plot colours to your liking. Just change the letters and numbers after the # in the colour call, and you can change the colours to your liking. In the example below I chose my colours based on the diverging colourblind safe selection of colours from Color Brewer. I’d recommend that you use this if you’re going to include colour in any plots for papers/presentations. ggplot(data = filtered_starwars, na.rm = T) + geom_point(mapping = aes(x = mass, y = height, colour = gender ), colour = &quot;#af8dc3&quot;, fill = &quot;#7fbf7b&quot;, shape = 21, size = 8, stroke = 3 ) + coord_cartesian(xlim = c(0, 180)) Try to mess about with the different definitions that we provided above. Change the colour, size, and stroke values to see how these variables work. We’ve used the aesthetics above to define categorical data by colour. But what happens if we use continuous data? ggplot(data = filtered_starwars, na.rm = T) + geom_point(mapping = aes(x = mass, y = height, colour = birth_year ) ) + coord_cartesian(xlim = c(0, 180)) You can see that we get a sliding scale for the hue of the points. Pretty neat, but also quite difficult to get a real idea of where on the scale the points lie. There are many inbuilt plots that you can create with ggplot2 by mapping your data to different geoms. To get an idea of all of different types of geoms, type ??geom. This should give you an index of all of the geom types available to ggplot2 in your Help pane of RStudio. 2.2 Exploring Different Geoms You’ve already learned about geom_point() above, but now we’ll explore the other geom types. For this exploration, we’ll use a different data set with a limited number of groups for ease of visualisation. I’ve simulated some data for this exercise. You can find this data in the inputs folder for this lesson. (If you’re interested, you can also find the script for generating this data in the data_generation folder). Let’s load this data into R. Here, we use the function read_csv rather than the base R read.csv. This is because read_csv is faster, typically does a better job at guessing the data types for your columns of data, and saves the output as a tibble. Note: read_csv will try to guess the types of data in each column for a data set loaded into R, but sometimes it can fail. We’ll cover instances where this fails, and how to remedy it, in the next lesson. rt_data &lt;- read_csv(&quot;inputs/rt_data.csv&quot;) How should we interpret this data? Imagine that we ran a lexical decision task on two groups of participants. In one condition, participants responded to sentences such as “Eric turned down the volume”, and had to indicate whether this sentence made sense by turning a knob. In the match condition, agreement matched the motion indicated in the sentence (e.g. left = yes, it makes sense), and in the mismatch condition agreement did not match the motion indicated in the sentence (e.g. right = yes, it makes sense). Zwaan and Taylor (2006, Experiment 4) hypothesised that when the action and response are matched, resposne times should be quicker than if they do not match. The participant column indicates individual participants, the gender column indicates the gender of our participants, response_condition indicates whether participants took part in the match or mismatch conditions, and reaction_time, our dependent variable, represents the average reaction time in milliseconds for each participant to respond to whether or not the sentences made sense. 2.2.1 Bar Plots Bar plots are one of the most common plots you’ll come across in Psychology. They’re most useful for representing counts of data that are divided into categories. For example, you could use a bar plot to show the number of male and female participants in your study. Using our simulated data set, we can use a bar plot to show the counts of males and females in our data set. ggplot(data = rt_data, mapping = aes(x = gender) ) + geom_bar() You’ll often find that psychologists like to plot continuous data in a bar plot. I’ve done this myself, but we can do better. One alternative would be to use a boxplot, a violin plot, or even a pirate plot which we’ll explore in the next lesson (well, we are plotting with Rrrrrrrrr…). Since we’ve got data which represents mean scores for each participant, this will be suitable for plotting straight away. If, however, you have raw data, remember to aggregate by participants or items prior to plotting. We’ll cover this in later classes. 2.2.2 Box Plots Box plots provide a better way to represent continuous outcomes (such as reaction times) than bar plots as they give you more information about the variance within your data set. 2.2.2.1 How to read a box plot The middle line represents the median The upper white section of the box the upper quartile: 75% of scores fall below this. The lower white section the lower quartile: 25% of scores fall below this. Together the quartiles represent the interquartile range: The middle 50% of scores. The limits of the whiskers (black lines) for the upper and lower parts of the graph represent the smallest and largest observations that are equal to the upper or lower quartiles minus or plus 1.5 times the interquartile range. Effectively, this is most of the rest of the data, apart from outliers. The dots represent outliers (i.e. those values outside of the whiskers) ggplot(data = rt_data, mapping = aes(x = response_condition, y = reaction_time ) ) + geom_boxplot() We can see that the median reaction time is highest for those in the mismatch condition. Notice also that the interquartile range is larger, and so is the upper limit of the whiskers. This all suggests that those in the mismatch group vary more from each other than those in the match condition. Also, notice the outlier in the match group. Someone obviously has a very slow reaction time compared to the rest of the cohort. I wonder why that could be? Perhaps it’s because they misinterpreted the instructions, accidentally putting themselves into the mismatch condition. Or, it could be because I simulated this data and didn’t control for how outliers were distributed. 2.2.3 Violin Plots Additionally, we have violin plots which show you the density of the mean scores. The wider the section of the violin, the more scores around that area. We set trim to FALSE within the violin plot so that we see the full tails of the data. If we set this to TRUE, then the tails are trimmed to the range of the data. It’s also useful to draw quantiles on the violin plot, so we can see the quantiles as with the box plot using draw_quantiles and by specifying where we want these quantiles. Here, we chose the upper and lower 25% and the interquartile range. ggplot(data = rt_data, mapping = aes(x = response_condition, y = reaction_time ) ) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75) ) 2.2.4 Density Plots Now we’ll look at ways to check for the distribution of your continuous variables. This is a good way to get an eye for whether or not your data are skewed and require a transformation on the data, or a non-parametric test when it comes to inferential statistics. Again, we’ll return to what this means in regards to statistics later in this course, but for now it’s a good idea to understand how you can check the distribution of your data prior to computing any statistics. We’ve added the additional argument fill to the aethetic mapping, to specify that we wanted different colours for our groups. We also specified the additional argument alpha in our density plot to specify that we wanted some opacity to the densities. ggplot(data = rt_data, mapping = aes(x = reaction_time, fill = response_condition ) ) + geom_density(alpha = 0.5) # alpha = opacity 2.2.5 Histograms If you have relatively few observations, a histogram may be more appropriate than a density plot. But, we’ll just stick to the same data as before to see how this works. Here, it’s often useful to set the binwidth prior to plotting. ggplot will try to set it to a default if it can, but this may not be what you want. Here, we’ve set the binwidth to 50, so we count how many observations we have for reaction times in 50ms bins, i.e. from 225 to 275ms, from 275 to 325ms etc. We also set the fill of the bars to white, and the colour to black so we get white bars with a black outline. ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;white&quot;, colour = &quot;black&quot; ) 2.3 Exercises If that’s not enough to make you fall asleep, try out the exercises below. We’ll cover these in the class with the solutions uploaded at the beginning of each session in a separate downloadable link. Try to solve these questions before resorting to the solutions. I’ll be there to help out where necessary. In this section, we’ll use some new data from the pre-packaged data sets in R. First off, we’ll load the chickwts data set, which looks at chicken weights by different feed types. data(chickwts) 2.3.1 Main Exercises 2.3.2 Question 1 Take a look at the first 6 rows of the data set. How does the data look? Is this appropriate for plotting? 2.3.3 Question 2 Calculate the overall means for the chick weights. 2.3.4 Question 3 Calculate the overall SD for the chick weights. 2.3.5 Question 4 Create the basis of a ggplot by defining the chickwts data as the data argument. Assign this all to the object chick_plot. 2.3.6 Question 5 Make a box plot of the chick weights by feed. To do this, use your chick_plot object and add the function for creating a boxplot. 2.3.7 Question 6 Add colour to your box plot by the feed type. 2.3.8 Question 7 Create a density distribution of the chick weights by feed type. Set different colours and fills by the feed type. To make all densities visible, set the transparency for all distributions to 0.4. 2.3.9 Question 8 Make a bar plot to show the counts of each feed type. 2.3.10 Question 9 Pick 6 hex colours from the Color Brewer website. Put these together in a variable, bar_colours. Create your bar plot again, but this time make the fill of the bars the same as those stored in bar_colours. Why do we not get a legend when we specify colours this way, but we do if we specify colours as in Question 7? We’ll return to adding colour to bars while keeping the legend labels in the next lesson. 2.3.11 Question 10 Make a histogram showing the overall distribution of the chick weights. Set the bin width to 50, and make the bars have a white fill and black border. 2.3.12 Additional Exercise Make a point plot from a data set of your choosing. Check out the inbuilt data sets in R by typing data() in the console. Customise this plot to your liking. References "],
["data-visualisation-2.html", "Chapter 3 Data Visualisation 2 3.1 Customising Your Plots 3.2 Pirate Plots 3.3 Faceting 3.4 Calculating Statisitcs in ggplot2 3.5 Combining Plots 3.6 Saving Plots 3.7 Exercises", " Chapter 3 Data Visualisation 2 In this section we’ll cover more advanced plotting methods in ggplot2. We’ll look at customising plots, making pirate plots, installing packages from GitHub, faceting, and stitching plots together. To get started, as always we’ll load our packages and saved data from the previous lesson. library(tidyverse) rt_data &lt;- read_csv(&quot;inputs/rt_data.csv&quot;) 3.1 Customising Your Plots Take the desnity plot below, this is functional, but it’s pretty ugly. ggplot(data = rt_data, mapping = aes(x = reaction_time, fill = response_condition ) ) + geom_density(alpha = 0.5) You already know how to change the colours of your aesthetics across all conditions and split by condition, so now we’ll look at other ways to improve the plot. In this version of the plot, we use the same code as before, but we add labels to the axes using labs(), assigning a nicer looking version of our variables to the x and y axes. Additionally, we change the scale of the x-axis using the scale_x_continuous function. To this function, we pass the limits that we want for our axis (between 200 and 600ms), and we identify the breaks, or where we want the ticks along the axis. We pass another function that we learned in the Lesson One, called seq(). This sets up a sequence of numbers for us without us having to type them all out. Here, it goes from 200 to 600 by ticks every 100ms; as a result, our axis gets labels of 200, 300, 400, 500, and 600. On top of this, we also improved the label for our legend using the guides() function. As our legend is only there to identify different parts of the graph with a different colour (from fill = response_condition in aes()), then we have to tell the guide to change the legend that pops up because of the differnt coloured parts of the plot. So, we change our guide, and change the legend that comes up because of the change in colour (guides(fill = guide_legend())) and within the guide legend, we change the title (title = &quot;Response Condition&quot;. I know this sounds like a lot to take in, and you’re very likely to forget how this works (I do all the time), but hopefully you can get a grip of it by seeing it in action. Finally, we’ve changed the theme of our plot to theme_bw(). This is one of many inbuilt themes in ggplot2, but I find it one of the cleanest. ggplot(data = rt_data, mapping = aes(x = reaction_time, fill = response_condition ) ) + geom_density(alpha = 0.5) + labs(x = &quot;Reaction Time (ms)&quot;, y = &quot;Density&quot; ) + scale_x_continuous(limits = c(200, 600), breaks = seq(from = 200, to = 600, by = 100 ) ) + guides(fill = guide_legend(title = &quot;Response Condition&quot;)) + theme_bw() With the histrogram below, we don’t have much new to introduce, expect this time we use theme_classic() instead of theme_bw(). This gets rid of the major and minor grid lines from the previous plot, and also keeps only the lines for the axes. However, we added the argument expand to the scale_y_continuous() function, and pass this the values 0 and 0. This makes removes the spacing between the plotted elements and the axes. These values simply indicate how much extra space should be added to the top and bottom of the plot. ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;#bcbddc&quot;, colour = &quot;#756bb1&quot; ) + scale_x_continuous(limits = c(200, 600), breaks = seq(from = 200, to = 600, by = 25 ) ) + scale_y_continuous(expand = c(0, 0)) + labs(x = &quot;Reaction Time (ms)&quot;, y = &quot;Count&quot;) + theme_classic() 3.2 Pirate Plots Now we’ll look at a new plot type that takes a bit of extra work to generate; pirate plots. Pirate plots are a great choice of plot for the indecisive. They are essentially the individual points of data, a bar plot, a violin plot, and a confidence interval interval all in one. This way, you get to see the raw, descriptive, and inferential data on one plot! This is a nice way to show data that are grouped by categories but with a continuous dependent variable. We could make these ourselves by doing some calculations and combining geoms in ggplot2. Or, we could just install a package from Github which will do that for us. First, we’ll have to install devtools in R which will allow us to install packages from Github. (For packages on CRAN, this isn’t necessary, but unfortunately ggpirate isn’t on CRAN at the time of writing.) To install devtools and the ggpirate package, uncomment and run the code below. Then as always, use library(ggpirate) to load the package. # install.packages(&quot;devtools&quot;) # devtools::install_github(&quot;mikabr/ggpirate&quot;) library(ggpirate) Below, we’ll make a pirate plot. Note that you only need the first two calls (lines 1-6) to create the plot: The first to set up how you’re going to map your data (and the source of your data), and the second to add the geom for the pirate plot itself. We added the aethetics of colour and fill to match our conditions within this call, so the two levels of response_condition have different colours. For the additional lines: labs allows you to manually change the x and y axis labels scale_x_discrete allows you to manipulate your x scale. Within this, we change the labels of the columns using labels. We changed them to Impaired (vs. impaired) and Intact (vs. intact) for stylisitc reasons. scale_y_continuous allows you to manipulate your y scale. Here, we set the limits, i.e. how far to extend our y-axis to between 0 and 600ms. Additionally, we set our breaks to increment in the sequence (seq) from 0 to 600 by 100ms. This way, we’ve added more granularity to our axis ticks. We use theme_bw() to change to a black and white theme. There are other themes in ggplot2, but this one is nice and clean. Next, theme allows you to manually specify other aspects of how your plot should look. Here, we used panel.grid.major.x and set this to nothing (element_blank()) because we don’t need vertical lines in our plot. Finally, we define the colour and fill for our plot using manual hex values using the scale_colour_manual() and scale_fill_manual() functions. ggplot(data = rt_data, mapping = aes(x = response_condition, y = reaction_time) ) + geom_pirate(mapping = aes(colour = response_condition, fill = response_condition) ) + labs(x = &quot;Motor Skill&quot;, y = &quot;Reaction Time (ms)&quot;) + scale_x_discrete(labels = c(&quot;Impaired&quot;, &quot;Intact&quot;)) + scale_y_continuous(limits = c(0, 600), breaks = seq(from = 0, to = 600, by = 100) ) + theme_bw() + theme(panel.grid.major.x = element_blank()) + scale_colour_manual(values = c(&quot;#af8dc3&quot;, &quot;#7fbf7b&quot;)) + scale_fill_manual(values = c(&quot;#af8dc3&quot;, &quot;#7fbf7b&quot;)) In this plot, we have coloured bars and lines indicating the mean scores, a box representing the 95% confidence interval assuming a normal sampling distribution, violins indicating density, and the raw data points. If the 95% confidence interval between the two groups doesn’t overlap, we can be fairly sure there is a significant difference between the groups. So, here we can be fairly certain the two groups differ in reaction times. 3.3 Faceting Another useful part of plotting in ggplot2 is that you can make facets of plots, or subplots. This is a good way to display your data if you have multiple categorical variables. Essentially, you’ll get a plot for each category in your data. 3.3.1 Facet Wrap If you want to create facets from one variable then use facet_wrap(). ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;white&quot;, colour = &quot;black&quot; ) + facet_wrap(~ response_condition) In this plot, we’ve specified a histogram as we normally would. However, we use facet_wrap() and a tilde (~) to create a formula for how to display our plots. We define our variable with which to split our plots to the right of the ~, and ggplot automatically plots the two separately at the same time. Notice that we get useful labels at the top of these plots, too. 3.3.2 Facet Grid If we wanted to make a facet by two variables, then we would use facet_grid() instead. In this case, we just add each variable to either side of the ~ and ggplot will do the splitting for us. Let’s see how this works if we split our data by gender and response condition. ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;white&quot;, colour = &quot;black&quot; ) + facet_grid(gender ~ response_condition) The order in which you specify the two variables matters. Try swapping around between facet_wrap(gender ~ response_condition) and facet_wrap(response_condition ~ gender) to see how this works. 3.4 Calculating Statisitcs in ggplot2 Sometimes, plotting just the means isn’t enough. We saw how useful the 95% confidence interval from ggpirate is for making inferences about the differences between groups. Nicely, we can get standard errors or confidence intervals around our data points within ggplot for other geoms. 3.4.1 Means and Error Bars Let’s say you wanted a bar plot with error bars showing the standard error. You can create this in ggplot using the stat_summary() function. In the first instance here, we tell it that we want to run the function mean over our data that make up the y-axis; hence fun.y = mean. We also need to specify which geom we want to return from this. Try changing the geom to point in the first stat_summary() call to see what happens when you run this plot with geom = &quot;point&quot;. Finally, we ask for another summary, but this time we want an error bar. So, for the geom call we request an error bar. Crucially, the function we require to get this errorbar is fun.data = &quot;mean_se&quot;. That’s because we need the mean to know where the centre the errorbar, and the standard error to get the limits of the bar. We manually changed the width of the bar to a quarter of the bar size using the width argument to stop ggplot returning super wide error bars. ggplot(data = rt_data, mapping = aes( x = response_condition, y = reaction_time, fill = response_condition ) ) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = 0.25) I don’t often use stat_summary in my own plots, as I often want to know exactly how I’ve calculated things like the standard error. Doing things by hand allows you to change error bar sizes appropriately for within- and between-subject designs. But, this is useful if you want to create a plot rapidly, or want to avoid the hassle of writing extra code. 3.4.2 Model Fits Here’ we’ll switch again to a different data set that has two continuous variables. The starwars data set is useful for this exercise. We can use the geom_smooth() function to fit a model to our data. This defaults to a loess method, but we can change this to a linear model (or other alternatives) as in the second plot below. By default, these smoothed plots display a ribbon around the fit which indicates the confidence interval (95% by default). # remove NA and mass above 1000 filtered_starwars &lt;- starwars %&gt;% drop_na(height, mass) %&gt;% filter(mass &lt; 1000) # plot ggplot(data = filtered_starwars, mapping = aes(x = mass, y = height) ) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; Next, we’ll change the defaults in order to fit a linear model. We do this in the geom_smooth function, method = &quot;lm&quot;. We can additionally specify a function, so we can change how we fit the data. Currently, our formula is y ~ x, which is a regular linear model. We could, however, fit a quadratic function to the data by using y ~ poly(x, 2). The function poly(x, 2) calculates orthogonal polynomials to a certain degree. The first being linear, second quadratic (think curved lines with 1 inflection point), the third cubic (think curved lines with 2 inflection points), etc.. For now, we’ll just fit the linear model. ggplot(data = filtered_starwars, mapping = aes(x = mass, y = height) ) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = y ~ x) Alternatively, if we have a fitted model saved from our analyses, we can add the fitted line directly using the stat_summary function. However, this requires some insight into the models fitted, so we’ll save this for later classes. Finally, we’ll look at how we can combine these smoothed fits with averages of scores. This is often useful if you’re looking at timecourse data and you want to summarise performance across a whole range of participants. We’ll also look at adding a few of the graphical flourishes from above to make the plot look really nice. Next, we’ll look at making the point and line points look a little better. For this, we’ll use the inbuilt ChickWeight data set from R. This is similar to the chickwts data set from the previous session, but crucially this data is not aggregaged across time. Instead, we have several measurements for the chicks at different time points. This makes this data especially nice for plotting changes over time in our sample. First, load the data. data(ChickWeight) Then we can see how the data looks. This data is stored as a data.frame, so we have to be careful when printing it out. We’ll use the head() function to get only the first 6 rows of data. head(ChickWeight) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 We have 4 columns: weight: out dependent variable, the weight of the chicks Time: an independent variable indicating the time point at which a measurement was taken Chick: an ID column for each chick Diet: an independent variable indicating the diet of the chick Unfortuantely, we only know that there are different diets from the data set, but not what those diets are. We also don’t know the units by which weight is measured or by which the measurements were taken. This is a good lesson in using sensible variable names. If you want yourself and others to understand your data years down the line with little background knowledge of your study, then use informative labels for your data! We’d expect that as time goes on, chick weights should increase. Also, we might expect that the rate of growth should differ for each diet. As such, we’ll plot this together using our data set. First, we’ll plot our chick data, using the stat_summary() function to plot some mean points for the chick weights, and the geom_smooth() function to fit a linear model to the data. We’ll make sure that the points, lines, and ribbons (indicating the confidence interval) are different colours for each level of the diet factor. We’re going to build up our plot bit by bit, so this time we can same our plot as an object, before returning the object to see the plot. chick_time_plot &lt;- ggplot(data = ChickWeight, # data mapping = aes(x = Time, # x-axis y = weight, # y-axis colour = Diet, # colour ID fill = Diet # fill ID ) ) + stat_summary(fun.y = mean, geom = &quot;point&quot;) + # point means geom_smooth(method = &#39;lm&#39;, formula = y ~ x) # linear fit # return the plot chick_time_plot We can improve this plot. So, let’s add some further information. What if we had a target weight we’d like our chickens to reach over time, and we want to see when each diet pushes chicks over this line? We can add this information by including a horizontal line using geom_hline(). We just need to specify where on the y-axis this line should go (yintercept), and which style we want for the line (linetype). We should also add an annotation to the line indicating what it represents. We can do this by using the annotate function, and specifying that we want a text geom. We need to specify the x and y values for where the text lies, and what the text should say. To add this to our original plot, we’ll overwrite the original plot name with itself, plus our additional arguments. chick_time_plot &lt;- chick_time_plot + geom_hline(yintercept = 150, linetype = 2) + # horizontal line annotate(geom = &quot;text&quot;, x = 2, y = 158, label = &quot;Target Weight&quot;) # line label # return the plot chick_time_plot Finally, we can improve the general look of the plot by adding some additional arguments. We’ll specify the labels for the x and y axes using the labs() function. We’ll also specify the limits of the y-axis using the coord_cartesian() function and specifying the minimum and maximum values for the limits of the y-axis. Next, we’ll specify a general basic theme for the plot using the theme_bw() argument. We’ll change a minor detail of this theme, namely the position and style of the legend by specifying additional theme() arguments. This has to come after theme_bw() so theme_bw() doesn’t overwrite our additional arguments. First, we simply state the x and y locations for the legend by defining the legend.position. Next, we want a black border around our legend, so we do this by changing the legend.background. Here, we set colour (the outline) to black, with a relatively small line size (1), and a solid linetype. Finally, we can change the title of our legend if we’d like. The original title is quite informative, but it’s useful to know how to do this. Here, we use the guides function, and specify both the guide legend title using guide_legend() for both the colour and fill properties. It’s important to do this for both properties as our data is identified by colours on both of these properties. If we just change the title of one of the properties (e.g. fill), then we’ll have two legends! chick_time_plot &lt;- chick_time_plot + labs(x = &quot;Trial&quot;, y = &quot;Weight&quot;) + # axis labels coord_cartesian(ylim = c(0, 300)) + # range of y-axis theme_bw() + # general theme theme(legend.position = c(x = 0.15, y = 0.8), # position of legend legend.background = element_rect(color = &quot;black&quot;, size = 1, linetype = &quot;solid&quot; ) # legend styling ) + guides(colour = guide_legend(&quot;Experimental Diet&quot;), fill = guide_legend(&quot;Experimental Diet&quot;) ) # legend title # return the plot chick_time_plot 3.5 Combining Plots Finally, while it’s all well and good plotting one model at a time, often if you’re writing a paper you want several plots together to save space. You could do this by hand in certain image editing packages, but luckily there’s a package for this on CRAN; cowplot. First, we need to install and load the package. Intall this by running the commented out code below. After that, load cowplot each time you want to use it. # install.packages(&quot;cowplot&quot;) library(cowplot) # run this each time In order to stitch two plots together, we need to save our plots as objects so they’re available to the R environment. This is the same process we used above for the ChickWeight plots. If we assign our plotting code to an object, then every time we run the name of the object, we’ll get the output of that code back. For keeping several plots in my environemnt at once, I often save them to an object. In this instance, we’ll save a plot of just the points to the name point_plot. That’s because we want to overlay a linear fit and a quadratic fit on this plot separately, but we don’t want to type the full code out twice. Instead, we fit the plot of points to point_plot, then add a linear or quadratic fit by adding the geom_smooth argument to point_plot and saving both under new names. We can do this like so: # create a plot of points point_plot &lt;- ggplot(data = filtered_starwars, mapping = aes(x = mass, y = height) ) + geom_point() + coord_cartesian(ylim = c(0, 300)) # create a linear plot by adding the fit to the plot of points linear_plot &lt;- point_plot + geom_smooth(method = &#39;lm&#39;, formula = y ~ x) When you do this, you won’t automatically see the plot once you run the code unless you run the object assigned to the plot. Let’s try this for a quadratic fit of the same data. # create a quadratic plot by adding the fit to the plot of points quadratic_plot &lt;- point_plot + geom_smooth(method = &#39;lm&#39;, formula = y ~ poly(x, 2)) # fit quadratic # return the plot quadratic_plot You can see that we’ve got the quadratic fit and the 95% confidence interval around this fit from the above code. Why does the plot look different to the base plots in ggplot2? cowplot loads some defaults for all plots outputted by ggplot to save you on typing out your own theme arguments. Now, we can combine these two plots into a single plot using the new functionalities from cowplot. combined_plots &lt;- plot_grid(linear_plot, quadratic_plot, labels = c(&quot;A&quot;, &quot;B&quot;), # label plots align = &quot;h&quot; # align axes ) combined_plots What if we want a title for our plots? We first have to define our title with a combination of ggdraw() and draw_label(). Inside draw_label() we used a new function, paste() to paste two strings of text together (with a space between the two strings). We could simply input the entire string, but we broke it down into two bits so we don’t exceed the 80 character width limit from our style guide! We further specify a bold fontface in the draw_label() command outside our pasted title. Finally, we display the plot by creating a plot_grid() of our plot and the title in this order so the title displays underneath the plot. We specify that we just want 1 column so the plot and title are stacked together, and we specify the relative heights of the title and plot separately so that the title is smaller than the plot so it doesn’t take up an equal amount of space as the plot. # create title title &lt;- ggdraw() + draw_label(paste(&quot;Linear (A) and Quadratic&quot;, &quot;(B) fits of Height and Weight&quot; ), fontface = &quot;bold&quot; ) # print plot as a grid combined_plots &lt;- plot_grid(combined_plots, title, ncol = 1, rel_heights = c(1, 0.1) ) # return the combined plots combined_plots You can find further information on adding joint titles, annotations, etc. in this article by Claus O. Wilke. 3.6 Saving Plots Finally, if you’ve went to the work of producing a nice plot, then it’ll be useful to know how to save it somewhere. To save our combined plot of the linear and quadratic fits, we’ll use ggsave(). You can name your plot anything you like, but remember to add a file extension at the end. I’ve used .png as this format suffers from fewer artifacts when comapred to JPEG, and it’s a pretty common filetype. ggsave(filename = &quot;outputs/starwars_mass_by_height.png&quot;, plot = combined_plots ) You can do a whole bunch of other things with ggplot, like adding vertical and horizontal lines (often useful for identifying chance performance in participants), and annotating plots directly (useful for adding statistics, or commenting on interesting sections of the data). We’ll cover these in the exercises, however, as these are just additional flavour to our plots! 3.7 Exercises 3.7.1 Main Exercises If all this complexity hasn’t given you nausea, try out the exercises below. We’ll cover these in the class with the solutions uploaded at the beginning of each session in a separate downloadable link. Try to solve these questions before resorting to the solutions. I’ll be there to help out where necessary. 3.7.2 Question 1 Let’s assume we want to know if the density of the mean reaction times differs across word class within our items. Using the lex_dec_items data set, make a density plot of the word frequency by class. Save this plot as lexical_density, and output the plot. 3.7.3 Question 2 Add a black and white theme to your lexical_density plot. Next, give the axis labels and legend labels uppercase names. Finally, give your legend title the name Word Class. Assign all of this to a new object lexical_density_improved, and output your plot below. Note: To change the legend labels, you need to use both scale_fill_discrete and scale_colour_discrete. Why do you think this is? Why don’t we just use scale_x_discrete as we did in class? 3.7.4 Question 3 There’s some repetition in the code for the plot above. Can you improve your code to remove that? 3.7.5 Question 4 Now we want to check the distribution of the reaction times depending on word class and the language spoken by our participants. Using the lex_dec data set, create a faceted plot that looks at the density of reaction times. This should be made up of a grid of densities split by native language and word class. Assign this to the object rt_density and output your plot. 3.7.6 Question 5 Now we want to explore if there’s any relationship between the mean reaction time to our items and the frequency of the item. Using the lex_dec_items data set, plot the relationship between word frequency and mean reaction time as a scatter plot. We want a fitted line and points for the mean reaction time. 3.7.7 Question 6 Now we want to know how many males and females took part in our experiment. Using the lex_demog data set, create a count of the number of males and females who took part in the experiment. Make all text in the plot uppercase, and make the plot theme black and white. Assign this to the object gender_count and output your plot. 3.7.8 Question 7 What if we want to know the mean age and distribution of ages split by language spoken and gender? Using the lex_demog data set, create a pirate plot of the ages by each gender. You can set the colour and fill to gender to have more colourful plot if you’d prefer. Additionally, we would like these plots split by the native language of the speaker, so facet your plot by native language. Assign this all to the object demographic_age, and output your plot. Can you see how a bar plot of the average ages might be misleading in this instance? Pay particular attention to the male bar for the other language group. 3.7.9 Question 8 Next, we want to see the mean (and standard error) of reaction times to words with different word frequencies. Using the lex_dec data set, create a scatter plot of the reaction time by word frequency. We would like this split by word class. Hint: Be sure to use the stat_summary function to get pointranges that represent the mean and standard error. Assign this to the object rt_pointrange, and output your plot. 3.7.10 Question 9 Finally, we want to show a few of our graphs in one display. Using the cowplot library, stitch together the plots for question 6, 7, 4, and 8. Add the labels A-D to identify each plot. Save these plots under the object combined_plots and return your combined plots. 3.7.11 Additional Exercise We can improve the combined plots above. Add a (short) simple title to your combined plots, and save this plot in the outputs folder. These plots won’t look perfect, and you may need to change the font and element sizes. That can be easily achieved, but we won’t do that here. "],
["data-manipulation-1.html", "Chapter 4 Data Manipulation 1 4.1 Getting Started 4.2 Data Formats 4.3 Reformatting Data 4.4 Joins 4.5 Checking for Unique and Duplicate Information 4.6 Exercises", " Chapter 4 Data Manipulation 1 In this chapter we’ll look at tidying up and cleaning our data with the tidyr package (Wickham and Henry 2018) from the tidyverse (Wickham 2017). We’ll also use the dplyr package (Wickham et al. 2017) from the tidyverse (Wickham 2017) to join different sets of data together into a useful format for plotting and running analyses. This data cleaning is an important first step on your way to working with your data, so it’s important for you to get acquainted with the different ways your data can be formatted, and how to get it in the format we want. This chapter, particularly the section on joins, was informed by the Glasgow University Research Cycle by Dale Barr and Lisa DeBruine. Please see this resource for further reading. 4.1 Getting Started First, we’ll load the packages necessary for this class. Nicely, tidyr and dplyr are part of the tidyverse family, so we don’t need to load this separately to the other packages in our library. library(tidyverse) Next, we’ll load some data from the languageR library. The data set we’ll look at is the lexdec data set, which looks at lexical decision latencies for English nouns from native and non-native speakers of English. If you load this data set from the languageR library then it’ll already be in the correct format for us to perform our analyses. So, I’ve made this data more messy; adding missing values, additional columns that represent more than one source of data, and few extra participants. On top of this, I’ve produced a separate data set which stores some (fake) demographic data for the participants. This data set contains information about the participant ID, their gender, age, and any programming languages that they know. Why did I add this last column? Well, sometimes your data contains additional information that isn’t important for your current analyses, so it’s good to get some experience with filtering our data. (We’ll cover filtering data in more detail in Lesson 5.) 4.2 Data Formats I’ve saved the data in both wide and long formats. In wide formats, each row represents one participant, and any information gathered from these participants is stored in a new column. So, let’s say we have several items where we gather some data (e.g. reaction times), here each column will represent an item, and each cell will store a participant’s score. In long formats, each column represents one measurement. Here, we could have a column for participant ID, a column for item number, and a column for reaction times. In this instance, each row should be unique by its combination of our three columns, but IDs (e.g. participant ID, item ID) will be repeated several times. 4.2.1 Loading Data Let’s load the messy data sets from the csv files in the Lesson 4 lesson_materials folder to get a better grasp of these formats. We’ll get some (scary looking, red) messages when we load the data. That’s because we haven’t specified how we want each column to be parsed. demo_wide &lt;- read_csv(&quot;inputs/lexical_decision_demographic_data_wide.csv&quot;) demo_long &lt;- read_csv(&quot;inputs/lexical_decision_demographic_data_long.csv&quot;) When we use read_csv() this function tries to guess the data type contained within any column. Basically, if all the data in a column are numbers, then it’ll be parsed as a numeric data type. If even one cell in a column is text, the whole column will be parsed as text. This can cause problems if most of the data are numbers and you want to do some calculations with this column, as you can’t add, subract, or divide with text columns! Often, dates are parsed as datetimes, which allows for some easy calculations for differences in times. But, our completion_time column was parsed as a character. Why? Because this column contains two datetimes separated by an underscore. It looks like the researchers (me) were too lazy to actually calculate the completion times, and just threw the start and end times togerther into one column! 4.2.2 Wide and Long Data Now we’ve loaded the data, and understand how R reads data, let’s look back at wide and long data formats. 4.2.2.1 Wide Data In the wide data format, each row is a participant (ID). We have columns representing all of the programming languages reported to be known by the participants. If a participant knows the language, they get a 1 in this column, otherwise they have an NA. We also have several other columns covering other information gathered. demo_wide ## # A tibble: 29 x 14 ## ID `C++` FORTRAN JavaScript Python R Ruby LANGUAGE progress ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 1 1 1 NA NA NA OTHER FINISH ## 2 23 1 NA NA NA NA NA &lt;NA&gt; no ## 3 24 NA 1 NA NA NA NA ENGLISH END ## 4 25 1 1 NA NA NA NA &lt;NA&gt; ethics ## 5 26 NA NA NA 1 NA 1 english ethics ## 6 27 NA 1 1 NA NA NA ENGLISH END ## 7 28 NA NA NA NA 1 1 &lt;NA&gt; ethics ## 8 29 NA NA 1 NA NA NA english ethics ## 9 30 NA 1 NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 10 A2 1 1 1 NA NA NA English END ## # ... with 19 more rows, and 5 more variables: gender &lt;chr&gt;, age &lt;int&gt;, ## # tester &lt;chr&gt;, funRec &lt;chr&gt;, completion_time &lt;chr&gt; 4.2.2.2 Long Data With the long data format, we have each ID for our data in one column, and the measurements for these variables in each cell. The main difference here is that we have a column at the end called computer_language which simply lists the language each participant knows. This cuts down on the need for redundant columns for a computer language when a participant doesn’t know that language. Compare the languages known for participant 22 in this data set and how it’s represented in the wide data set. In order to display this properly, I’ll cut out the columns tester, funRec, and completion time, when printing for this website, but you needn’t do this in R. demo_long ## # A tibble: 54 x 6 ## ID LANGUAGE progress gender age computer_language ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 C++ ## 2 22 OTHER FINISH female 30 FORTRAN ## 3 22 OTHER FINISH female 30 JavaScript ## 4 23 &lt;NA&gt; no male 30 C++ ## 5 24 ENGLISH END female 30 FORTRAN ## 6 25 &lt;NA&gt; ethics male 18 C++ ## 7 25 &lt;NA&gt; ethics male 18 FORTRAN ## 8 26 english ethics male 31 Python ## 9 26 english ethics male 31 Ruby ## 10 27 ENGLISH END female 44 FORTRAN ## # ... with 44 more rows We end up with some repetition here (several rows with the same ID, language, gender, etc.), but each row is unique when we consider all measurements. This is a common data format for raw data, as we’ll see next. 4.2.2.3 Understanding our Loaded Data The NA value is important in R; as Hadley Wickham says, it is the evidence of absence. Missing values however are more problematic in that they are the absence of evidence. If you want to indicate missing data, use NA, and not N/A or N_A etc. as you have to tell R to parse these as NAs. We also have columns indicating the language known by the participant, their progress in the experiment (i.e. did they finish it or not?), their gender, age, who tested them and two final columns. The funRec column tells us whether they liked the experiment or not (on a 0-7 scale) and whether they’d recommend the experiment to others (yes/no). Unfortunately, these values are separated by a dash. This is bad practice as each cell should represent one data point, not two. The same can be said for completion time, with the dates and times for starting and ending the experiment separated by an underscore. 4.3 Reformatting Data 4.3.1 Gathering Data Let’s say we want to perform some operations to change how our data looks. What if we want to turn our data from a wide format into long format? We might do this if we want to make a bar plot which counts how many people know each programming language. To gather data that is spread across several columns, we use the gather() function. In this function, we have to specify a few things. As always, with our tidyverse functions we need to tell R which data set on which to perform the function. We have to say what we will call our new column which contains the headings of the columns we want to gather. Here, we call it prog_lang, and it will contain the column names for each programming language. (Normally, I’d use a more readable name, but I want to show as many columns for this data on the website.) Next, we need to specify a value, which will contain the numbers from the programming language columns we’ve gathered together. This will essentially tell us whether or not people know that language or not. Finally, we need to give the function the columns to gather together. We can do this by name, or by number. Since our programming language columns are all together, from column number 2 to 7, we can just specify the range of 2:7. gather(data = demo_wide, key = prog_lang, value = known, 2:7 ) ## # A tibble: 174 x 8 ## ID LANGUAGE progress gender age tester prog_lang known ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 22 OTHER FINISH female 30 GW C++ 1 ## 2 23 &lt;NA&gt; no male 30 GW C++ 1 ## 3 24 ENGLISH END female 30 GW C++ NA ## 4 25 &lt;NA&gt; ethics male 18 GW C++ 1 ## 5 26 english ethics male 31 GW C++ NA ## 6 27 ENGLISH END female 44 GW C++ NA ## 7 28 &lt;NA&gt; ethics male 23 GW C++ NA ## 8 29 english ethics male 34 GW C++ NA ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW C++ NA ## 10 A2 English END female 33 RHB C++ 1 ## # ... with 164 more rows That worked nicely, but it seems that it’s formatted the data so it goes through each programming language alphabetically first, so our IDs are spread all over the data set. To fix this, we can use another function, called arrange, which takes a data argument and a column by which to arrange the data. 4.3.1.1 The Pipe At this point, I’ll introduce you to a new way of writing our commands which is better when we want to apply several functions. Instead of nesting it all together, we can write our commands from left to right, like how we read English text. Here, we can use the pipe %&gt;% at the end of a line, which can be read as “and then do…”. Below, we simply give our data.frame, demo_wide, and use the pipe to apply the gather function. This is the same code as above, just represented in a different way. Read this like, “take our data, and then, gather the columns together.” demo_wide %&gt;% gather( key = prog_lang, value = known, 2:7 ) ## # A tibble: 174 x 8 ## ID LANGUAGE progress gender age tester prog_lang known ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 22 OTHER FINISH female 30 GW C++ 1 ## 2 23 &lt;NA&gt; no male 30 GW C++ 1 ## 3 24 ENGLISH END female 30 GW C++ NA ## 4 25 &lt;NA&gt; ethics male 18 GW C++ 1 ## 5 26 english ethics male 31 GW C++ NA ## 6 27 ENGLISH END female 44 GW C++ NA ## 7 28 &lt;NA&gt; ethics male 23 GW C++ NA ## 8 29 english ethics male 34 GW C++ NA ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW C++ NA ## 10 A2 English END female 33 RHB C++ 1 ## # ... with 164 more rows Still, we’re left with the same grouping problem, so we can apply another function, arrange() at the end to arrange the data by ID. demo_wide %&gt;% gather( key = prog_lang, value = known, 2:7 ) %&gt;% arrange(ID) ## # A tibble: 174 x 8 ## ID LANGUAGE progress gender age tester prog_lang known ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 22 OTHER FINISH female 30 GW C++ 1 ## 2 22 OTHER FINISH female 30 GW FORTRAN 1 ## 3 22 OTHER FINISH female 30 GW JavaScript 1 ## 4 22 OTHER FINISH female 30 GW Python NA ## 5 22 OTHER FINISH female 30 GW R NA ## 6 22 OTHER FINISH female 30 GW Ruby NA ## 7 23 &lt;NA&gt; no male 30 GW C++ 1 ## 8 23 &lt;NA&gt; no male 30 GW FORTRAN NA ## 9 23 &lt;NA&gt; no male 30 GW JavaScript NA ## 10 23 &lt;NA&gt; no male 30 GW Python NA ## # ... with 164 more rows This is now in a better format. However, we have a lot of rows with NA in the language, where people don’t know that language. This is redundant information. Additionally, the known column is now redundant if we remove the languages people don’t know, so we can remove this column too. We’ll save this data under the name demo_gathered for comparison with the long formatted data set we already loaded. demo_gathered &lt;- demo_wide %&gt;% gather( key = prog_lang, value = known, 2:7, na.rm = TRUE ) %&gt;% arrange(ID) %&gt;% select(-known) demo_gathered ## # A tibble: 54 x 7 ## ID LANGUAGE progress gender age tester prog_lang ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW C++ ## 2 22 OTHER FINISH female 30 GW FORTRAN ## 3 22 OTHER FINISH female 30 GW JavaScript ## 4 23 &lt;NA&gt; no male 30 GW C++ ## 5 24 ENGLISH END female 30 GW FORTRAN ## 6 25 &lt;NA&gt; ethics male 18 GW C++ ## 7 25 &lt;NA&gt; ethics male 18 GW FORTRAN ## 8 26 english ethics male 31 GW Python ## 9 26 english ethics male 31 GW Ruby ## 10 27 ENGLISH END female 44 GW FORTRAN ## # ... with 44 more rows We used the argument, na.rm = TRUE to remove any rows in our value column, known, with an NA. Since we did that, the known column contains all 1s, as the only languages left are the ones people know. So, we used the select function from dplyr to remove the known column. This function is used to select the column you want to keep in your data set. If you provide a column name with the - prefix, this tells R to keep everything except that column; so we drop it from our data set! 4.3.2 Separating Columns In our wide formatted data, we have two columns which store two data points in each cell: funRec has information on whether people found the experiment fun, and whether they’d recommend it to others. Let’s split this into separate columns. We just need to supply the name of the column to separate col, and what you want it split into, as a list of the names the columns should take, into. As in previous examples, I’ll remove the middle rows from the data here, but feel free to print them all yourself in R. I’ve presented the code to do this below. Remember, if we want to supply multiple names, we need to concatenate (c) these names together. demo_gathered %&gt;% separate( col = funRec, into = c(&quot;fun&quot;, &quot;recommend&quot;) ) ## # A tibble: 54 x 5 ## ID tester fun recommend completion_time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 GW 7 no 2018-03-22 23:06:11_2018-03-23 00:25:51 ## 2 22 GW 7 no 2018-03-22 23:06:11_2018-03-23 00:25:51 ## 3 22 GW 7 no 2018-03-22 23:06:11_2018-03-23 00:25:51 ## 4 23 GW 6 yes 2018-03-26 00:30:20_2018-03-26 02:15:52 ## 5 24 GW 4 yes 2018-03-21 11:09:38_2018-03-21 12:16:28 ## 6 25 GW 0 no 2018-03-25 13:03:58_2018-03-25 14:45:25 ## 7 25 GW 0 no 2018-03-25 13:03:58_2018-03-25 14:45:25 ## 8 26 GW 4 no 2018-03-24 06:46:30_2018-03-24 08:17:29 ## 9 26 GW 4 no 2018-03-24 06:46:30_2018-03-24 08:17:29 ## 10 27 GW 5 no 2018-03-21 03:23:57_2018-03-21 04:28:01 ## # ... with 44 more rows Take a look at the two new columns. They are both parsed as characters, even though the fun column only contains numbers. We can ask R to convert the data types for the split column during in the separate function using convert = TRUE. demo_gathered %&gt;% separate( col = funRec, into = c(&quot;fun&quot;, &quot;recommend&quot;), convert = TRUE ) ## # A tibble: 54 x 5 ## ID tester fun recommend completion_time ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 GW 7 no 2018-03-22 23:06:11_2018-03-23 00:25:51 ## 2 22 GW 7 no 2018-03-22 23:06:11_2018-03-23 00:25:51 ## 3 22 GW 7 no 2018-03-22 23:06:11_2018-03-23 00:25:51 ## 4 23 GW 6 yes 2018-03-26 00:30:20_2018-03-26 02:15:52 ## 5 24 GW 4 yes 2018-03-21 11:09:38_2018-03-21 12:16:28 ## 6 25 GW 0 no 2018-03-25 13:03:58_2018-03-25 14:45:25 ## 7 25 GW 0 no 2018-03-25 13:03:58_2018-03-25 14:45:25 ## 8 26 GW 4 no 2018-03-24 06:46:30_2018-03-24 08:17:29 ## 9 26 GW 4 no 2018-03-24 06:46:30_2018-03-24 08:17:29 ## 10 27 GW 5 no 2018-03-21 03:23:57_2018-03-21 04:28:01 ## # ... with 44 more rows That looks much better! separate() is smart enough to know how to separate values if they are split by special characters. Before, we had an underscore in the funRec column, so it split the data by that. If this fails, you can directly specify how the values are separated using the sep argument. We also wanted to split the completion_time column. It looks like the first value is the start time, and the second is the end time. So lets separate these together with the funRec column. demo_gathered %&gt;% separate( col = funRec, into = c(&quot;fun&quot;, &quot;recommend&quot;), convert = TRUE ) %&gt;% separate( col = completion_time, into = c(&quot;start_time&quot;, &quot;end_time&quot;) ) ## Warning: Expected 2 pieces. Additional pieces discarded in 54 rows [1, 2, ## 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## # A tibble: 54 x 6 ## ID tester fun recommend start_time end_time ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 GW 7 no 2018 03 ## 2 22 GW 7 no 2018 03 ## 3 22 GW 7 no 2018 03 ## 4 23 GW 6 yes 2018 03 ## 5 24 GW 4 yes 2018 03 ## 6 25 GW 0 no 2018 03 ## 7 25 GW 0 no 2018 03 ## 8 26 GW 4 no 2018 03 ## 9 26 GW 4 no 2018 03 ## 10 27 GW 5 no 2018 03 ## # ... with 44 more rows Oops, it looks like separate struggled to split our completion_time column correctly. That’s because it wants to split at every dash, whitespace, colon, and underscore; pretty much the whole completion_time column! Let’s be more specific and tell separate() to just split the columns at the underscore. Let’s also overwrite our demo_wide data (assign the new data to the old data name) to use this new format in the next section. demo_gathered &lt;- demo_gathered %&gt;% separate( col = funRec, into = c(&quot;fun&quot;, &quot;recommend&quot;), convert = TRUE ) %&gt;% separate( col = completion_time, into = c(&quot;start_time&quot;, &quot;end_time&quot;), sep = &quot;_&quot; ) # see the data demo_gathered ## # A tibble: 54 x 6 ## ID tester fun recommend start_time end_time ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 GW 7 no 2018-03-22 23:06:11 2018-03-23 00:25:51 ## 2 22 GW 7 no 2018-03-22 23:06:11 2018-03-23 00:25:51 ## 3 22 GW 7 no 2018-03-22 23:06:11 2018-03-23 00:25:51 ## 4 23 GW 6 yes 2018-03-26 00:30:20 2018-03-26 02:15:52 ## 5 24 GW 4 yes 2018-03-21 11:09:38 2018-03-21 12:16:28 ## 6 25 GW 0 no 2018-03-25 13:03:58 2018-03-25 14:45:25 ## 7 25 GW 0 no 2018-03-25 13:03:58 2018-03-25 14:45:25 ## 8 26 GW 4 no 2018-03-24 06:46:30 2018-03-24 08:17:29 ## 9 26 GW 4 no 2018-03-24 06:46:30 2018-03-24 08:17:29 ## 10 27 GW 5 no 2018-03-21 03:23:57 2018-03-21 04:28:01 ## # ... with 44 more rows That looks a lot better! Notice that we didn’t try to convert the start_time and end_time columns as this data type doesn’t play nicely with separte. We’ll look into how to convert between data types in Lesson 5. Note: If every row doesn’t produce the same number of columns, you can control what happens here with the extra argument. We won’t cover this, but it’s useful to know if you get into problems with separate() because of this issue. Let’s compare our gathered data to the long formatted data we already loaded. It’s exactly the same, only we separated the two problematic columns – funRec and completion_time – and we have a different label for the programming languages known (prog_lang vs. computer_language). head(demo_gathered) head(demo_long) ## # A tibble: 6 x 11 ## ID LANGUAGE progress gender age tester fun recommend start_time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no 2018-03-22 ~ ## 2 22 OTHER FINISH female 30 GW 7 no 2018-03-22 ~ ## 3 22 OTHER FINISH female 30 GW 7 no 2018-03-22 ~ ## 4 23 &lt;NA&gt; no male 30 GW 6 yes 2018-03-26 ~ ## 5 24 ENGLISH END female 30 GW 4 yes 2018-03-21 ~ ## 6 25 &lt;NA&gt; ethics male 18 GW 0 no 2018-03-25 ~ ## # ... with 2 more variables: end_time &lt;chr&gt;, prog_lang &lt;chr&gt; ## # A tibble: 6 x 9 ## ID LANGUAGE progress gender age tester funRec completion_time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7-no 2018-03-22 23:06:11_~ ## 2 22 OTHER FINISH female 30 GW 7-no 2018-03-22 23:06:11_~ ## 3 22 OTHER FINISH female 30 GW 7-no 2018-03-22 23:06:11_~ ## 4 23 &lt;NA&gt; no male 30 GW 6-yes 2018-03-26 00:30:20_~ ## 5 24 ENGLISH END female 30 GW 4-yes 2018-03-21 11:09:38_~ ## 6 25 &lt;NA&gt; ethics male 18 GW 0-no 2018-03-25 13:03:58_~ ## # ... with 1 more variable: computer_language &lt;chr&gt; 4.3.3 Spreading Data What if we want to go from long format to wide format? This can be useful if we want to do a paired-samples t-test, where we might want the first scores in one column, and the second scores in another. (We’ll cover t-tests in Lesson 6.) To make our long data wide, we use the spread() function from tidyr. To spread our data we need a key, the column containing the values we’d like to make column headers. We then also need a value, indicating the column containing the scores associated with the values. Often, this would be conditions in an experiment and test results. Our problem here is a little more complex. Remember that we dropped the redundant column telling us whether or not people knew a programming language? Well, we need this back so we have a value column to work from. We’ll mutate our data to create this column. To do this, we use the mutate() function from dplyr. (We’ll look at this process in detail in Lesson 5.) Here, we just set everything in our new known column to 1 as we know if a language is present in a participant’s row, then they know it! demo_gathered %&gt;% mutate(known = 1) %&gt;% spread(key = prog_lang, value = known) ## # A tibble: 29 x 11 ## ID fun recommend start_time end_time `C++` FORTRAN JavaScript ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 22 7 no 2018-03-22 2~ 2018-03-2~ 1.00 1.00 1.00 ## 2 23 6 yes 2018-03-26 0~ 2018-03-2~ 1.00 NA NA ## 3 24 4 yes 2018-03-21 1~ 2018-03-2~ NA 1.00 NA ## 4 25 0 no 2018-03-25 1~ 2018-03-2~ 1.00 1.00 NA ## 5 26 4 no 2018-03-24 0~ 2018-03-2~ NA NA NA ## 6 27 5 no 2018-03-21 0~ 2018-03-2~ NA 1.00 1.00 ## 7 28 0 no 2018-03-25 2~ 2018-03-2~ NA NA NA ## 8 29 6 no 2018-03-24 1~ 2018-03-2~ NA NA 1.00 ## 9 30 1 yes 2018-03-22 0~ 2018-03-2~ NA 1.00 NA ## 10 A2 4 yes 2018-03-23 0~ 2018-03-2~ 1.00 1.00 1.00 ## # ... with 19 more rows, and 3 more variables: Python &lt;dbl&gt;, R &lt;dbl&gt;, ## # Ruby &lt;dbl&gt; Great, that looks exactly the same as our demo_wide data, only with our nicely split columns. Note: If you have your values spread across several columns, spread will spread by every unique value, so be sure to collapse your values into one column before you do this. You can do this using the unite() function, or pasting values together with mutate(), but we’ll cover this more in Lesson 5. 4.4 Joins Finally, we’ll look at combining data together from separate tables. This is a common problem when we store demographic information in one data set, and test scores in another. Let’s say we’re interested in differences in performance by age. To do this, we somehow need to join together the demographic information of age with the correct participant ID in the test data. Let’s load some raw data for the lexical decision times to see how we might join data together from separate data sets. lexdec_data &lt;- read_csv(&quot;inputs/lexical_decision_raw_data.csv&quot;) To make these examples easier to digest, we’ll simply look at a single trial for an individual participant. Again, to do this we’ll use some subsetting techniques that we’ll go into in more detail in Lesson 5. # keep only trials (rows) where the word is ant lexdec_subset &lt;- lexdec_data %&gt;% filter(word == &quot;ant&quot;) How does the data look? We have 26 recorded entries. For two participants, they have missing values (NA) for these trials, indicating that they didn’t complete this trial, or the trial wasn’t recorded. lexdec_subset ## # A tibble: 25 x 9 ## subject trial native_language word class frequency length correct ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 A1 157 English ant animal 5.35 3 correct ## 2 A3 41 Other ant animal 5.35 3 correct ## 3 C 86 English ant animal 5.35 3 correct ## 4 D 138 Other ant animal 5.35 3 correct ## 5 I 105 Other ant animal 5.35 3 correct ## 6 J 43 Other ant animal 5.35 3 correct ## 7 K 183 English ant animal 5.35 3 correct ## 8 M1 171 English ant animal 5.35 3 correct ## 9 M2 117 Other ant animal 5.35 3 correct ## 10 P 115 Other ant animal 5.35 3 correct ## # ... with 15 more rows, and 1 more variable: RT &lt;dbl&gt; This data set contains all the information for our trials, including the data to identify each trial, and what the score was on our dependent variables (correct/incorrect response, reaction time). Here we have the data in a nice format where each column represents a variable, and each cell represents a value for that variable. Let’s look at our demographic data set. As you can tell, our our data is in a long format. Additionally, it looks like we don’t have any record of the language spoken by subject 23. On top of this, we have the cryptically named “no” entry in the progress column. I’m guessing this means that they decided to withdraw from the experiment. This means that we have more information on participants than we will have in the lexdec_subset dataset. The implications of this will become apparent as we try out different joining operations. demo_gathered ## # A tibble: 54 x 11 ## ID LANGUAGE progress gender age tester fun recommend start_time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no 2018-03-22~ ## 2 22 OTHER FINISH female 30 GW 7 no 2018-03-22~ ## 3 22 OTHER FINISH female 30 GW 7 no 2018-03-22~ ## 4 23 &lt;NA&gt; no male 30 GW 6 yes 2018-03-26~ ## 5 24 ENGLISH END female 30 GW 4 yes 2018-03-21~ ## 6 25 &lt;NA&gt; ethics male 18 GW 0 no 2018-03-25~ ## 7 25 &lt;NA&gt; ethics male 18 GW 0 no 2018-03-25~ ## 8 26 english ethics male 31 GW 4 no 2018-03-24~ ## 9 26 english ethics male 31 GW 4 no 2018-03-24~ ## 10 27 ENGLISH END female 44 GW 5 no 2018-03-21~ ## # ... with 44 more rows, and 2 more variables: end_time &lt;chr&gt;, ## # prog_lang &lt;chr&gt; 4.4.1 Mutating Joins There are a number of joining operations we can do that will mutate (change the look of) our data: left_join(data_one, data_two): Keeps everything in data_one and adds everything present in both data_one and data_two right_join(data_one, data_two): Keeps everything in data_two and adds everything present in both data_two and data_one inner_join(data_one, data_two): Keeps everything present in both data sets. full_join(data_one, data_two): Keeps everything from both data sets. Adds NAs if information is present in only one data set. Don’t worry about the number of different joins here, they all take a similar form, but just do slightly different things to your data. All of these joins take a by argument, which asks you which columns by which you want to combine the data. If we want to make sure we match up the data, we have to make sure our columns have the same headings across the two data sets. Take a look at the two data sets above, it looks like we identify subjects with subject in the lexdec_data data set, and by ID in the demo_gathered data set. We also have the identifier for the language spoken as native_language in the lexdec_data data set, and as LANGUAGE in the demo_gathered data set. We can use rename from dplr to rename our columns. Here we just supply the new name and the old name. The names in demo_gathered are messy, so we’ll change those to match the lexdec_data names. demo_gathered &lt;- rename(demo_gathered, subject = ID, native_language = LANGUAGE ) demo_gathered ## # A tibble: 54 x 11 ## subject native_language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no ## 2 22 OTHER FINISH female 30 GW 7 no ## 3 22 OTHER FINISH female 30 GW 7 no ## 4 23 &lt;NA&gt; no male 30 GW 6 yes ## 5 24 ENGLISH END female 30 GW 4 yes ## 6 25 &lt;NA&gt; ethics male 18 GW 0 no ## 7 25 &lt;NA&gt; ethics male 18 GW 0 no ## 8 26 english ethics male 31 GW 4 no ## 9 26 english ethics male 31 GW 4 no ## 10 27 ENGLISH END female 44 GW 5 no ## # ... with 44 more rows, and 3 more variables: start_time &lt;chr&gt;, ## # end_time &lt;chr&gt;, prog_lang &lt;chr&gt; Note: We could alternatively set by to by = c(&quot;subject&quot; = &quot;ID&quot;, &quot;native_language&quot; = &quot;LANGUAGE&quot;) to join by variables with different names across the data sets, but I find it’s good practice to be consistent with your naming. 4.4.1.1 Full Join Now we can join the data sets together. We’ll do a full_join() first, just to see what happens. full_join(lexdec_subset, demo_gathered, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 55 x 18 ## subject trial native_language word class frequency length correct ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 A1 157 English ant animal 5.35 3 correct ## 2 A3 41 Other ant animal 5.35 3 correct ## 3 C 86 English ant animal 5.35 3 correct ## 4 C 86 English ant animal 5.35 3 correct ## 5 D 138 Other ant animal 5.35 3 correct ## 6 D 138 Other ant animal 5.35 3 correct ## 7 I 105 Other ant animal 5.35 3 correct ## 8 J 43 Other ant animal 5.35 3 correct ## 9 J 43 Other ant animal 5.35 3 correct ## 10 J 43 Other ant animal 5.35 3 correct ## # ... with 45 more rows, and 10 more variables: RT &lt;dbl&gt;, progress &lt;chr&gt;, ## # gender &lt;chr&gt;, age &lt;int&gt;, tester &lt;chr&gt;, fun &lt;int&gt;, recommend &lt;chr&gt;, ## # start_time &lt;chr&gt;, end_time &lt;chr&gt;, prog_lang &lt;chr&gt; We’ve successfully merged the two data sets, but we now have multiple rows for our responses because we kept the programming language column. This is problematic if we want to calculate any statistics directly on this data frame, as we’ll end up with what seems like multiple observations for a single trial. We have a couple of workarounds for this problem: Merge with the demographic data in a wide format, in which case we’ll have multiple columns each representing a different programming language. Merge with the demographic data set in a long format, but exclude the prog_lang column and filter the leftover duplicate rows prior to merging. For now, we’ll stick with 1 as it required fewer steps. But first, we want to transform our nicely tidied demographic data set into a wide format. Just reuse the code from the spreading section to do this: tidy_demo_wide &lt;- demo_gathered %&gt;% mutate(known = 1) %&gt;% # create a value column spread(key = prog_lang, value = known) # data to wide format # see the output tidy_demo_wide ## # A tibble: 29 x 16 ## subject native_language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no ## 2 23 &lt;NA&gt; no male 30 GW 6 yes ## 3 24 ENGLISH END female 30 GW 4 yes ## 4 25 &lt;NA&gt; ethics male 18 GW 0 no ## 5 26 english ethics male 31 GW 4 no ## 6 27 ENGLISH END female 44 GW 5 no ## 7 28 &lt;NA&gt; ethics male 23 GW 0 no ## 8 29 english ethics male 34 GW 6 no ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes ## 10 A2 English END female 33 RHB 4 yes ## # ... with 19 more rows, and 8 more variables: start_time &lt;chr&gt;, ## # end_time &lt;chr&gt;, `C++` &lt;dbl&gt;, FORTRAN &lt;dbl&gt;, JavaScript &lt;dbl&gt;, ## # Python &lt;dbl&gt;, R &lt;dbl&gt;, Ruby &lt;dbl&gt; Now, if we try the full join, we’ll merge together the two data sets so we have all of the information in one place! We want to match the data sets by the subject ID and the native language spoken by the participants, as these two columns appear in both data sets. As before, we have so much data that I’ll subset things so we can see the relevant information, but be sure to print the whole output in R yourself. full_join(lexdec_subset, tidy_demo_wide, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 5 x 6 ## subject trial native_language progress correct RT ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A1 157 English &lt;NA&gt; correct 876 ## 2 23 41 &lt;NA&gt; no correct 602 ## 3 24 43 ENGLISH END correct 541 ## 4 28 NA &lt;NA&gt; ethics &lt;NA&gt; NA ## 5 A2 NA English END &lt;NA&gt; NA As you can see, we now have 1 row for each subject. In cases where we don’t have data on a subject, we simply have NAs in those cells. Look at subject 23, you can see that we don’t have data on their native language, but we have their trial information. This means they are in both data sets, but they have missing data in both cases. Look at subject A2, they have missing data for their trial information, but we know their native language and progress. This means they are missing from the lexdec_subset data set, but they are present in the tidy_demo_wide data set. Look at subject 28, they have missing trial data, indicating they aren’t present in the lexdec_subset data set, and they are missing a native language, which indicates this data is also missing in the tidy_demo_wide data set. 4.4.1.2 Inner Join This keeps data only present in both data sets. We have lost participants A1 and A2 because A1 wasn’t present in the tidy_demo_wide data set, and A2 wasn’t present in the lexdec_subset data set. inner_join(lexdec_subset, tidy_demo_wide, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 24 x 6 ## subject trial native_language progress correct RT ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A3 41 Other END correct 607 ## 2 C 86 English END correct 725 ## 3 D 138 Other END correct 628 ## 4 I 105 Other END correct 560 ## 5 J 43 Other END correct 516 ## 6 K 183 English END correct 442 ## 7 M1 171 English END correct 427 ## 8 M2 117 Other END correct 530 ## 9 P 115 Other END correct 533 ## 10 R1 30 English END correct 483 ## # ... with 14 more rows 4.4.1.3 Left Join Left joins only keep the data that is present in the left data set (lexdec_subset) and adds anything that matches up from the right data set (tidy_demo_wide). Here we have participant A1 because they are in the lexdec_subset, even if they are missing from the tidy_demo_wide data set. left_join(lexdec_subset, tidy_demo_wide, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 25 x 6 ## subject trial native_language progress correct RT ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A1 157 English &lt;NA&gt; correct 876 ## 2 A3 41 Other END correct 607 ## 3 C 86 English END correct 725 ## 4 D 138 Other END correct 628 ## 5 I 105 Other END correct 560 ## 6 J 43 Other END correct 516 ## 7 K 183 English END correct 442 ## 8 M1 171 English END correct 427 ## 9 M2 117 Other END correct 530 ## 10 P 115 Other END correct 533 ## # ... with 15 more rows 4.4.1.4 Right Join This works like the left join, only it keeps everything present in the right data set and anything matching from the left data set. Here, we do not have data on participant A1 because they are not present in the tidy_demo_wide data set. right_join(lexdec_subset, tidy_demo_wide, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 29 x 6 ## subject trial native_language progress correct RT ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 22 116 OTHER FINISH correct 467 ## 2 23 41 &lt;NA&gt; no correct 602 ## 3 24 43 ENGLISH END correct 541 ## 4 25 NA &lt;NA&gt; ethics &lt;NA&gt; NA ## 5 26 NA english ethics &lt;NA&gt; NA ## 6 27 114 ENGLISH END correct 586 ## 7 28 NA &lt;NA&gt; ethics &lt;NA&gt; NA ## 8 29 NA english ethics &lt;NA&gt; NA ## 9 30 77 &lt;NA&gt; &lt;NA&gt; correct 608 ## 10 A2 NA English END &lt;NA&gt; NA ## # ... with 19 more rows 4.4.2 Filtering Joins We can filter data by using joins. These next joins don’t merge columns, but instead allow us to just subset our data. 4.4.2.1 Semi Join With a semi-join we keep all rows and columns from the left data set where we have matching values in the right data set. Crucially, we do not keep the columns from the right data set. semi_join(lexdec_subset, tidy_demo_wide, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 24 x 9 ## subject trial native_language word class frequency length correct ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 A3 41 Other ant animal 5.35 3 correct ## 2 C 86 English ant animal 5.35 3 correct ## 3 D 138 Other ant animal 5.35 3 correct ## 4 I 105 Other ant animal 5.35 3 correct ## 5 J 43 Other ant animal 5.35 3 correct ## 6 K 183 English ant animal 5.35 3 correct ## 7 M1 171 English ant animal 5.35 3 correct ## 8 M2 117 Other ant animal 5.35 3 correct ## 9 P 115 Other ant animal 5.35 3 correct ## 10 R1 30 English ant animal 5.35 3 correct ## # ... with 14 more rows, and 1 more variable: RT &lt;dbl&gt; Here we only kept data in the lexdec_subset for subjects that were present in both data sets. Notice how we do not have data for subjects A1 and A2. This works like an inner join, but does not duplicate rows. Notice that we get the same result with the long demographic data set as with the wide demographic data set. semi_join(lexdec_subset, demo_gathered, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 24 x 9 ## subject trial native_language word class frequency length correct ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 A3 41 Other ant animal 5.35 3 correct ## 2 C 86 English ant animal 5.35 3 correct ## 3 D 138 Other ant animal 5.35 3 correct ## 4 I 105 Other ant animal 5.35 3 correct ## 5 J 43 Other ant animal 5.35 3 correct ## 6 K 183 English ant animal 5.35 3 correct ## 7 M1 171 English ant animal 5.35 3 correct ## 8 M2 117 Other ant animal 5.35 3 correct ## 9 P 115 Other ant animal 5.35 3 correct ## 10 R1 30 English ant animal 5.35 3 correct ## # ... with 14 more rows, and 1 more variable: RT &lt;dbl&gt; 4.4.2.2 Anti Join An anti-join works like the inverse of a semi-join. Here, we get all the values from the left table that do not have a match in the right table. anti_join(lexdec_subset, tidy_demo_wide, by = c(&quot;subject&quot;, &quot;native_language&quot;)) ## # A tibble: 1 x 9 ## subject trial native_language word class frequency length correct RT ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A1 157 English ant anim~ 5.35 3 correct 876 In this case, we only get participant A1 from the lexdec_subset data set, as we do not have any demographic information on this subject in the tidy_demo_wide data set. 4.4.3 Binding Joins We can bind rows from separate data sets with the same number of columns using the bind_rows() command. This is useful if we have ran an experiment in two parts on differnet sets of subjects, and we simply want to put all of the responses in one data set. Alternatively, we can bind columns from separate data sets with the same number of rows using the bind_cols() command. This is useful if we have an experiment in two parts where we want to want to add some additional information about all of participants to one data set. 4.5 Checking for Unique and Duplicate Information Finally, we can use a number of functions to check for unique information across two different data sets. intersect() gives us all the rows in two tables that match exactly. This is useful if we have messy data stored in multiple tables and we’re not sure if we have duplicates. Note that every cell has to match exactly for this to work. union() gives us all of the rows from two tables except any duplicates. setdiff() gives us rows from our first data set that aren’t present in the second. 4.6 Exercises 4.6.1 Introduction and Setup For these exercises, we will look at the core concepts from this lesson. We’ll also get some hands-on experience with binding joins and checking for duplicates, two concepts that we’ve touched on but not went into much detail. For these exercises we’ll use some toy data sets; ex_demo_data, which has demographic information on 6 participants, and ex_test_data, which has IQ test scores for 6 participants. Crucially, the first data set has some missing values, and the second has the same participant tested twice. # load the tidyverse library(tidyverse) # demographic data ex_demo_data &lt;- tibble( subject = seq(1: 6), height = c(NA, 170, 160, 165, NA, 180), weight = c(70, 65, 80, NA, 77, 90), age = c(18, 19, 19, NA, 22, 28) ) # IQ test scores ex_test_data &lt;- tibble( subject = c(1, 3, 4, 4, 5, 6, 7), IQ = c(150, 160, 155, 155, 190, 120, 140) ) 4.6.2 Long and Wide Data 4.6.2.1 Question 1 Put the ex_demo_data into a long format with three columns: subject, measurement_id, and measurement. The measurement column should contain the scores for the height, weight, and age of the participants. The measurement_id column should contain text specifying which measurement belongs to which variable (height, weight, or age). Assign this to the variable long_data and return this table of data. 4.6.2.2 Question 2 Turn your newly created long_data back into a wide format. 4.6.3 Uniting and Separating Columns Here we have some messy data where we have two values for two variables in one column; height_weight. messy_demo_data &lt;- unite(ex_demo_data, &quot;height_weight&quot;, c(&quot;height&quot;, &quot;weight&quot;), sep = &quot;_&quot; ) messy_demo_data ## # A tibble: 6 x 3 ## subject height_weight age ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 NA_70 18.0 ## 2 2 170_65 19.0 ## 3 3 160_80 19.0 ## 4 4 165_NA NA ## 5 5 NA_77 22.0 ## 6 6 180_90 28.0 4.6.3.1 Question 3 Separate the messy columns into two tidy columns for height and weight. Should you convert the values when separating the column? If so, why? 4.6.4 Mutating Joins 4.6.4.1 Question 4 Join the ex_demo_data and ex_test_data together by subject number, keeping only data with a match in ex_test_data. 4.6.4.2 Question 5 Join the ex_demo_data and ex_test_data together by subject number, keeping only data with a match in ex_demo_data. 4.6.4.3 Question 6 Why do we get different results in question 4 and question 5? 4.6.5 Filtering Joins 4.6.5.1 Question 7 Return all of the values from ex_demo_data that have a match in ex_test_data. Look at subject 4, why do we get a different result to that from question 5? Look at the columns returned, why does this differ from question 5? 4.6.6 Binding Joins Here we have some new data looking at the demographic scores for new subjects. We also have another rating for all of the participants from our study and we want to add this to the demographic data. new_demographics &lt;- tibble( subject = c(9, 10), height = c(170, 190), weight = c(76, 85), age = c(40, 59) ) eye_colour &lt;- tibble( eye_colour = sample(c(&quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;), size = 8, replace = TRUE ) ) 4.6.6.1 Question 8 Add the rows from new_demographics to ex_demo_data. Assign this to all_demo_data and return this table. 4.6.6.2 Question 9 Add the eye colour column to the all_demo_data table. Why did we not have a subject identifier in the eye_colour data set? Can you predict the result if we did have this information? 4.6.7 Checking for Duplicates We have some new test data below. extra_test_data &lt;- tibble( subject = c(1, 9, 10), IQ = c(150, 156, 179) ) 4.6.7.1 Question 10 Return rows with duplicates from the ex_test_data and extra_test_data data sets. References "],
["data-manipulation-2.html", "Chapter 5 Data Manipulation 2 5.1 Getting Started 5.2 Understanding our Data 5.3 Preparing our Data 5.4 Selecting Columns 5.5 Creating and Changing Columns 5.6 Filtering to Observations 5.7 Arranging Data 5.8 Summarising Data 5.9 Grouping Data 5.10 Chaining Many Functions 5.11 Saving Data 5.12 Exercises", " Chapter 5 Data Manipulation 2 In this section, we’ll look at getting our data in the correct format for reporting your analyses. We’ll cover topics such as renaming and reordering variables, creating new variables, and creating summaries of your data. To do this, we’ll use the dplyr package (Wickham et al. 2017) from the tidyverse (Wickham 2017). We’ll cover the following functions that work with the group_by functioin: arrange(): order variables (i.e. ordering in rows) select(): pick out variables (i.e. subsetting by columns, changing column order) filter(): pick out observations (i.e. subsetting by observations) mutate(): create new variables summarise(): create a summary of variables (e.g. mean, SD, n) The group_by function allows you to change the scope by which these functions work. For example, we can group our data by condition with group_by(condition) and then create summary statistics with summarise(). Additionally, we’ll cover renaming variables with rename(). You should notice some familiar patterns with how we use and mix together these functions as we progress. 5.1 Getting Started As always, we first need to load the tidyverse set of packages for this Chapter. library(tidyverse) Next, we’ll load the raw and (wide formatted) demographic data sets from the lexical decision experiment from the previous Chapter. To do so, we’ll use read_csv() here, for reasons explained in the previous Chapter. demo &lt;- read_csv(&quot;inputs/lexical_decision_demographic_data_wide.csv&quot;) You should get a warning when loading the data, telling you how each column is parsed. We can specify during the loading process how to parse each column, for example setting strings to factors. However, we’ll do this manually in dplyr() to give you more experience using this pacakge for mutating your data. 5.2 Understanding our Data Let’s look at the data we’ve just loaded to get an idea of how we want to change it. When we have many rows and we attempt to print a tibble, we won’t see all columns in the console. One way to get an idea of how all columns look is to transpose these columns and take a look at the first few observations. To do this, we can use glimpse() to get a glimpse of our data. glimpse(demo) ## Observations: 29 ## Variables: 14 ## $ ID &lt;chr&gt; &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;... ## $ `C++` &lt;int&gt; 1, 1, NA, 1, NA, NA, NA, NA, NA, 1, 1, NA, NA,... ## $ FORTRAN &lt;int&gt; 1, NA, 1, 1, NA, 1, NA, NA, 1, 1, NA, NA, 1, N... ## $ JavaScript &lt;int&gt; 1, NA, NA, NA, NA, 1, NA, 1, NA, 1, NA, NA, NA... ## $ Python &lt;int&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, 1, ... ## $ R &lt;int&gt; NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, 1, ... ## $ Ruby &lt;int&gt; NA, NA, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, ... ## $ LANGUAGE &lt;chr&gt; &quot;OTHER&quot;, NA, &quot;ENGLISH&quot;, NA, &quot;english&quot;, &quot;ENGLIS... ## $ progress &lt;chr&gt; &quot;FINISH&quot;, &quot;no&quot;, &quot;END&quot;, &quot;ethics&quot;, &quot;ethics&quot;, &quot;EN... ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;f... ## $ age &lt;int&gt; 30, 30, 30, 18, 31, 44, 23, 34, 32, 33, 20, 31... ## $ tester &lt;chr&gt; &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;... ## $ funRec &lt;chr&gt; &quot;7-no&quot;, &quot;6-yes&quot;, &quot;4-yes&quot;, &quot;0-no&quot;, &quot;4-no&quot;, &quot;5-n... ## $ completion_time &lt;chr&gt; &quot;2018-03-22 23:06:11_2018-03-23 00:25:51&quot;, &quot;20... As we can see, we have information on the subject’s ID, columns indicating any programming languages they might know, a column specifying their language spoken, how far they got in the experiment, some demographic information, who tested them, whether they liked the experiment and would recommend it to others, and the start and end times for their part in the experiment. We can also see how this data was parsed, with numeric columns as integers, and all other columns as characters. However, some of this data would be best represented as a different format. We’ll look into changing this later in this Chapter. 5.3 Preparing our Data Our data is quite messy as it stands. One of the biggest issues is that we have two variables contained within one column for both the funRec and completion_time columns. Let’s separate these two columns using the separate() function from tidyr. demo &lt;- demo %&gt;% separate(col = funRec, into = c(&quot;fun&quot;, &quot;recommend&quot;)) %&gt;% separate(col = completion_time, into = c(&quot;start&quot;, &quot;end&quot;), sep = &quot;_&quot;) glimpse(demo) ## Observations: 29 ## Variables: 16 ## $ ID &lt;chr&gt; &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;30... ## $ `C++` &lt;int&gt; 1, 1, NA, 1, NA, NA, NA, NA, NA, 1, 1, NA, NA, NA, ... ## $ FORTRAN &lt;int&gt; 1, NA, 1, 1, NA, 1, NA, NA, 1, 1, NA, NA, 1, NA, 1,... ## $ JavaScript &lt;int&gt; 1, NA, NA, NA, NA, 1, NA, 1, NA, 1, NA, NA, NA, NA,... ## $ Python &lt;int&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, 1, NA, N... ## $ R &lt;int&gt; NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, 1, 1, NA... ## $ Ruby &lt;int&gt; NA, NA, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, NA, 1... ## $ LANGUAGE &lt;chr&gt; &quot;OTHER&quot;, NA, &quot;ENGLISH&quot;, NA, &quot;english&quot;, &quot;ENGLISH&quot;, N... ## $ progress &lt;chr&gt; &quot;FINISH&quot;, &quot;no&quot;, &quot;END&quot;, &quot;ethics&quot;, &quot;ethics&quot;, &quot;END&quot;, &quot;... ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female... ## $ age &lt;int&gt; 30, 30, 30, 18, 31, 44, 23, 34, 32, 33, 20, 31, 33,... ## $ tester &lt;chr&gt; &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW... ## $ fun &lt;chr&gt; &quot;7&quot;, &quot;6&quot;, &quot;4&quot;, &quot;0&quot;, &quot;4&quot;, &quot;5&quot;, &quot;0&quot;, &quot;6&quot;, &quot;1&quot;, &quot;4&quot;, &quot;... ## $ recommend &lt;chr&gt; &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;... ## $ start &lt;chr&gt; &quot;2018-03-22 23:06:11&quot;, &quot;2018-03-26 00:30:20&quot;, &quot;2018... ## $ end &lt;chr&gt; &quot;2018-03-23 00:25:51&quot;, &quot;2018-03-26 02:15:52&quot;, &quot;2018... 5.4 Selecting Columns First off, we should probably drop any columns from the data that we don’t need to use. We’re not going to look at the programming languages they know, so we can drop all of these columns using select(). We’ll create a subset of our data that doesn’t contain these columns. Here, we’ll ask to keep all of the columns we want to keep. We can do this by name or column number. # by name demo %&gt;% select(ID, LANGUAGE, progress, gender, age, tester, fun, recommend, start, end ) ## # A tibble: 29 x 10 ## ID LANGUAGE progress gender age tester fun recommend start end ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no 2018~ 2018~ ## 2 23 &lt;NA&gt; no male 30 GW 6 yes 2018~ 2018~ ## 3 24 ENGLISH END female 30 GW 4 yes 2018~ 2018~ ## 4 25 &lt;NA&gt; ethics male 18 GW 0 no 2018~ 2018~ ## 5 26 english ethics male 31 GW 4 no 2018~ 2018~ ## 6 27 ENGLISH END female 44 GW 5 no 2018~ 2018~ ## 7 28 &lt;NA&gt; ethics male 23 GW 0 no 2018~ 2018~ ## 8 29 english ethics male 34 GW 6 no 2018~ 2018~ ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes 2018~ 2018~ ## 10 A2 English END female 33 RHB 4 yes 2018~ 2018~ ## # ... with 19 more rows # by number demo %&gt;% select(c(1, 8:16)) ## # A tibble: 29 x 10 ## ID LANGUAGE progress gender age tester fun recommend start end ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no 2018~ 2018~ ## 2 23 &lt;NA&gt; no male 30 GW 6 yes 2018~ 2018~ ## 3 24 ENGLISH END female 30 GW 4 yes 2018~ 2018~ ## 4 25 &lt;NA&gt; ethics male 18 GW 0 no 2018~ 2018~ ## 5 26 english ethics male 31 GW 4 no 2018~ 2018~ ## 6 27 ENGLISH END female 44 GW 5 no 2018~ 2018~ ## 7 28 &lt;NA&gt; ethics male 23 GW 0 no 2018~ 2018~ ## 8 29 english ethics male 34 GW 6 no 2018~ 2018~ ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes 2018~ 2018~ ## 10 A2 English END female 33 RHB 4 yes 2018~ 2018~ ## # ... with 19 more rows Alternatively, we could have just specified the columns we wanted to drop. To do so, we can tell R to select all columns, except (-) column numbers 2 through 7. Remember, we have to concatenate these values to apply tell R to remove all of these columns, and not just the first one. This time, we’ll save our data as demo_sub, as we’ve made a subset of our original demographic data. demo_sub &lt;- demo %&gt;% select(-c(2:7)) demo_sub ## # A tibble: 29 x 10 ## ID LANGUAGE progress gender age tester fun recommend start end ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no 2018~ 2018~ ## 2 23 &lt;NA&gt; no male 30 GW 6 yes 2018~ 2018~ ## 3 24 ENGLISH END female 30 GW 4 yes 2018~ 2018~ ## 4 25 &lt;NA&gt; ethics male 18 GW 0 no 2018~ 2018~ ## 5 26 english ethics male 31 GW 4 no 2018~ 2018~ ## 6 27 ENGLISH END female 44 GW 5 no 2018~ 2018~ ## 7 28 &lt;NA&gt; ethics male 23 GW 0 no 2018~ 2018~ ## 8 29 english ethics male 34 GW 6 no 2018~ 2018~ ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes 2018~ 2018~ ## 10 A2 English END female 33 RHB 4 yes 2018~ 2018~ ## # ... with 19 more rows There are a number of helper functions in select() which can help you to stop having to explicitly mention every column you want to keep. These are: starts_with(&quot;string&quot;): this keeps any columns with names starting with “string” ends_with(&quot;string&quot;): this keeps any columns with names ending with “string” contains(&quot;string&quot;): this keeps any columns with names containing “string” at any point matches(regular_expression): this keeps any columns that match a regular expression num_range(&quot;prefix&quot;, range): this keeps any columns with a matching prefix and a following range of numbers. For example, `num_range(“measurement”, 1:3) would keep all columns called “measurement1”, “measurement2”, and “measurement3”. For this simple case here, these aren’t necessary, but they’re good to know for working with larger data sets. 5.4.1 Renaming and Reordering Columns We can use the rename() function to change our columns names. We have some messy names here, so we’ll look at how we can rename them now. This renaming function takes the order of our new names are equal to our old names. Let’s just overwrite our old subsetted data to change the names. demo_sub &lt;- demo_sub %&gt;% rename(language = LANGUAGE) demo_sub ## # A tibble: 29 x 10 ## ID language progress gender age tester fun recommend start end ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no 2018~ 2018~ ## 2 23 &lt;NA&gt; no male 30 GW 6 yes 2018~ 2018~ ## 3 24 ENGLISH END female 30 GW 4 yes 2018~ 2018~ ## 4 25 &lt;NA&gt; ethics male 18 GW 0 no 2018~ 2018~ ## 5 26 english ethics male 31 GW 4 no 2018~ 2018~ ## 6 27 ENGLISH END female 44 GW 5 no 2018~ 2018~ ## 7 28 &lt;NA&gt; ethics male 23 GW 0 no 2018~ 2018~ ## 8 29 english ethics male 34 GW 6 no 2018~ 2018~ ## 9 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes 2018~ 2018~ ## 10 A2 English END female 33 RHB 4 yes 2018~ 2018~ ## # ... with 19 more rows Finally, what if we want to put the age and gender columns next to the ID column? We can use select() to pick out our first two columns, then everything() to grab every remaining column and stick them on the end. demo_sub %&gt;% select(ID, age, gender, everything()) ## # A tibble: 29 x 10 ## ID age gender language progress tester fun recommend start end ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 30 female OTHER FINISH GW 7 no 2018~ 2018~ ## 2 23 30 male &lt;NA&gt; no GW 6 yes 2018~ 2018~ ## 3 24 30 female ENGLISH END GW 4 yes 2018~ 2018~ ## 4 25 18 male &lt;NA&gt; ethics GW 0 no 2018~ 2018~ ## 5 26 31 male english ethics GW 4 no 2018~ 2018~ ## 6 27 44 female ENGLISH END GW 5 no 2018~ 2018~ ## 7 28 23 male &lt;NA&gt; ethics GW 0 no 2018~ 2018~ ## 8 29 34 male english ethics GW 6 no 2018~ 2018~ ## 9 30 32 female &lt;NA&gt; &lt;NA&gt; GW 1 yes 2018~ 2018~ ## 10 A2 33 female English END RHB 4 yes 2018~ 2018~ ## # ... with 19 more rows 5.5 Creating and Changing Columns If we want to create a new column in our data, or it we want to change how a column is represented, we have to mutate our data usingmutate(). What if we want to make a new column that tells us how long people spent on the experiment? We can create a new column by subtracting the start time of each participant from their end time. # this will not run # demo_sub &lt;- demo_sub %&gt;% # mutate(time = end - start) Oops, that didn’t work. Why not? It looks like the start and end columns are stored as characters. We can’t perform a subtraction on a character, so we need to tell R that these are numbers. Specifically, we need to tell R that these are datetimes. One package that makes this very easy is the lubridate package (Grolemund and Wickham 2011), which is designed to make working with datetimes very easy. Let’s install this before we convert our start and end columns to datetimes. library(lubridate) # install.packages(lubridate) # uncomment and run this only once library(lubridate) The lubridate package has a number of useful functions that allow us to covert our data types to datetimes, as well as ways to run calculations on these times. One function we’ll use here is the ymd_hms() function, which converts data to the POSIXct format. The upshoot of this is that we can now subract one time from the other to get a duration. Below, we’ll mutate the start and end columns to be themselves, only converted to the proper datetime format. We’ll also create a new column, time, made up of subtracting the start times from the end times to get a duration. demo_sub &lt;- demo_sub %&gt;% mutate(start = ymd_hms(start), end = ymd_hms(end), time = end - start ) glimpse(demo_sub) ## Observations: 29 ## Variables: 11 ## $ ID &lt;chr&gt; &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;30&quot;... ## $ language &lt;chr&gt; &quot;OTHER&quot;, NA, &quot;ENGLISH&quot;, NA, &quot;english&quot;, &quot;ENGLISH&quot;, NA... ## $ progress &lt;chr&gt; &quot;FINISH&quot;, &quot;no&quot;, &quot;END&quot;, &quot;ethics&quot;, &quot;ethics&quot;, &quot;END&quot;, &quot;e... ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;... ## $ age &lt;int&gt; 30, 30, 30, 18, 31, 44, 23, 34, 32, 33, 20, 31, 33, ... ## $ tester &lt;chr&gt; &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;... ## $ fun &lt;chr&gt; &quot;7&quot;, &quot;6&quot;, &quot;4&quot;, &quot;0&quot;, &quot;4&quot;, &quot;5&quot;, &quot;0&quot;, &quot;6&quot;, &quot;1&quot;, &quot;4&quot;, &quot;3... ## $ recommend &lt;chr&gt; &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;y... ## $ start &lt;dttm&gt; 2018-03-22 23:06:11, 2018-03-26 00:30:20, 2018-03-2... ## $ end &lt;dttm&gt; 2018-03-23 00:25:51, 2018-03-26 02:15:52, 2018-03-2... ## $ time &lt;time&gt; 1.327778 hours, 1.758889 hours, 1.113889 hours, 1.6... Did you notice that we can refer to columns we’ve asked to create within mutate() to create other columns? Pretty cool! We can perform any number of operations on our columns in mutate(). But what if we want to do this and only keep the new columns? We can use transmute(). Below, we’ll ask to keep the subject ID column, but to create our new start and end columns with mutate. Since the difference from the mean will (typically) be quite small, we can convert from hours to minutes by setting the units within our time variable to minutes. # calculate difference from mean completion time transmuted_time &lt;- demo_sub %&gt;% mutate( start = ymd_hms(start), end = ymd_hms(end), time = end - start ) %&gt;% transmute( ID, time_diff = time - mean(time) ) # define the units of time, this can be mins, hours, days, etc. units(transmuted_time$time_diff) &lt;- &quot;mins&quot; # print the object transmuted_time ## # A tibble: 29 x 2 ## ID time_diff ## &lt;chr&gt; &lt;time&gt; ## 1 22 -7.18563218390805 ## 2 23 18.6810344827586 ## 3 24 -20.0189655172414 ## 4 25 14.5977011494253 ## 5 26 4.13103448275861 ## 6 27 -22.785632183908 ## 7 28 17.4810344827586 ## 8 29 8.14770114942528 ## 9 30 -13.9022988505747 ## 10 A2 -5.86896551724138 ## # ... with 19 more rows Note that we can change a column to many different data types by simply setting the column name to itself and the data type you want. For example, mutate(start = as.numeric(start)). We can also mutate multiple columns at once using mutate_all() or mutate_at(), but we won’t cover this here. Check out the variants on the dplyr verbs to see how you can improve your code! So far, we’ve learned how to create new columns, including keeping these with the original data set (mutate()) or keeping only the new columns (transmute()). We’ve also seen how we can subset our data to the most relevant columns using select(). Next, we’ll look at how we can subset our data by selecting certain observations (i.e. rows) within our data. For this, we’ll use the filter() function. 5.6 Filtering to Observations The filter() function allows us to filter our data to certain observations. We can use a number of logical operations to filter by certain conditions. As with the other dplyr functions, filter() must take the data as an argument, but afterwards it needs to take a rule by which to subset your data. Here, you just specify the conditons by which to keep observations. Nicely, the filter() argument can be piped, and used in conjunction with other dplyr functions. Take a look at the demo_sub data set. glimpse(demo_sub) ## Observations: 29 ## Variables: 11 ## $ ID &lt;chr&gt; &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;30&quot;... ## $ language &lt;chr&gt; &quot;OTHER&quot;, NA, &quot;ENGLISH&quot;, NA, &quot;english&quot;, &quot;ENGLISH&quot;, NA... ## $ progress &lt;chr&gt; &quot;FINISH&quot;, &quot;no&quot;, &quot;END&quot;, &quot;ethics&quot;, &quot;ethics&quot;, &quot;END&quot;, &quot;e... ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;... ## $ age &lt;int&gt; 30, 30, 30, 18, 31, 44, 23, 34, 32, 33, 20, 31, 33, ... ## $ tester &lt;chr&gt; &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;, &quot;GW&quot;... ## $ fun &lt;chr&gt; &quot;7&quot;, &quot;6&quot;, &quot;4&quot;, &quot;0&quot;, &quot;4&quot;, &quot;5&quot;, &quot;0&quot;, &quot;6&quot;, &quot;1&quot;, &quot;4&quot;, &quot;3... ## $ recommend &lt;chr&gt; &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;y... ## $ start &lt;dttm&gt; 2018-03-22 23:06:11, 2018-03-26 00:30:20, 2018-03-2... ## $ end &lt;dttm&gt; 2018-03-23 00:25:51, 2018-03-26 02:15:52, 2018-03-2... ## $ time &lt;time&gt; 1.327778 hours, 1.758889 hours, 1.113889 hours, 1.6... You can see that we have a number of columns, but crucially, we have one where we kept track of the subject’s progress in our experiment. How many labels do we have for this? We can check for the unique labels in oru data using the unique() function. Alternatively, if our data is a factor, then it should have defined levels, so we could use the levels() function. To save time recoding our data types, we’ll just use the unique() function for now. unique(demo_sub$progress) ## [1] &quot;FINISH&quot; &quot;no&quot; &quot;END&quot; &quot;ethics&quot; NA We have a few labels: FINISH, no, END, ethics, and NA. It looks like we have a slightly messy data set, as both FINISH and END indicate the same point of progress; these subjects completed the experiment. If we just want to subset our data to those who finished the experiment, we could use these labels to filter out observations. 5.6.1 Filtering with Logical Operations demo_sub %&gt;% filter(progress == &quot;FINISH&quot; | progress == &quot;END&quot;) ## # A tibble: 23 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no ## 2 24 ENGLISH END female 30 GW 4 yes ## 3 27 ENGLISH END female 44 GW 5 no ## 4 A2 English END female 33 RHB 4 yes ## 5 A3 Other END male 20 RHB 3 no ## 6 C English END female 31 RHB 1 yes ## 7 D Other END non-binary 33 RHB 5 yes ## 8 I Other END male 36 RHB 5 no ## 9 J Other END female 28 RHB 6 yes ## 10 K English END male 33 RHB 1 yes ## # ... with 13 more rows, and 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, ## # time &lt;time&gt; Read the above command like, “filter to observations where the progress is equal to FINISH or the progress is equal to END”. Remember, to check if something is equal to another thing, we have to use double equals (==) rather than a single equal (=) as a single equal is an assignment operator. Finally, the pipe between these two statements is an OR operator, so we keep any observations that are equal to FINISH or equal to END. This type of filtering can get confusing if we have more than 2 conditions that we would like to check. One way around this is to use the %in% operator, which evaluates to TRUE if an observation is in a sequence that you provide. The best way to see how this works is to try it out yourself: demo_sub %&gt;% filter(progress %in% c(&quot;FINISH&quot;, &quot;END&quot;)) ## # A tibble: 23 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no ## 2 24 ENGLISH END female 30 GW 4 yes ## 3 27 ENGLISH END female 44 GW 5 no ## 4 A2 English END female 33 RHB 4 yes ## 5 A3 Other END male 20 RHB 3 no ## 6 C English END female 31 RHB 1 yes ## 7 D Other END non-binary 33 RHB 5 yes ## 8 I Other END male 36 RHB 5 no ## 9 J Other END female 28 RHB 6 yes ## 10 K English END male 33 RHB 1 yes ## # ... with 13 more rows, and 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, ## # time &lt;time&gt; You can see that we get the same result here, but we can easily add more and more values inside the parentheses to be evaluated. We can combine multiple filtering arguments by different columns, too. What if we wanted to get only those who completed the experiment, were tested by “RHB”, and are over the age of 30? 5.6.2 Combining Filtering Criteria demo_sub %&gt;% filter(progress %in% c(&quot;FINISH&quot;, &quot;END&quot;), tester == &quot;RHB&quot;, age &gt; 30 ) ## # A tibble: 9 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A2 English END female 33 RHB 4 yes ## 2 C English END female 31 RHB 1 yes ## 3 D Other END non-binary 33 RHB 5 yes ## 4 I Other END male 36 RHB 5 no ## 5 K English END male 33 RHB 1 yes ## 6 R1 English END female 31 RHB 6 yes ## 7 R3 English END female 35 RHB 0 no ## 8 V Other END male 37 RHB 7 no ## 9 W1 English END non-binary 31 RHB 1 no ## # ... with 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, time &lt;time&gt; 5.6.3 Removing by Ceriteria Similarly to keeping those that match some crtieria, we can remove those who meet some criteria. Let’s say we just want to look at those who haven’t finished the experiment. We can do this by using the ! (read: not) operator. Here, we ask to not keep those who have their progress as FINISH or END. demo_sub %&gt;% filter(!progress %in% c(&quot;FINISH&quot;, &quot;END&quot;)) ## # A tibble: 6 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23 &lt;NA&gt; no male 30 GW 6 yes ## 2 25 &lt;NA&gt; ethics male 18 GW 0 no ## 3 26 english ethics male 31 GW 4 no ## 4 28 &lt;NA&gt; ethics male 23 GW 0 no ## 5 29 english ethics male 34 GW 6 no ## 6 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes ## # ... with 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, time &lt;time&gt; 5.6.4 Handling NAs We always need to be wary of NAs in R. If we ask R whether something is equal to an NA (an unknown value) it is very literal in that it tells us it can’t know. That’s because R has no information about what the NA is! Try filtering our data to only those with NAs for the language known. # this will not run demo_sub %&gt;% filter(language == NA) ## # A tibble: 0 x 11 ## # ... with 11 variables: ID &lt;chr&gt;, language &lt;chr&gt;, progress &lt;chr&gt;, ## # gender &lt;chr&gt;, age &lt;int&gt;, tester &lt;chr&gt;, fun &lt;chr&gt;, recommend &lt;chr&gt;, ## # start &lt;dttm&gt;, end &lt;dttm&gt;, time &lt;time&gt; Instead, we have to surround our variable with the is.na() function, to check whether these values are NAs. demo_sub %&gt;% filter(is.na(language)) ## # A tibble: 4 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23 &lt;NA&gt; no male 30 GW 6 yes ## 2 25 &lt;NA&gt; ethics male 18 GW 0 no ## 3 28 &lt;NA&gt; ethics male 23 GW 0 no ## 4 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes ## # ... with 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, time &lt;time&gt; Also note that by default filter() excludes cases that don’t meet your criteria, or are NAs, so if you want to keep NAs you have to ask for them explicitly. Let’s look at filtering to only those who finished the experiment (FINISH or END) and those who we’re unsure about (NA). Here, we look for observations for progress that are equal to FINISH or equal to END or which are NAs. demo_sub %&gt;% filter(progress == &quot;FINISH&quot; | progress == &quot;END&quot; | is.na(progress)) ## # A tibble: 24 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 OTHER FINISH female 30 GW 7 no ## 2 24 ENGLISH END female 30 GW 4 yes ## 3 27 ENGLISH END female 44 GW 5 no ## 4 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes ## 5 A2 English END female 33 RHB 4 yes ## 6 A3 Other END male 20 RHB 3 no ## 7 C English END female 31 RHB 1 yes ## 8 D Other END non-binary 33 RHB 5 yes ## 9 I Other END male 36 RHB 5 no ## 10 J Other END female 28 RHB 6 yes ## # ... with 14 more rows, and 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, ## # time &lt;time&gt; Alternatively, we can filter our data to anything that is not an NA (or that does not meet any range of crieria). To do this, we use the ! logical operator (read this as “not”). Similarly, we can combine turn this around and ask to not keep those that match the criteria above. Here, we have to ask to throw away those with progress that isn’t FINISH and isn’t END, and we want to throw out those with an NA for their progress. demo_sub %&gt;% filter(!progress %in% c(&quot;FINISH&quot;, &quot;END&quot;) &amp; !is.na(progress)) ## # A tibble: 5 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23 &lt;NA&gt; no male 30 GW 6 yes ## 2 25 &lt;NA&gt; ethics male 18 GW 0 no ## 3 26 english ethics male 31 GW 4 no ## 4 28 &lt;NA&gt; ethics male 23 GW 0 no ## 5 29 english ethics male 34 GW 6 no ## # ... with 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, time &lt;time&gt; 5.7 Arranging Data Often, with psychological data, we want to order our columns by the subject number, then their observations. This makes it easy to think about how individuals progressed through the experiment. We can arrange our data using arrange() from dplyr. Again, this function takes the data, and then sorts by any columns that you give it. This function defaults to having lowest mnumbers first. So, if we sorted by subject ID and trial ID, you would get the lowest value for subjects first, and their lowest number for trials first. We have a simple case here, where we only have one observation for each participant. Here, numbers come before letters, but those with numbered IDs were tested at the end of the experiment. How, then, should we order the data? One option is to order the data such that IDs appear in descending order (using the desc() function). demo_sub %&gt;% arrange(desc(ID)) ## # A tibble: 29 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Z Other END female 28 RHB 0 no ## 2 W2 English END female 29 RHB 1 yes ## 3 W1 English END non-binary 31 RHB 1 no ## 4 V Other END male 37 RHB 7 no ## 5 T2 Other END male 27 RHB 0 yes ## 6 T1 English END male 22 RHB 4 no ## 7 S English END female 25 RHB 4 no ## 8 R3 English END female 35 RHB 0 no ## 9 R2 English END non-binary 27 RHB 5 no ## 10 R1 English END female 31 RHB 6 yes ## # ... with 19 more rows, and 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, ## # time &lt;time&gt; Alternatively, we know that the tester GW ran the experiment last, so we could sort by tester. But let’s say that we want to sort by who tested them first, and then by who started the experiment first. To do this, we simply need to provide the start and tester columns to the function. demo_sub %&gt;% arrange(tester, start) ## # A tibble: 29 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 27 ENGLISH END female 44 GW 5 no ## 2 24 ENGLISH END female 30 GW 4 yes ## 3 30 &lt;NA&gt; &lt;NA&gt; female 32 GW 1 yes ## 4 22 OTHER FINISH female 30 GW 7 no ## 5 26 english ethics male 31 GW 4 no ## 6 29 english ethics male 34 GW 6 no ## 7 25 &lt;NA&gt; ethics male 18 GW 0 no ## 8 28 &lt;NA&gt; ethics male 23 GW 0 no ## 9 23 &lt;NA&gt; no male 30 GW 6 yes ## 10 S English END female 25 RHB 4 no ## # ... with 19 more rows, and 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, ## # time &lt;time&gt; Again, we can flip the order for tester by using the desc() function. demo_sub %&gt;% arrange(desc(tester), start) ## # A tibble: 29 x 11 ## ID language progress gender age tester fun recommend ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 S English END female 25 RHB 4 no ## 2 J Other END female 28 RHB 6 yes ## 3 M2 Other END female 27 RHB 4 no ## 4 W2 English END female 29 RHB 1 yes ## 5 Z Other END female 28 RHB 0 no ## 6 R1 English END female 31 RHB 6 yes ## 7 R3 English END female 35 RHB 0 no ## 8 C English END female 31 RHB 1 yes ## 9 A2 English END female 33 RHB 4 yes ## 10 V Other END male 37 RHB 7 no ## # ... with 19 more rows, and 3 more variables: start &lt;dttm&gt;, end &lt;dttm&gt;, ## # time &lt;time&gt; Note: If we have missing values, these always come at the end of the table. 5.8 Summarising Data Finally, we get onto the most important section for psychologists; how to report summaries of your data. summarise() collapses across all observations in your data set to produce a single row of data. Within the summarise() function, we have to specify what we would like to create. To do this, we just use the same functions we would use to create a new column in our original data set. Let’s say we want to calculate the average time spent on the experiment. We just need to specify what our column will be called (mean_time here), and how we should create this column. Here, we’ve created it by calculating the mean() over the time column in our data. demo_sub %&gt;% summarise(mean_time = mean(time)) ## # A tibble: 1 x 1 ## mean_time ## &lt;time&gt; ## 1 1.44753831417625 Now we know that the average completion time was 1.45 minutes. However, this doesn’t tell us much. We might also want to know the standard deviation for the mean times, as well as the count (i.e. how many people completed the experiment). To do this, we just need to list more arguments. To add these summaries, we use the inbuilt sd() function, as well as the inbuilt n() function. We don’t pass anything to the n() function because it simply counts the number of observations over which we’ve summarised our data. You don’t need to understand the mutating part of this chain, but just know that it replaces the first cell in time with an NA. demo_sub %&gt;% summarise(mean_time = mean(time), sd_time = sd(time), N = n() ) ## # A tibble: 1 x 3 ## mean_time sd_time N ## &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 1.44753831417625 0.275443582881955 29 Finally, we have to be wary of NAs when calculating our statistics with summarise(). Here, I’ll introduce a missing value to our data before we calculate our summary. demo_sub %&gt;% mutate(time = replace(time, 1, NA)) %&gt;% summarise(mean_time = mean(time), sd_time = sd(time), N = n() ) ## # A tibble: 1 x 3 ## mean_time sd_time N ## &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 &lt;NA&gt; NaN 29 dplyr is strict, in that it will give you all NAs for the summary of a value if that value contains NAs. This is good as you’ll be aware if you have missing data before you report anything. To suppress this behaviour, we simply have to ask R to remove NAs when calculating the mean and standard deviation. To remove NAs, we need to explicitly set na.rm = TRUE in the functions we use. Read this as “Should I remove NAs? Yes!”. demo_sub %&gt;% mutate(time = replace(time, 1, NA)) %&gt;% summarise(mean_time = mean(time, na.rm = TRUE), sd_time = sd(time, na.rm = TRUE), N = n() ) ## # A tibble: 1 x 3 ## mean_time sd_time N ## &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 1.45181547619048 0.279515577285935 29 5.9 Grouping Data One of the most convenient functions in dplyr is the group_by function. This plays well with a number of the functions we’ve looked at above, but it’s most useful when calculating summaries. In this experiment, we cared about reaction times for those who are native and non-native English speakers. But did their completion times differ at all? To save on splitting our data and calculating summaries on the subsets of data, we can instead group our data by the language spoken, and pass these both to the summary() function. We do this using group_by(). demo_sub %&gt;% group_by(language) %&gt;% summarise(mean_time = mean(time, na.rm = TRUE), sd_time = sd(time, na.rm = TRUE), N = n() ) ## # A tibble: 6 x 4 ## language mean_time sd_time N ## &lt;chr&gt; &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 english 1.54986111111111 0.0473368706294328 2 ## 2 English 1.41704545454545 0.26597344132959 11 ## 3 ENGLISH 1.09083333333333 0.0326054793547131 2 ## 4 Other 1.48638888888889 0.317281110753131 9 ## 5 OTHER 1.32777777777778 NaN 1 ## 6 &lt;NA&gt; 1.60111111111111 0.258434767023681 4 As you can see, we got a summary by all of the groups in our language column. Unfortunately for us here, R doesn’t identify groups with the same name but different capitalisation as the same group. To fix this, we can chain the functions we’ve used above before calculating our descriptive statistics. I’ll fix the names by setting the language column values to the language column values, only with all of the text in lowercase (using tolower() from baseR). This effectively overwrites the old values with our new, improved naming scheme. demo_sub %&gt;% mutate(language = tolower(language)) %&gt;% group_by(language) %&gt;% summarise(mean_time = mean(time, na.rm = TRUE), sd_time = sd(time, na.rm = TRUE), N = n() ) ## # A tibble: 3 x 4 ## language mean_time sd_time N ## &lt;chr&gt; &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 english 1.39125925925926 0.260337334300721 15 ## 2 other 1.47052777777778 0.303311384194351 10 ## 3 &lt;NA&gt; 1.60111111111111 0.258434767023681 4 At a glance, there doesn’t seem to be much of a difference in the completion time for the English and Other groups. However, for the NA group, subjects seem to take a little longer than the other two groups. That’s perhaps something we would look into if we wanted to analyse their data. Finally, we can use group_by() along with other functions, such as filtering our data to the two participants with the lowest times from the language groups. Let’s do that now. To do this, we use another function from baseR called rank(). This rank orders all scores from the smallest to the largest value. Thus, if we group by language and filter to times with a rank of 1, we will filter to the lowest time in our language groups. demo_sub %&gt;% mutate(language = tolower(language)) %&gt;% group_by(language) %&gt;% filter(rank(time) == 1) %&gt;% glimpse() ## Observations: 3 ## Variables: 11 ## $ ID &lt;chr&gt; &quot;27&quot;, &quot;30&quot;, &quot;J&quot; ## $ language &lt;chr&gt; &quot;english&quot;, NA, &quot;other&quot; ## $ progress &lt;chr&gt; &quot;END&quot;, NA, &quot;END&quot; ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot; ## $ age &lt;int&gt; 44, 32, 28 ## $ tester &lt;chr&gt; &quot;GW&quot;, &quot;GW&quot;, &quot;RHB&quot; ## $ fun &lt;chr&gt; &quot;5&quot;, &quot;1&quot;, &quot;6&quot; ## $ recommend &lt;chr&gt; &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot; ## $ start &lt;dttm&gt; 2018-03-21 03:23:57, 2018-03-22 04:16:08, 2018-03-2... ## $ end &lt;dttm&gt; 2018-03-21 04:28:01, 2018-03-22 05:29:05, 2018-03-2... ## $ time &lt;time&gt; 1.067778 hours, 1.215833 hours, 1.073056 hours You can see that we have times of around 1 hour for each group, which is below the means we calculated before. 5.9.1 Ungrouping Data Finally, if you want to perform further operations on a set of data after you’ve initially grouped it, you can ungroup the data using ungroup() prior to running further calculations. Below, we remove the quickest people in each group before calculating the grand mean. demo_sub %&gt;% mutate(language = tolower(language)) %&gt;% group_by(language) %&gt;% filter(rank(time) != 1) %&gt;% ungroup() %&gt;% summarise(mean_time = mean(time, na.rm = TRUE), sd_time = sd(time, na.rm = TRUE), N = n() ) ## # A tibble: 1 x 3 ## mean_time sd_time N ## &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 1.4854594017094 0.264484846847179 26 5.10 Chaining Many Functions As you saw before, our data wasn’t exactly tidy when we first loaded it. We can avoid a lot of intermediate steps, and make some of the processes we’ve went through above if we just chain our functions together. Here, we’ll tidy up our data before calculating some descriptive statistics. # load package library(lubridate) # load, clean, and transform data demo_clean &lt;- read_csv(&quot;inputs/lexical_decision_demographic_data_wide.csv&quot;) %&gt;% separate(col = funRec, into = c(&quot;fun&quot;, &quot;recommend&quot;)) %&gt;% separate(col = completion_time, into = c(&quot;start&quot;, &quot;end&quot;), sep = &quot;_&quot;) %&gt;% select(c(1, 8:16)) %&gt;% rename(language = LANGUAGE) %&gt;% mutate(start = ymd_hms(start), end = ymd_hms(end), time = end - start, language = tolower(language) ) In one step, we’ve used several of the commands that we used above to clean our data and to get it in the correct format for what we want to do with it. We can then pass this cleaned data to our group_by() and summarise() functions to generate summaries as before. demo_clean %&gt;% group_by(language) %&gt;% summarise(mean_time = mean(time, na.rm = TRUE), sd_time = sd(time, na.rm = TRUE), N = n() ) ## # A tibble: 3 x 4 ## language mean_time sd_time N ## &lt;chr&gt; &lt;time&gt; &lt;time&gt; &lt;int&gt; ## 1 english 1.39125925925926 0.260337334300721 15 ## 2 other 1.47052777777778 0.303311384194351 10 ## 3 &lt;NA&gt; 1.60111111111111 0.258434767023681 4 Or we can even plot the data using the pipe (%&gt;%) with ggplot2. Notice that the ggplot elements end the line in a + not a %&gt;%. demo_clean %&gt;% ggplot(mapping = aes(x = language, y = time)) + geom_boxplot() ## Don&#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous. There are some helper functions we haven’t covered here that might help you out with dplyr, but you should have most of what you need now to do the majority of your data processing tasks. 5.11 Saving Data Finally, we should look at saving our data. There are a few ways we can do this. 5.11.1 CSV Files One way is to save your data as a .csv file, in a similar format to the data you loaded into R. However, this way we lose any customised data types (such as start and end times being POSIXct) if we are to load the data back up into R. Still, it’s always good to have a copy of your data that is platform agnostic, so we should save it as a .csv so other people can load it up with any program they like. This format is also good for exploring your data in excel. We’ll save our data in the outputs folder as a csv, using the write_csv() function from readr (Wickham, Hester, and Francois 2017), a part of the tidyverse (Wickham 2017). To save the data, we just need to specify the object to save (the table of your data) in the write_csv() function, along with the path to save the data. Here, we’ve specified that we want to save the data in the ouputs folder, and then given it a name. Folders are separated by a forwardslash, with the last string of text representing the file name. File names must end with .csv to specify the file type. write_csv(demo_clean, &quot;outputs/filtered_demographic_data.csv&quot;) We could also save the data as an excel file, but it’s generally best to keep platform agnostic for your data backups. If you want to save your data in a useful format for further manipulation, save it as an RData file! That way, you can keep using R to work with your objects. 5.11.2 R Data Files You don’t want to re-run all of your pre-analysis code every time you want to make a graph/model data, so saving your data after cleaning it up is generally a good idea. However, some of the R-specific information can get lost when saving as a .csv or other format (e.g. dates will be re-loaded as characters unless otherwise specified). Instead, we can save as an RData file which retains this information. Additionally, RData files generally take up less space on your hard drive than a .csv. Finally, we can save object types which don’t play well with .csv and other formats in an RData file, such as model outputs and nested data sets (which is pretty advanced; think of a table of data within each cell of a larger table). Here, we will save our data as an .RData file, which we can then load up again in R for further processing. To do so, we need to specify the object(s) to save (we can save several in one data file!), and we need to specify the file as above. Here, we need to be specific by using the file argument to define where to save our data. save(demo_clean, file = &quot;outputs/filtered_demographic_data.RData&quot;) When it comes to loading RData files back up, we just need to use the load() function from baseR. Bear in mind that the names of the objects you’ve saved in the data file will be the same ones you see on loading, so use informative names! Here, we just need to specify the path from which to load our data. load(&quot;outputs/filtered_demographic_data.RData&quot;) 5.12 Exercises We’ll try some exercises to get you used to using all of the functions we’ve discussed here. For these exercises we’ll a simulated data set I created in the data_generation folder. This data was saved as a tibble in the RData file, so we just use load() to open this data into R. # load libraries library(tidyverse) library(lubridate) # load data load(&quot;inputs/sim_data.RData&quot;) This data set looks at reaction times in spotting a target when people have had caffeine or not, and when they respond with their dominant or non-dominant hand. This data set is entirely made up, so we can’t be sure how realistic these results are. First, you should take a look at your data to understand it. A simple first step is to see what the data table itself looks like. glimpse(data) ## Observations: 100 ## Variables: 5 ## $ subject &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... ## $ age &lt;int&gt; 34, 56, 23, 53, 44, 21, 55, 47, 29, 31, 35, 56, 34, 6... ## $ caffeine &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, n... ## $ response &lt;fct&gt; non-dominant, non-dominant, non-dominant, non-dominan... ## $ DV &lt;dbl&gt; 398.6627, 396.3824, 400.1234, 401.9182, 397.6403, 398... After this, we should make a more detailed plot to understand the general trends in the data. 5.12.1 Question 1 Make a plot of your data which looks at the relationship beween caffeine and DV. Split this plot into two to show the differences across the two response conditions. What are the general trends in the data? 5.12.2 Question 2 Subset your data to remove the ages from the data set. 5.12.3 Question 3 Rename the DV column to something more informative, like reaction_time 5.12.4 Question 4 What if we care about differences in the ages? Let’s assume we have a prediction that caffeine only has an effect on those above 30 years of age. Subset your data to just those above 30 years of age. 5.12.5 Question 5 Rearrange the data set by age, starting from the highest age. 5.12.6 Question 6 Calculate mean centered scores for each subject and add these to a new column called DV_c (DV, centered). The formula for this is subjects_score - mean(all_scores). Can you work out what the mean should (approximately) be? 5.12.7 Question 7 Let’s assume we have a prediction that response times should be slower above 30 years of age. Create a new column, age_group that puts participants into two groups 30_or_under, and above_30. Hint: Look up the ifelse() function using ?ifelse() to see how you can use logical operations to achieve this. 5.12.8 Question 8 Calculate the mean, standard deviation, and number of observations for each group. 5.12.9 Question 9 Calculate the mean, standard deviation, and number of observations for each group, excluding those with the 3 highest ages from each group. 5.12.10 Question 10 Take the data and do this all together: Rename the DV column to response_time. Remove any observations where the age of the participant is above 60. Combine the two columns, age and caffeine, into one column called condition. Hint: Use paste() here. Use an underscore separator for the condition names. Remove the caffeine and response columns and reorder the data so we have subject first, followed by age, condition, and response_time. Calculate mean, standard deviation, and n for condition on the response time column. Call your new column names anything sensible. 5.12.11 Question 11 Was there any point in us combining the two factors into a single condition column? Do the same process above without making a summary of the data. Feed this data into a boxplot, with condition on the x-axis, and response_time on the y-axis. References "],
["traditional-tests.html", "Chapter 6 Traditional Tests", " Chapter 6 Traditional Tests Basic parametric and non-parametric analyses. Content to follow. "],
["simulation-and-calculating-power.html", "Chapter 7 Simulation and Calculating Power", " Chapter 7 Simulation and Calculating Power How to simualte data and perform power analyses. Content to follow. "],
["mixed-effects-models-1.html", "Chapter 8 Mixed Effects Models 1", " Chapter 8 Mixed Effects Models 1 Lesson One of Linear/Hierarchical Mixed Effects Modelling. Content to follow. "],
["mixed-effects-models-2.html", "Chapter 9 Mixed Effects Models 2", " Chapter 9 Mixed Effects Models 2 Basic non-parametric tests. Content to follow. "],
["creating-reproducible-documents.html", "Chapter 10 Creating Reproducible Documents 10.1 The Final Lesson", " Chapter 10 Creating Reproducible Documents How to create Rmarkdown documents for reproducible documentation of analyses. Content to follow. 10.1 The Final Lesson Congratulations, you’ve made it! "],
["references.html", "References", " References "]
]
