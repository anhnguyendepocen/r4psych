[
["index.html", "R for Psych Preface &amp; Overview", " R for Psych Glenn Williams 2018-02-21 Preface &amp; Overview In this course you’ll learn how to use R for data analysis and presentation. This course has a particular focus on using R for psychology (hence the name), but it should be applicable to most cases in the social sciences. Here, we’ll primarily take a tidyverse first approach to R. This means we’ll be relying primarily on a collection of packages designed to get things done quickly, with highly readable syntax. We will still cover some of the base R functions and the basics of programming, but our aim is to quickly and efficiently use R for managing and processing data. Along the way, we’ll look into how R encourages open and reproducible science, and how R can be useful for managing your research projects as a whole. By the time you’re finished, you will be able to tell R how (and where) to read input files (e.g. raw data from an experiment), how to perform operations on your data (e.g. data wrangling and aggregation), and how to produce and save ouputs based on your data (e.g. graphs and test statistics). You’ll also be able to produce documents that incorporate your R code with formatted text so every time you update your code, your written statistics, tables, and graphs update automatically. For this, we’ll explore R-markdown, specifically using R notebooks. We will rely on the R for Data Science (R4DS) textbook by Garrett Grolemund and Hadley Wickham as a core text. Follow the link for a free online version of the book uploaded by the authors. R4DS by Wickham &amp; Grolemund The above book assumes some familiarity with programming and/or R at the outset, but covers the basics in Chapter 4. As in this course, the aim here is to get you doing productive things (e.g. producing a graph) as quickly as possible. If, however, you feel like you’d prefer a basic grounding in R prior to doing so, you can check out R for Beginners by Emmanuel Paradis. This is a short introduction to all of the core concepts required for working with your data in R (including reading, manipulating, and saving data). As this course focuses on using R for psychologists, we’ll cover a range of traditional parameteric and non-parametric analyses, such as: Correlations t-tests ANOVA We will also cover topics such as power analysis, particularly using simulation based methods (which are scalable for any set of tests), before we move on to more advanced methods, such as hierarchical mixed effects modelling (or linear mixed effects models; LMM). Throughout, we will use examples and assignments to entrench the concepts taught in this module. Finally, we will focus on creating reproducible analyses and write-ups using R markdown. To join the rest of your class in discussing this course, please join the R @ Abertay Slack channel. We will use this channel for all communications about the course, including help, hints, and tips about the course content. To download the course content, get the lesson materials from my GitHub repo. You can follow along with the slides using the R notebooks. "],
["introduction.html", "Chapter 1 Introduction 1.1 Installation and Setup 1.2 Working Directory and File Paths 1.3 Packages 1.4 Objects and Functions 1.5 Creating and Accessing Data 1.6 Good Practice 1.7 Exercises", " Chapter 1 Introduction To begin with, we’ll focus on getting you started in R. We’ll look into installing our two key programs, R and RStudio. After that, we’ll look into how R can communicate with files on your system. Finally, we’ll look at the core packages for this course, how to do basic data manipulation using base R (R Core Team 2017), and tips for good practice when working with R. We’ll complete some exercises at the end so you can get a handle of the key concepts introduced here. 1.1 Installation and Setup To get started, at the very least you’ll have to download R from CRAN. Select the correct distribution for your operating system and then click through to install R for the first time. For Windows users, you’ll see a new page; at the top click on “Download R [version number] for Windows.” Click on that and follow the prompts in the installer. If you have a 64 bit system, install the 64 bit version of R as you’ll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you’re working with very large data sets.) In R, you don’t want to type commands directly to the console. Instead, you should use a text editor to write your script and send it to the console as and when you want. This way, you’ll have a document of what you did, and any minor changes you have to make are very easy to implement (vs. writing it all out from scratch again). One of the most popular ways to work on your R scripts is to use an Integrated Development Environment (IDE). The most popular IDE for R is RStudio, which you can download from the RStudio website. In the navigation panel, select products and choose RStudio. Scroll down until you see Download RStudio Desktop. Finally, for the Free tier, click the Download button and select the correct installer for your operating system. For Windows users, you should select RStudio [version number] - Windows Vista/7/8/10. Follow the prompts on the installer, and your system should automatically pick up that you already have R on your system. From there, be sure to open the RStudio program to get started. This is the logo that looks like this: The RStudio logo Once open, you’ll see something that should look like this. Only, you won’t have the top pane unless you choose to start a new script (File, New File, R script) or if you’ve opened an existing script already. The RStudio environment, with added highlights RStudio can be split into 4 main panes: The editor: Type, edit, and save scripts as you would in any text editor. The console: Execute scripts by typing and pressing enter. The environment and history: View objects (e.g. values, variables, and user-defined functions etc.) stored in memory for this working session. You can also see a history of your commands in the History tab. The viewer: view any files in your working directory, see your last plot from this session, view installed packages on your machine (you can also load them here), view the help documentation for any R commands, in the viewer you can view (surprise, surprise) markdown/other documents. All of these panes are very useful when developing your R scripts, but RStudio has some other features such as syntax highlighting, code completion, and an easy interface for compiling your reproducible R-markdown notebooks. For advanced users, you can also set up projects that play nicely with Github for version control of your work. Finally, RStudio defaults to giving you the option to save your workspace (e.g. all of the data you’ve created) when you close the program and to restore this workspace once you restart RStudio. I’d advise you to change RStudio’s defaults to never do this as this could cause unforseen errors when, for example, you want to change parts of script and you accidentally delete the stage to create a variable that is crucial later on. This deletion may not be obvious if you reload your workspace, but you won’t have a record of how you created said variable. To make these changes, go to Tools, Global Options then deselect Restore .RData into workspace at startup and from the dropdown menu on Save workspace to .RData on exit: to Never. 1.2 Working Directory and File Paths When you create an R file (File, New File, R Script), where that file sits is its working directory. This is where R interprets any commands that go outside of your R session. For example, if you want to read data into R, or output a graph, R will do so in respect to the working directory. You can get your working directory like so: getwd() You should see something along the lines of “C:/Users/YOUR_NAME/FOLDER/RFILE.R” Now, you can set your working directory to any folder on your computer with an absolute file path. Often, people use absolute file paths in order read inputs and send outputs to different folders on their computer, telling R exactly where to look. However, I recommend against this. That’s because if you change computers or pass your code over to someone else R will kick out an error saying it can’t find that folder; other computers are highly unlikely to have the same folder structure or even username as you, and fixing this can be frustrating. Also, it can be a pain having to type out your full file path, so let’s avoid that. Below, I’ve outlined one method for working that will allow you to use relative filepaths (relatively) easily. If you want to keep your folder from getting cluttered, keep your R script(s) in a folder, with an Inputs and Outputs folder at that same level, like so: Potential Folder Structure for Your Projects This way, when you want to read in raw data, or output a graph (or something else) you can use a relative file path rather than an absolute file path to define where your files are taken from/should go. This saves you a lot of typing of file paths, and it means your R scripts will work on someone else’s computer if you just send the entire folder containing the R script, Inputs, and Outputs folders. To read in or save data using a relative file path, do this: # reads a CSV file from the Inputs folder my_data &lt;- read.csv(&quot;Inputs/my_data.csv&quot;) # writes a CSV file to the Outputs folder my_data &lt;- write.csv(my_data, &quot;Outputs/my_data_again.csv&quot;) You just have to type in the folder name and a slash (to indicate to go below that folder) and the name of your data. This saves you from all of the hassle of using an abosolute file path described above. 1.3 Packages Next, while you can do a lot in base R, if you can think of some task that is reasonably laborious there’s probably a package out there that can help you to achieve a certain goal. For us, the whole data processing, analysis, and presentation workflow can be made so much simpler by using a set of packages from the tidyverse library (Wickham 2017). This package is required for what we’re about to do next, so install tidyverse (using install.packages(&quot;tidyverse&quot;)). Once installed, you needn’t do it again. But on each new session you have to define which packages you want to load into R using the library() command. Make sure to uncomment the install.packages(&quot;tidyverse&quot;) line if you haven’t installed this package yet. # install.packages(&quot;tidyverse&quot;) # do this only once to install the package library(tidyverse) # do this to load your package every time you open R ## -- Attaching packages ---------------------------------- tidyverse 1.2.1 -- ## v ggplot2 2.2.1 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.2.0 ## v readr 1.1.1 v forcats 0.2.0 ## -- Conflicts ------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() By default, R installs packages from CRAN, which is essentially a centralised network for all things R. However, some useful packages may not have been submitted here, and you can install them from other places, like GitHub. We won’t install packages from github or other repositories in this course. But it’s worth knowing that you can install packages from just about anywhere! Most of the time, you won’t have any trouble using functions from a loaded package. However, there can be cases when you have two packages installed that use the same function name. To tell R exactly which version of a function to use, we can specify both the package and function name in the form package::function_name(). For example, we can use the group_by() function from the package dplyr by typing dplyr::group_by(). You won’t come across this in this course, as we’ll be using packages that have functions with unique names, but it’s worth bearing in mind if you come across problems with functions you know should work in the future. Note: You may notice that in my code chunks I have some green text following a #. These are comments in R and are not read when you execute your code. Comments are very useful for telling you what each section of your code does, and how. I tend to comment quite heavily, as there’s always a chance you’ll forget what you’ve done when you go back to it after a few months away from the code. 1.4 Objects and Functions You can use R like a calculator, e.g. 1 + 4 ## [1] 5 You can execute this directly in the console, or run it from your R script by selecting that line of code (or highlighting several lines) and pressing Ctrl + Enter (for mac, this is probably Cmd + Enter). While I said you shouldn’t use the console when writing scripts, you can use it to test out a bit of code, or to quickly use R as a calculator when you’re in a meeting and feeling like mental arithmetic is a step too far. You can see that once the code is run, the R console returns the result back to you. You see a history of what you asked, and what was returned. Top tip: use the up arrow key from the console and R will automatically fill in the last line/block of code you ran. Press up again to cycle back to older inputs, and down to back to the most recent ones. R always waits for you to finish an expression before it runs your code. So, if you ended your line with a +, it’ll wait for the next number. This is useful for complex expressions that can take up multiple lines, e.g. 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 ## [1] 45 Watch the console when you type this out. You’ll notice that if you press enter after typing 7 + on the new line you will no longer see &gt; but you’ll see +. The same happens even if you pass a different mathematical operator (e.g. -, %, ^). This is there to tell you that R is not waiting for a new statement, but is waiting for you to finish off the current statement. If you see &gt; it means that whatever you can start a new statement. R also parses text if included in quotes. The same rule applies about finishing expressions here; if you don’t close your quote, then R will wait for you to do so. This means you can spread your text over several lines (by pressing Enter) and R will parse that as one expression. Note with our output we get which indicates that a new line follows the comma. &quot;Hello, world!&quot; ## [1] &quot;Hello, world!&quot; &quot;Hello, world (except you Donald Trump, nobody want&#39;s to talk to you)&quot; ## [1] &quot;Hello, world (except you Donald Trump,\\n nobody want&#39;s to talk to you)&quot; Crucially, you can store this sort of information in a variable. Variables are useful because you may want to use the results of a calculation and do some further operations on that result. This is especially useful if you’re not sure what that first result could be. We assign values to a variable using the assignment operator &lt;- (read this as create from). summed_numbers &lt;- 5 + 8 We can then output this variable later on, like so: (This is often useful for checking that your variables store what you think they store.) summed_numbers ## [1] 13 Or we can perform operations on the variable. summed_numbers*5 ## [1] 65 Note: You cannot start variables with a number, you cannot use special characters (e.g. %!*) and you cannot include spaces in your variable name. Also, note that your capitalisation matters, so Summed_numbers is not summed_numbers. (I often get errors due to this issue…) R has the simple arithmetic operations you’d expect from any program. For example, you can: add x + y, subtract x - y, multiply x * y divide x/y, exponentiate x^y find the modulus x %% y, (e.g. 5 mod 2 = 1; i.e. the remainder from how many times 2 goes into 5) and conduct integer division x %/% y (e.g. 5 int div 2 = 2) R also has some logical operations built in. For example: less than x &lt; y less than or equal to x &lt;= y greater than x &gt; y greater than or equal to x &gt;= y exactly equal to x == y not equal to x != y not x !x x OR y x | y x AND y x &amp; y test if X is true isTRUE(x) These come in pretty handy for performing most operations on our data. If you’re unfamiliar with these, don’t worry. We’ll cover how you might use some of these in a staggered format as you progress through this course. Nicely, R also has a number of functions built in. This means you don’t need to write your own function if you want to sum a sequence of numbers. I’m sure I couldn’t provide an exhaustive list here, but as stated above we’ll cover most of the common functions as you get to grips with some data. Still, lets get an idea of how these functions work. Below, we will sum the numbers 1 to 4. This function is built into R already, so we don’t have to write a whole lot of code for this. sum(1, 2, 3, 4) ## [1] 10 What if we want to calculate the mean score from this set of numbers? That’s also built into R. mean(1, 2, 3, 4) ## [1] 1 Notice that whenever we want to run a function, these functions always have a name and are followed by parentheses (e.g. mean(), sum()). What goes in the parentheses? The argument you want to pass to the function. These can have default values, or not (requiring you to specify the argument). Above, we passed the values 1 through 4 as the arguments to the mean() function. Later, we’ll look at functions that ask for arguments from separate data types (e.g. numbers and characters). If you’re unsure what an argument does, you can always ask R what it does, how it does it, and what to pass to it by using ?, e.g. ?mean(). This will bring up a document in the Help window of RStudio. Typing out all of these numbers each time we want to perform some operation is a little tedious. To fix this, we can use what we learned above and store these numbers into a variable. In order to save these into their own variable, we use the c function. Think of this as concatenation. When we combine values into a variable, this variable is stored in our global environment. This means that we can perform operations on the variable later on, without the worry of typing our the values again. This is particularly useful if you want to store values from one function (say a statistical test) that you cannot pre-define but that you want to use later on. Let’s give that a go. First, we’ll concatenate our values into a variable using the c() function described above. Here, we simply define that we want to concatenate some values, and list each value separated by a comma. stored_values &lt;- c(1, 2, 3, 4) If we then call a function that works with a set of numbers, it should also work if we call that function on the variable storing those numbers. Let’s see how this works with the mean() function. mean(stored_values) ## [1] 2.5 Great! That’ll save is a lot of time writing and rewriting code later on. This also allows our code to be flexible, in that we can write a script that performs operations on variables that can take any range of values. This, to me, is one of the nicest things about doing your analyses in R. While you may spend more time getting your script up and running in the first place when compared to using point-and-click methods (e.g. in SPSS), if you gain new data or run a new experiment, it’s likely that your script can simply be re-run with no (or few) changes at very little cost to your time. Now, this part is pretty important but may only be obvious if you’ve programmed in other languages. R is a vectorised language, which means that, as with the sum() function above, R can perform operations on the entire variable So, if you want to increment all values in your variable by 1, you can simply tell R to do so in one line of code, without the need for loops or other complex methods. stored_values + 1 ## [1] 2 3 4 5 1.5 Creating and Accessing Data So, you’ve made it through all of the boring stuff. Now we can look at manipulating some data in R. Let’s pretend we’ve administered IQ tests to 100 participants. For some reason, we’re interested in whether dog and cat owners have different IQs. I’m sure some of you can come up with some incredibly biased hypotheses here, but we’ll leave that out for now. We’ll create this data in R, then take a look at it using some of the packages from the tidyverse library that you installed and loaded above. If the functions below don’t work, be sure to load the tidyverse library (using library(tidyverse)) before running the code below. First, we’ll create a variable containing the numbers 1 to 100, using the seq() function; this can be our participant ID variable. Next, we’ll use the sample() function to sample randomly (and with replacement) from the two labels “cat” and “dog” which acts as our factor for pet ownership. (Note that here I use the sample() function on the concatenated values of “cat” and “dog”, but this would work equally well with a variable containing these labels.) Finally, we’ll use the rnorm() function to generate some random numbers (sampled from the normal distribution) with a mean of 150, and a standard deviation of 15 to act as our IQ scores. Remember how I said we’d look at functions that take several arguments? Well, thats what all of these functions below do. We define each parameter of the argument within the function call. So, if we want to generate a sequence of numbers from 1 to 100 , in increments of 1s (e.g. seq(from = 1, to = 100, by = 1)), then we define each argument with its name (e.g. from/to/by) and tell R which values to set for these arguments using =. # create participant IDs participant &lt;- seq(from = 1, to = 100, by = 1) # create pet ownership codes set.seed(88) pet &lt;- sample(c(&quot;cat&quot;, &quot;dog&quot;), 100, replace = TRUE) # create IQ scores set.seed(88) IQ_score &lt;- rnorm(n = 100, mean = 150, sd = 15) IQ_score &lt;- round(IQ_score) # round IQ scores Note: I’ve overspecified the functions above. You can simply run seq(1: 100) and rnorm(100, 150, 15) to get the same results, but it’s a little less readable. Here, and throughout, I’ll use the most readable version so you have the best idea of what you’re doing. Because we’re using random sampling, I’ve also set a seed, so that you’ll sample the same values as me. If you want people to be able to reproduce your random sampling so they get the same data, set a seed! Finally, it’s important to note that you can call a function on the result of another function in R. Right now, each number in the participant variable is unique. We can see how many participants are in the study by asking R “how long is the variable, participant?”. We do this like so: length(participant) ## [1] 100 Cool, we have 100 participants! But what if we have several observations for one participant? I’ll define that (very simply) in the code below: participant[2] &lt;- 1 # assign the second value of participant the integer 1 head(participant) # return the first 6 values of the participant variable ## [1] 1 1 3 4 5 6 Now, using length(participant) will return 100, because there’s still 100 values in the participant variable. How do we find how many unique numbers are in the participant variable? We simply nest the unique() function within the length() function: length(unique(participant)) ## [1] 99 This asks R how many unique values are in the variable participant. That’s 99! 1.5.0.1 Data Frames and Tibbles Now, in the real world, if you tested IQs you’d typically have this data stored in a table somewhere prior to reading it into R. So lets pair the data together into a table in R. The most useful way to do this is to create a data.frame. But, since we’ve already loaded the tidyverse, we may as well use the built in tibbles. These are like data frames, but they have some different defaults that make working in the tidyverse easier. IQ_data &lt;- tibble::tibble( participant_number = participant, pet_id = pet, IQ = IQ_score ) What’s the main advantage of using a tibble over a data.frame here? Tibbles don’t automatically convert certain data types. With data.frame, character data is automatically converted to a factor. This can be useful, but not always. The thing I like most is that tibbles show the data type under the column name. This is taken from str() which tells you the structure of your data. Also, tibbles default to printing the first 10 rows of data when you type their name in the console. This will stop you from getting overwhelmed by a massive printout if you have a lot of data. Let’s see all of this in action. IQ_data ## # A tibble: 100 x 3 ## participant_number pet_id IQ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.00 cat 147 ## 2 1.00 cat 160 ## 3 3.00 dog 185 ## 4 4.00 cat 122 ## 5 5.00 dog 157 ## 6 6.00 dog 152 ## 7 7.00 cat 152 ## 8 8.00 dog 119 ## 9 9.00 dog 131 ## 10 10.0 dog 160 ## # ... with 90 more rows If you want more than 10 rows, or a specific number of columns, tell R that’s what you’d like. Here, we’ve asked for the first 12 rows, and the width of the table to be infinity so that R prints out all columns even if they don’t fit on 1 row in the console. However, with only 3 columns, that isn’t an issue here. print(IQ_data, n = 12, width = Inf) ## # A tibble: 100 x 3 ## participant_number pet_id IQ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.00 cat 147 ## 2 1.00 cat 160 ## 3 3.00 dog 185 ## 4 4.00 cat 122 ## 5 5.00 dog 157 ## 6 6.00 dog 152 ## 7 7.00 cat 152 ## 8 8.00 dog 119 ## 9 9.00 dog 131 ## 10 10.0 dog 160 ## 11 11.0 dog 138 ## 12 12.0 dog 165 ## # ... with 88 more rows Unfortunately, some older functions in R won’t allow you to use a tibble. If this is the case, simply convert your tibble to a data.frame using the as.data.frame() function. Note, we use head() to see the head of our data frame, or the first 6 values. This is necessary here to avoid printing out each row, as we’re not in using a tibble any more. Notice that We’ve assigned the data.frame version of our IQ data to a new variable, rather than overwriting the previous variable. This is good practice when testing your code, as you never know what might break, resulting in data loss. (Although this wansn’t strictly necessary here.) IQ_data_df &lt;- as.data.frame(IQ_data) head(IQ_data_df) ## participant_number pet_id IQ ## 1 1 cat 147 ## 2 1 cat 160 ## 3 3 dog 185 ## 4 4 cat 122 ## 5 5 dog 157 ## 6 6 dog 152 1.5.0.2 Accessing Tibble Information With tibbles and data.frames, you often want to access a specific column in order to perform some calculation. Let’s say we want to calculate the mean IQ score in our data set. Why doesn’t the code below work? mean(IQ_data) ## Warning in mean.default(IQ_data): argument is not numeric or logical: ## returning NA ## [1] NA R gives us a warning, saying that the argument to our function (i.e. the tibble, IQ_data) is not numeric or logical. Therefore, R cannot calculate the mean on something that isn’t a number (or set of numbers). Instead, we have to point R to the correct column within the tibble. This is often done by name using $ , or by name/position in the tibble using [[ and the name/position followed by ]]. First, let’s see that we’re accessing everything correctly here. # by name IQ_data$IQ IQ_data[[&quot;IQ&quot;]] # by position IQ_data[[3]] ## [1] 147 160 185 122 157 152 Tibbles are stricter than data frames, in that they’ll always return a tibble. For example, with a data.frame if you ask for a value in a specific row of a specific column under the format data_frame[row, column], R will return a vector of the value at that position in the data.frame. With tibbles, your single value will be returned as a tibble. # tibble output IQ_data[1, 3] # row 1, col 3 ## # A tibble: 1 x 1 ## IQ ## &lt;dbl&gt; ## 1 147 # data.frame output IQ_data_df[1, 3] ## [1] 147 Now, lets calculate the mean on the correct column in the tibble. Personally, I prefer using $ for cases like this. But, feel free to use any of the above methods. mean(IQ_data$IQ) ## [1] 150.17 We can combine what we learned above about c() to access multiple columns at once: IQ_data[, c(&quot;participant_number&quot;, &quot;IQ&quot;)] ## # A tibble: 100 x 2 ## participant_number IQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 147 ## 2 1.00 160 ## 3 3.00 185 ## 4 4.00 122 ## 5 5.00 157 ## 6 6.00 152 ## 7 7.00 152 ## 8 8.00 119 ## 9 9.00 131 ## 10 10.0 160 ## # ... with 90 more rows Or we can access a range of values by specifying the rows we’d like to access: IQ_data[c(1, 2, 3, 4, 5), c(&quot;participant_number&quot;, &quot;IQ&quot;)] ## # A tibble: 5 x 2 ## participant_number IQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 147 ## 2 1.00 160 ## 3 3.00 185 ## 4 4.00 122 ## 5 5.00 157 or, to save typing, we can just define this from a starting to an ending value for the rows: IQ_data[1:5, c(&quot;participant_number&quot;, &quot;IQ&quot;)] ## # A tibble: 5 x 2 ## participant_number IQ ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 147 ## 2 1.00 160 ## 3 3.00 185 ## 4 4.00 122 ## 5 5.00 157 In Session 4 we’ll look at more intuitive ways of accessing data from columns and rows in your data frames. However, for now we’ll quickly look at ways to manipulate data using the base R functionality so you have some idea how indexing works in R. 1.5.0.3 Manipulating Tibble Information 1.5.0.3.1 Changing Values We can use these same principles to edit the information within our data. Say, we’d like to change participant number in row 2 back to 2, we just need to do this: # specifying row and column by index IQ_data[2, 1] &lt;- 2 # specifying row by index and column by name IQ_data[2, &quot;participant_number&quot;] &lt;- 2 # specifying index within a column IQ_data$participant_number[2] &lt;- 2 You can see how all of this combined can result in some pretty powerful flexibility in how you access and manipulate your data in R. 1.5.0.3.2 Adding and Removing Columns To add a row to a data frame, we simply need to specify what we want to add and assign it a new name. Let’s say that we want to add a column that indicates the operating system used by each participant. We may have this because we made assumptions that people who use Windows, macOS, or the Linux families of operating systems differ in their IQ. This is a silly example for several reasons, not only because you can use more than one system; but we’ll stick with this for now. Imagine we already have a sample of operating systems to draw from. You don’t need to understand how this works, but briefly I’ve used the inbuilt sample() function to pick from the three names with replacement, skewing the probabilities to select windows most often, followed by mac, then linux. All that matters is that we’re assigning 100 names to a variable. set.seed(1000) # make the sampling procedure the same for us all operating_system &lt;- sample(c(&quot;windows&quot;, &quot;mac&quot;, &quot;linux&quot;), size = 100, replace = TRUE, prob = c(0.5, 0.3, 0.2) ) In the IQ_data set, we can add a new column for the operating systems used by the participants like so: IQ_data$operating_system &lt;- operating_system # add new column head(IQ_data) # view first 6 rows ## # A tibble: 6 x 4 ## participant_number pet_id IQ operating_system ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.00 cat 147 windows ## 2 2.00 cat 160 mac ## 3 3.00 dog 185 windows ## 4 4.00 cat 122 mac ## 5 5.00 dog 157 mac ## 6 6.00 dog 152 windows Note that you can rename the column to anything you like. But, for consistency, I like to keep the same name as the variable which acts as the data source. Finally, we can remove the new column (and any column) by setting the entire column to nothing (NULL), like so: IQ_data$operating_system &lt;- NULL # remove new column head(IQ_data) # view first 6 rows ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.00 cat 147 ## 2 2.00 cat 160 ## 3 3.00 dog 185 ## 4 4.00 cat 122 ## 5 5.00 dog 157 ## 6 6.00 dog 152 Now the data is back to its original format. We’ll look at another way to remove one or several columns from your data frame in Session 4, but for now this a quick way to get things done using base R. 1.5.0.3.3 Adding and Removing Rows What if we want to add a new row to our data? This may be less common than adding a new column for data processing purposes, but it’s good to know anyway. First, we need to know what should go in each cell. Remember that we have to keep the data square, so you can’t have missing values when you add a row. If you don’t have any data, you can just put NA (with no quotations) to keep the data square but to show that you don’t have any value for a given cell. Let’s assume we want to add a new participant, 101, who has a dog but an unknown IQ. IQ_data[101, ] &lt;- c(101, &quot;dog&quot;, NA) # add new row with values tail(IQ_data) # see last 6 rows ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 96 cat 148 ## 2 97 dog 178 ## 3 98 dog 156 ## 4 99 cat 165 ## 5 100 dog 150 ## 6 101 dog &lt;NA&gt; Here, we have to define all our values to be added in parentheses, using the c() function: participant number is 101 pet_id is &quot;dog&quot; IQ is NA (i.e. unknown) We then assign this to our data frame using the assignment operator (&lt;-). We have to tell R where these values should go in our data frame. Because we’re adding a new row, we specify the data frame, with row 101, and all columns (remember, an empty value after the comma = all columns). We can then remove this row again setting our data frame to itself, minus the 101st row: IQ_data &lt;- IQ_data[-101, ] # remove the new row tail(IQ_data) # see last 6 rows ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 95 dog 188 ## 2 96 cat 148 ## 3 97 dog 178 ## 4 98 dog 156 ## 5 99 cat 165 ## 6 100 dog 150 This is a pain as it isn’t consistent with removing the rows. But, in Session 4 we’ll look at removing rows in a consistent way to removing columns. Still, it’s useful to know how this process works. 1.5.0.4 Other Data Types For most purposes, you’ll only need vectors and data frames/tibbles when you want to work with your data. But it’s worth being aware of other data types, such as lists and matrices. With lists, you can define a vector that contains other objects. Think of this as nesting vectors within vectors (very meta). person_quality &lt;- list(glenn = c(&quot;handsome&quot;, &quot;smart&quot;, &quot;modest&quot;), not_glenn = c(&quot;less_handsome&quot;, &quot;less_smart&quot;, &quot;less_modest&quot;) ) We can then access the elements of a list similarly to how we access vectors, but the name associated with the list will also be returned: person_quality[1] ## $glenn ## [1] &quot;handsome&quot; &quot;smart&quot; &quot;modest&quot; If, instead, we just want the values associated with that entry, we can use [[your list name]]: person_quality[[1]] ## [1] &quot;handsome&quot; &quot;smart&quot; &quot;modest&quot; We can edit these values in a similar way to how we did with a data frame. This time, we just need to tell R which vector to access first (here, it’s the first vector, using double brackets so we can access the values stored there, and not the name (as would happen if we just had 1 bracket)), and then specify which location in the vector you’d like to assign a new value using a separate bracket here: person_quality[[1]][4] &lt;- &quot;liar&quot; person_quality[1] ## $glenn ## [1] &quot;handsome&quot; &quot;smart&quot; &quot;modest&quot; &quot;liar&quot; An advantage to using lists over data frames or tibbles is that the data need not be square. That is, you can have uneven lengths for your entries. Notice how glenn and not_glenn have different numbers of elements in the list. With a data frame, this is problematic. Let’s try adding another participant number to our IQ_data tibble. IQ_data[101, 1] &lt;- 101 tail(IQ_data) ## # A tibble: 6 x 3 ## participant_number pet_id IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 96 cat 148 ## 2 97 dog 178 ## 3 98 dog 156 ## 4 99 cat 165 ## 5 100 dog 150 ## 6 101 &lt;NA&gt; &lt;NA&gt; Did you notice that R automatically introduced NA values in the final cells for the pet_id and IQ columns? Matrices work very similarly to data frames and tibbles, but they’re even stricter. They can only contain the same data type throughout, so we can’t mix columns containing characters and numbers without converting them all to the same data type (hint: it’s a character!). I’ll show you how to make a matrix, but we won’t linger on that as I haven’t found them all that useful in my own work. matrix_example &lt;- matrix(rep(1: 25), nrow = 5, ncol = 5 ) matrix_example ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 1.6 Good Practice 1.6.1 Data Checking Tips Finally, a few tips on checking your data before you manipulate your data: If you’re unsure what you have in your working directory, use either check your environment (the panel in light blue in the RStudio screenshot above) or type ls() in the console. This will list everything currently in the global environment. If you want to know the data class for some object, use the class() function (e.g. class(IQ_data)). If you want to know the structure (including object classes) for some object, use the str() function (e.g. str(IQ_data). Nicely, str() also tells you how many variables are in the object, and how many observations you have in total. Most importantly with anything in R, if you aren’t sure how a function works, or what it’s done to your data, check how that function works! You can do this with any function by typing ? followed by the function name into the console (e.g. ?str()). 1.6.2 Style I strongly recommend that you choose a style guide and stick to it throughout when you write your R code. This will make it easier to notice any errors in your code, and increases readability for you and others. Consistency is key here. Since we’re using a tidyverse first approach to teaching R in this course, I recommend this one by the creator of the tidyverse: R Style Guide by Hadley Wickham The important things to take home are that: Use sensible variable names: if a column shows, e.g. participant weight, call it participant_weight Use verbs to describe user-defined functions: if you write a function to make all the descriptive statistics you could ever want, call it something like make_descriptives() use a consistent style, like snake_case_here, or even camelCase, but don’t mix_snake_and_camelCase. comment your code with descriptions of why you’ve done something: you can often work out how you did it by following your code, but the why is easily lost! 1.7 Exercises Try out the exercises below, we’ll cover these in the class with the solutions uploaded at the beginning of each session in a separate downloadable link. Try to solve these questions before resorting to the solutions. I’ll be there to help out where necessary. First, I want you to figure out why some code doesn’t work, then we’ll move on to you manipulating data. For the opener, we’ll look at the basics of solving issues with your code. What’s wrong with the following code? # create a sequence from 0 to 40 in intervals of 2 sequence &lt; - seq(from = 0, to = 40, by = 2) Why doesn’t this return anything? # draw 100 times from a uniform distribution between 1 and 10 uniform_distribution &lt;- runif(n = 100, min = 1, max = 10) uniform_distrbiution Now, we’ll work with a data set. But I’d like you to produce it. We’ll break this down into steps for now. Lets pretend we have 100 participants. Create a variable that will store participant IDs. Their IDs can be anything you like, but some sort of logical numbering looks best. Hint: seq() is your friend here! Create a variable that will store their scores on some test. Let’s assume participant scores are drawn from a normal distribution with a mean of 10 and an SD of 0.75. Hint: rnorm() is a wonderful thing. Finally, create some ages for your participants. Here, you’ll have to use a new command called sample(). See if you can figure out how to use this command. If you’re lost, remember to use ? on the function. Create a data frame consisting of your participant IDs, their ages, and test scores. Take a look at the start and the end of your data frame. Do the values look reasonable? (Don’t worry about outliers or missing values etc., we’ll check those in later classes.) Access the row (and all columns) for participant 20. What do the scores look like? Are they the same as the person sitting next to you? Why or why not, might this be the case? Access just the test score for participant 73. Output some simple descriptive statistics from your sample. We want to know: Mean age Mean score SD score (Hint: Use sd()) It may be useful to store these descriptives in another data frame for further use later on. Can you do that? Access all rows and columns where the test score is greater than 12. Hint: use quotations to define the column name, and perform a logical operation on this. Access all rows from the participant_number and IQ columns where the test score is greater than 13. Hint: use the c() function to select multiple columns. References "],
["data-visualisation-1.html", "Chapter 2 Data Visualisation 1 2.1 Getting Started 2.2 Exploring Different Geoms 2.3 Exercises", " Chapter 2 Data Visualisation 1 In this session we’ll look at data visualisation using the ggplot2 package (Wickham 2009) from the tidyverse (Wickham 2017). As with most R stats courses, we’re focusing on data visualisation early on as this allows you to get a good grasp of your data and any general patterns within those data prior running any inferential tests. We’ll follow Hadley Wickham’s approach in R for Data Science by getting you working on producing some pretty graphs from the outset to see how ggplot works. After that, we’ll look at how ggplot can handle several data types. Along the way, we’ll add some customisation to our graphs so you can see the flexibility of this package. 2.1 Getting Started First, we’ll load the packages necessary for this class. Nicely, ggplot2 is part of the tidyverse family, so we don’t need to load this separately to the other packages in our library. library(tidyverse) Sorry to have made you create your own data frames before, but R and it’s packages often come with in-built data sets. We’ll use the starwars data set from dplyr() which loaded with the tidyverse package. Why star wars? It’s convenient, and I’m a big nerd, so indulge me. Because this is built into R, you won’t see it in your Data pane in the Global Environment. That doesn’t matter for us, but rest assured it is there. Let’s get a sense of how this data looks. How about printing the first 10 entries? 2.1.1 The Star Wars Tibble # look at first 10 entries starwars ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Sk~ 172 77.0 blond fair blue 19.0 male ## 2 C-3PO 167 75.0 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32.0 &lt;NA&gt; white, bl~ red 33.0 &lt;NA&gt; ## 4 Darth V~ 202 136 none white yellow 41.9 male ## 5 Leia Or~ 150 49.0 brown light brown 19.0 female ## 6 Owen La~ 178 120 brown, gr~ light blue 52.0 male ## 7 Beru Wh~ 165 75.0 brown light blue 47.0 female ## 8 R5-D4 97 32.0 &lt;NA&gt; white, red red NA &lt;NA&gt; ## 9 Biggs D~ 183 84.0 black light brown 24.0 male ## 10 Obi-Wan~ 182 77.0 auburn, w~ fair blue-gray 57.0 male ## # ... with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; I know that the starwars data set is saved as a tibble. That allowed me to just print its name to see the first 10 entries. But be wary of this with large data sets where you don’t know how it’s stored. You don’t want to flood your console if your data is stored as a data.frame! Let’s plot the mass and height of our characters against each other to see if there’s a trend. 2.1.2 Plotting in ggplot2 ggplot(data = starwars) + geom_point(mapping = aes(x = mass, y = height)) ## Warning: Removed 28 rows containing missing values (geom_point). So, we can see just how easy it is to create a plot of points in ggplot. Well done! There seems to be a positive relationship between mass and height in the starwars data set. We also got a couple of surprises: (1) a warning about 28 rows that contain missing values, and (2) a really big outlier. First, we’ll explore how ggplot works, then we’ll look into these surprises. The ggplot() function always needs to take a data set. This data set should hold everything you want to plot. Crucially, ggplot builds up the plots in layers. So making the first call ggplot(data = starwars) tells ggplot where it should look for data, but it doesn’t do much else aside from making a grey background. After this, you need to add some layers to your plot in the form of geometric objects (or geoms, for short). We decided that because we want to look at the link between mass and height, two continuous variables, that adding some points to the plot will be most useful for getting an idea of how the data are related. To do this, we used the geom_point() function. There are other geoms we could add, but for now we’ll focus on points. Crucially, geom functions take as an argument the mapping in your data. That is, how the visuals of the plot are mapped to your data. This mapping is always defined in terms of the aesthetics of your plot aes(), e.g. which variables to map onto the x and y axis, in this case. You can see how this makes ggplot so flexible: Your data argument is flexible, so you can pass different data sets to the same chunk of code by changing out what you pass to the data = argument. Your aesthetics are flexible, so you can pass different columns to your x and y axis Your aesthetics are even more flexible because they can take aesthetics other than just what to plot on the x and y axes Let’s see the flexibility of your aesthetics in action 2.1.3 Cleaning Before Plotting We saw before that R gave us a warning that we have rows containing missing values. In this instance, this just means that 28 people weren’t plotted because they didn’t have height and/or mass values. I’ll save filtering data properly (and what the %&gt;% (read pipe) symbol does) for another lesson, but we’ll get rid of them for now by running this code, below: filtered_starwars &lt;- starwars %&gt;% drop_na(height, mass) 2.1.4 Changing Your Aesthetics Let’s say we’re interested in showing the relationship between mass and height within genders. How can we do this? One way would be to add colour to our the points on the plot in order to highlight the different genders. ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ) ) We don’t get that warning now that we removed the NAs and passed the filtered data as an argument. That wasn’t really necessary as ggplot can handle that for us, but it stops the horrible warning from popping up! Warnings in R are there to tell you that the output of your code may not contain what you wanted it to contain. In the previous plot, ggplot dropped those with missing heights and masses, even though we didn’t explictly tell it to do so. Here, because we filtered those entried with missing values before plotting, we don’t get a warning. Warnings are different to errors in that your code will still work, but you need to check out whether it did what you wanted it to do. On the other hand, errors indicate you did something wrong and your code will fail to run. Now, we now have colour to see the relationship between mass and height across the genders in the data set. But it’s a little difficult to see this relationship given the outlier. Let’s see what that outlier is, and whether we should remove it from our data. Again, you’ll learn how this filtering works in later lessons, for now, I just want to show you how it’s useful to understand your data prior to creating a final plot of your data. filtered_starwars %&gt;% filter(mass &gt; 1000) ## # A tibble: 1 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jabba D~ 175 1358 &lt;NA&gt; green-tan,~ orange 600 herma~ ## # ... with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # overwrite data to remove outlier (keep all with mass below 1000) # filtered_starwars &lt;- filtered_starwars %&gt;% filter(mass &lt; 1000) # not run Of course, it’s Jabba the Hutt. We could choose to throw Jabba out of our data by using the code above to overwrite the data (commented out), but for now, we’ll see just how powerful ggplot is without throwing away our data. ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ) ) + coord_cartesian(xlim = c(0, 180)) This time we added a function coord_cartesian() to the end of our ggplot() call. We defined the limits of the x axis to be between 0 and 180. This way, we can get a better look at the trends in our data. Why do we define limits inside this function? Well, we could have also manually defined the scale with scale_x_continuous(limits = c(0, 180)). This may seem more inuitive, but it throws out the data points outside the limits prior to plotting. Why is this a problem? ggplot has some nice functionalities such as drawing lines of best fit for you based on the data in the plot. If you throw data away while plotting, your line of best fit will shift. So, if you decide that you want to change your scale but keep your model fits for all data, use coord_cartesian(). Given this is an outlier, your choice doesn’t matter if you’re just trying to show trends like this, but your choice is important if you want to show any inferential statistics associated with the data. Next up, we’ll look at changing a few components of the points on the plot. ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ), alpha = 0.7, # opacity shape = 17, # triangles size = 4) + # bigger points coord_cartesian(xlim = c(0, 180)) We’ve added 3 variables outside of the aes() mappings. This means that all of the points within the plot are changed in the same way: They all become a little bit transparent through the alpha variable definition; they all take a triangle shape through the shape variable definition (and the number associated), and they are all the same size due to the size variable definition. If we put these variables within the aes() mappings and associated them with a variable within the data set, such as gender, then each point would be affected differently depending on which level of the gender factor the individual data points belong to. It’s important to remember that everything within aes() is mapped onto variables with which to display your data. So, the x location, y location, and anything else that you define within aes() can vary by your data set. Everything outside of it will affect all levels of your data. Here, we’ll define colour both within and outside the aes() mapping, causing a clash. Try this plot below to see how clashes are resolved in ggplot: ggplot(data = filtered_starwars) + geom_point(mapping = aes(x = mass, y = height, colour = gender ), colour = &quot;red&quot;) + coord_cartesian(xlim = c(0, 180)) The different colours for each level of gender are now gone, along with the legend! Our variable definition for colour outside of the aes() mappings overrides that within aes(). This is because we’ve manually set the aesthetic properties of our plot by defining colour as an argument of the geom function, rather than of the aethetic mapping. R has some built in shapes that we can define within our plots. For shapes, these are divided into three categories: Colour border with hollow fill (0 - 14) Colourless border with colour fill (15 - 18) Colour border with colour fill (21 - 24) Bear in mind that colour and fill are different properties that we can control within our plots. Below, I’ve used hex values to specify the exact colours that I’d like for the colour (around the border) and the fill for our points. You can find a nice hex selector at htmlcolorcodes.com which will allow you to customise your plot colours to your liking. Just change the letters and numbers after the # in the colour call, and you can change the colours to your liking. In the example below I chose my colours based on the diverging colourblind safe selection of colours from Color Brewer. I’d recommend that you use this if you’re going to include colour in any plots for papers/presentations. ggplot(data = filtered_starwars, na.rm = T) + geom_point(mapping = aes(x = mass, y = height, colour = gender ), colour = &quot;#af8dc3&quot;, fill = &quot;#7fbf7b&quot;, shape = 21, size = 8, stroke = 3 ) + coord_cartesian(xlim = c(0, 180)) Try to mess about with the different definitions that we provided above. Change the colour, size, and stroke values to see how these variables work. We’ve used the aesthetics above to define categorical data by colour. But what happens if we use continuous data? ggplot(data = filtered_starwars, na.rm = T) + geom_point(mapping = aes(x = mass, y = height, colour = birth_year ) ) + coord_cartesian(xlim = c(0, 180)) You can see that we get a sliding scale for the hue of the points. Pretty neat, but also quite difficult to get a real idea of where on the scale the points lie. There are many inbuilt plots that you can create with ggplot2 by mapping your data to different geoms. To get an idea of all of different types of geoms, type ??geom. This should give you an index of all of the geom types available to ggplot2 in your Help pane of RStudio. 2.2 Exploring Different Geoms You’ve already learned about geom_point() above, but now we’ll explore the other geom types. For this exploration, we’ll use a different data set with a limited number of groups for ease of visualisation. I’ve simulated some data for this exercise. You can find this data in the inputs folder for this lesson. (If you’re interested, you can also find the script for generating this data in the data_generation folder). Let’s load this data into R. Here, we use the function read_csv rather than the base R read.csv. This is because read_csv is faster, typically does a better job at guessing the data types for your columns of data, and saves the output as a tibble. Note: read_csv will try to guess the types of data in each column for a data set loaded into R, but sometimes it can fail. We’ll cover instances where this fails, and how to remedy it, in the next lesson. rt_data &lt;- read_csv(&quot;inputs/rt_data.csv&quot;) How should we interpret this data? Imagine that we ran a lexical decision task on two groups of participants. In one condition, participants responded to sentences such as “Eric turned down the volume”, and had to indicate whether this sentence made sense by turning a knob. In the match condition, agreement matched the motion indicated in the sentence (e.g. left = yes, it makes sense), and in the mismatch condition agreement did not match the motion indicated in the sentence (e.g. right = yes, it makes sense). Zwaan and Taylor (2006, Experiment 4) hypothesised that when the action and response are matched, resposne times should be quicker than if they do not match. The participant column indicates individual participants, the gender column indicates the gender of our participants, response_condition indicates whether participants took part in the match or mismatch conditions, and reaction_time, our dependent variable, represents the average reaction time in milliseconds for each participant to respond to whether or not the sentences made sense. 2.2.1 Bar Plots Bar plots are one of the most common plots you’ll come across in Psychology. They’re most useful for representing counts of data that are divided into categories. For example, you could use a bar plot to show the number of male and female participants in your study. Using our simulated data set, we can use a bar plot to show the counts of males and females in our data set. ggplot(data = rt_data, mapping = aes(x = gender) ) + geom_bar() You’ll often find that psychologists like to plot continuous data in a bar plot. I’ve done this myself, but we can do better. One alternative would be to use a boxplot, a violin plot, or even a pirate plot (well, we are plotting with Rrrrrrrrr…). Since we’ve got data which represents mean scores for each participant, this will be suitable for plotting straight away. If, however, you have raw data, remember to aggregate by participants or items prior to plotting. We’ll cover this in later classes. 2.2.2 Box Plots Box plots provide a better way to represent continuous outcomes (such as reaction times) than bar plots as they give you more information about the variance within your data set. 2.2.2.1 How to read a box plot The middle line represents the median The upper white section of the box the upper quartile: 75% of scores fall below this. The lower white section the lower quartile: 25% of scores fall below this. Together the quartiles represent the interquartile range: The middle 50% of scores. The limits of the whiskers (black lines) for the upper and lower parts of the graph represent the smallest and largest observations that are equal to the upper or lower quartiles minus or plus 1.5 times the interquartile range. Effectively, this is most of the rest of the data, apart from outliers. The dots represent outliers (i.e. those values outside of the whiskers) ggplot(data = rt_data, mapping = aes(x = response_condition, y = reaction_time ) ) + geom_boxplot() We can see that the median reaction time is highest for those in the mismatch condition. Notice also that the interquartile range is larger, and so is the upper limit of the whiskers. This all suggests that those in the mismatch group vary more from each other than those in the match condition. Also, notice the outlier in the match group. Someone obviously has a very slow reaction time compared to the rest of the cohort. I wonder why that could be? Perhaps it’s because they misinterpreted the instructions, accidentally putting themselves into the mismatch condition. Or, it could be because I simulated this data and didn’t control for how outliers were distributed. 2.2.3 Violin Plots Additionally, we have violin plots which show you the density of the mean scores. The wider the section of the violin, the more scores around that area. We set trim to FALSE within the violin plot so that we see the full tails of the data. If we set this to TRUE, then the tails are trimmed to the range of the data. It’s also useful to draw quantiles on the violin plot, so we can see the quantiles as with the box plot using draw_quantiles and by specifying where we want these quantiles. Here, we chose the upper and lower 25% and the interquartile range. ggplot(data = rt_data, mapping = aes(x = response_condition, y = reaction_time ) ) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75) ) 2.2.4 Density Plots Now we’ll look at ways to check for the distribution of your continuous variables. This is a good way to get an eye for whether or not your data are skewed and require a transformation on the data, or a non-parametric test when it comes to inferential statistics. Again, we’ll return to what this means in regards to statistics later in this course, but for now it’s a good idea to understand how you can check the distribution of your data prior to computing any statistics. Again, only the first two lines here are necessary to make the plot. The additional lines change the axis labels, set the limits and break points on the x-axis label, change the title of the legend, and set the theme to black and white. ggplot(data = rt_data, mapping = aes(x = reaction_time, fill = response_condition ) ) + geom_density(alpha = 0.5) # alpha = opacity Did you notice how now that we have reaction time plotted on the x-axis (vs. the y-axis in the pirate plot), we had to change to using scale_x_continuous() and changing the limits and breaks within it? Also, we changed the title of our legend to allow for capitalisation and to remove the underscore between words by using guides(fill = guide_legend(title = &quot;Response Condition&quot;)). 2.2.5 Histograms If you have relatively few observations, a histogram may be more appropriate than a density plot. But, we’ll just stick to the same data as before to see how this works. Here, it’s often useful to set the binwidth prior to plotting. ggplot will try to set it to a default if it can, but this may not be what you want. Here, we’ve set the binwidth to 50, so we count how many observations we have for reaction times in 50ms bins, i.e. from 225 to 275ms, from 275 to 325ms etc. ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;white&quot;, colour = &quot;black&quot; ) 2.3 Exercises Try out the exercises below, we’ll cover these in the class with the solutions uploaded at the beginning of each session in a separate downloadable link. Try to solve these questions before resorting to the solutions. I’ll be there to help out where necessary. In this section, we’ll use some new data from the pre-packages data sets in R. First off, we’ll load the chickwts data set, which looks at chicken weights by different feed types. data(chickwts) 2.3.1 Main Exercises Take a look at the first 6 rows of the data set. How does the data look? Is this appopriate for plotting? Calculate the overall means for the chick weights Calculate the overall SD for the chick weights Create the basis of a ggplot by defining the chickwts data as the data argument. Assign this all to the object chick_plot. Make a box plot of the chick weights by feed. To do this, use your chick_plot object and assign it the function for creating a boxplot. Add colour to your box plot by the feed type. Set the colour to anything you’d like. Remember, you can use the Color Brewer website to find some nice colours using hex values. Create a density distribution of the chick weights by feed type. Set different colours by the feed type. To make all densities visible, set the transparency for all distributions to 0.6. Make a bar plot to show the counts of each feed type. Pick 6 hex colours from the Colour Brewer website. Put these together in a variable, bar_colours. Create your bar plot again, but this time make the fill of the bars the same as those stored in bar_colours. Why do we not get a legend when we specify colours this way, but we do if we specify colours as in Question 7? Make a histogram showing the overall distribution of the chick weights. Set the bin width to 50, and make the bars have a white fill and black border. 2.3.2 Additional Exercise Make a point plot from a data set of your choosing. Check out the inbuilt data sets in R by typing data() in the console. Customise this plot to your liking. References "],
["data-visualisation-2.html", "Chapter 3 Data Visualisation 2 3.1 Customising Your Plots 3.2 Pirate Plots 3.3 Faceting 3.4 Calculating Statisitcs in ggplot2 3.5 Combining Plots 3.6 Saving Plots 3.7 Exercises", " Chapter 3 Data Visualisation 2 In this section we’ll cover more advanced plotting methods in ggplot2. We’ll look at customising plots, making pirate plots, installing packages from GitHub, faceting, and stitching plots together. To get started, as always we’ll load our packages and saved data. library(tidyverse) rt_data &lt;- read_csv(&quot;inputs/rt_data.csv&quot;) 3.1 Customising Your Plots Take the desnity plot below, this is functional, but it’s pretty ugly. ggplot(data = rt_data, mapping = aes(x = reaction_time, fill = response_condition ) ) + geom_density(alpha = 0.5) You already know how to change the colours of your aesthetics across all conditions and split by condition, so now we’ll look at other ways to improve the plot. In this version of the plot, we use the same code as before, but we add labels to the axes using labs(), assigning a nicer looking version of our variables to the x and y axes. Additionally, we change the scale of the x-axis using the scale_x_continuous function. To this function, we pass the limits that we want for our axis (between 200 and 600ms), and we identify the breaks, or where we want the ticks along the axis. We pass another function that we learned in the Lesson One, called seq(). This sets up a sequence of numbers for us without us having to type them all out. Here, it goes from 200 to 600 by ticks every 100ms; as a result, our axis gets labels of 200, 300, 400, 500, and 600. On top of this, we also improved the label for our legend using the guides() function. As our legend is only there to identify different parts of the graph with a different colour (from fill = response_condition in aes()), then we have to tell the guide to change the legend that pops up because of the differnt coloured parts of the plot. So, we change our guide, and change the legend that comes up because of the change in colour (guides(fill = guide_legend())) and within the guide legend, we change the title (title = &quot;Response Condition&quot;. I know this sounds like a lot to take in, and you’re very likely to forget how this works (I do all the time), but hopefully you can get a grip of it by seeing it in action. Finally, we’ve changed the theme of our plot to theme_bw(). This is one of many inbuilt themes in ggplot2, but I find it one of the cleanest. ggplot(data = rt_data, mapping = aes(x = reaction_time, fill = response_condition ) ) + geom_density(alpha = 0.5) + labs(x = &quot;Reaction Time (ms)&quot;, y = &quot;Density&quot; ) + scale_x_continuous(limits = c(200, 600), breaks = seq(from = 200, to = 600, by = 100 ) ) + guides(fill = guide_legend(title = &quot;Response Condition&quot;)) + theme_bw() With the histrogram below, we don’t have much new to introduce, expect this time we use theme_classic() instead of theme_bw(). This gets rid of the major and minor grid lines from the previous plot, and also keeps only the lines for the axes. However, we added the argument expand to the scale_y_continuous() function, and pass this the values 0 and 0. This makes removes the spacing between the plotted elements and the axes. These values simply indicate how much extra space should be added to the top and bottom of the plot. ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;#bcbddc&quot;, colour = &quot;#756bb1&quot; ) + scale_x_continuous(limits = c(200, 600), breaks = seq(from = 200, to = 600, by = 25 ) ) + scale_y_continuous(expand = c(0, 0)) + labs(x = &quot;Reaction Time (ms)&quot;, y = &quot;Count&quot;) + theme_classic() 3.2 Pirate Plots Now we’ll look at a new plot type that takes a bit of extra work to generate; pirate plots. Pirate plots are a great choice of plot for the indecisive. They are essentially the individual points of data, a bar plot, a violin plot, and a confidence interval interval all in one. This way, you get to see the raw, descriptive, and inferential data on one plot! This is a nice way to show data that are grouped by categories but with a continuous dependent variable. We could make these ourselves by doing some calculations and combining geoms in ggplot2. Or, we could just install a package from Github which will do that for us. First, we’ll have to install devtools in R which will allow us to install packages from Github. (For packages on CRAN, this isn’t necessary, but unfortunately ggpirate isn’t on CRAN at the time of writing.) To install devtools and the ggpirate package, uncomment and run the code below. Then as always, use library(ggpirate) to load the package. # install.packages(&quot;devtools&quot;) # devtools::install_github(&quot;mikabr/ggpirate&quot;) library(ggpirate) Below, we’ll make a pirate plot. Note that you only need the first two calls (lines 1-6) to create the plot: The first to set up how you’re going to map your data (and the source of your data), and the second to add the geom for the pirate plot itself. We added the aethetics of colour and fill to match our conditions within this call, so the two levels of response_condition have different colours. For the additional lines: labs allows you to manually change the x and y axis labels scale_x_discrete allows you to manipulate your x scale. Within this, we change the labels of the columns using labels. We changed them to Impaired (vs. impaired) and Intact (vs. intact) for stylisitc reasons. scale_y_continuous allows you to manipulate your y scale. Here, we set the limits, i.e. how far to extend our y-axis to between 0 and 600ms. Additionally, we set our breaks to increment in the sequence (seq) from 0 to 600 by 100ms. This way, we’ve added more granularity to our axis ticks. We use theme_bw() to change to a black and white theme. There are other themes in ggplot2, but this one is nice and clean. Finally, theme allows you to manually specify other aspects of how your plot should look. Here, we used panel.grid.major.x and set this to nothing (element_blank()) because we don’t need vertical lines in our plot. ggplot(data = rt_data, mapping = aes(x = response_condition, y = reaction_time) ) + geom_pirate(aes(colour = response_condition, fill = response_condition) ) + labs(x = &quot;Motor Skill&quot;, y = &quot;Reaction Time (ms)&quot;) + scale_x_discrete(labels = c(&quot;Impaired&quot;, &quot;Intact&quot;)) + scale_y_continuous(limits = c(0, 600), breaks = seq(from = 0, to = 600, by = 100) ) + theme_bw() + theme(panel.grid.major.x = element_blank()) In this plot, we have coloured bars and lines indicating the mean scores, a box representing the 95% confidence interval assuming a normal sampling distribution, violins indicating density, and the raw data points. If the 95% confidence interval between the two groups doesn’t overlap, we can be fairly sure there is a significant difference between the groups. So, here we can be fairly certain the two groups differ in reaction times. 3.3 Faceting Another useful part of plotting in ggplot2 is that you can make facets of plots, or subplots. This is a good way to display your data if you have multiple categorical variables. Essentially, you’ll get a plot for each category in your data. 3.3.1 Facet Wrap If you want to create facets from one variable then use facet_wrap(). ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;white&quot;, colour = &quot;black&quot; ) + facet_wrap(~ response_condition) In this plot, we’ve specified a histogram as we normally would. However, we use facet_wrap() and a tilde (~) to create a formula for how to display our plots. We define our variable with which to split our plots to the right of the ~, and ggplot automatically plots the two separately at the same time. Notice that we get useful labels at the top of these plots, too. 3.3.2 Facet Grid If we wanted to make a facet by two variables, then we would use facet_grid() instead. In this case, we just add each variable to either side of the ~ and ggplot will do the splitting for us. Let’s see how this works if we split our data by gender and response condition. ggplot(data = rt_data, mapping = aes(x = reaction_time)) + geom_histogram(binwidth = 50, fill = &quot;white&quot;, colour = &quot;black&quot; ) + facet_grid(gender ~ response_condition) The order in which you specify the two variables matters. Try swapping around between facet_wrap(gender ~ response_condition) and facet_wrap(response_condition ~ gender) to see how this works. 3.4 Calculating Statisitcs in ggplot2 Sometimes, plotting just the means isn’t enough. We saw how useful the 95% confidence interval from ggpirate is for making inferences about the differences between groups. Nicely, we can get standard errors or confidence intervals around our data points within ggplot for other geoms. 3.4.1 Means and Error Bars Let’s say you wanted a bar plot with error bars showing the standard error. You can create this in ggplot using the stat_summary() function. In the first instance here, we tell it that we want to run the function mean over our data that make up the y-axis; hence fun.y = mean. We also need to specify which geom we want to return from this. Try changing the geom to point in the first stat_summary() call to see what happens when you run this plot with geom = &quot;point&quot;. Finally, we ask for another summary, but this time we want an error bar. So, for the geom call we request an error bar. Crucially, the function we require to get this errorbar is fun.data = &quot;mean_se&quot;. That’s because we need the mean to know where the centre the errorbar, and the standard error to get the limits of the bar. We manually changed the width of the bar to a quarter of the bar size using the width argument to stop ggplot returning super wide error bars. ggplot(data = rt_data, mapping = aes( x = response_condition, y = reaction_time, fill = response_condition ) ) + stat_summary(fun.y = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, width = 0.25) I don’t often use stat_summary in my own plots, as I often want to know exactly how I’ve calculated things like the standard error. Doing things by hand allows you to change error bar sizes appropriately for within- and between-subject designs. But, this is useful if you want to create a plot rapidly, or want to avoid the hassle of writing extra code. 3.4.2 Model Fits Here’ we’ll switch again to a different data set that has two continuous variables. The starwars data set is useful for this exercise. We can use the geom_smooth() function to fit a model to our data. This defaults to a loess method, but we can change this to a linear model (or other alternatives) as in the second plot below. By default, these smoothed plots display a ribbon around the fit which indicates the confidence interval (95% by default). # remove NA and mass above 1000 filtered_starwars &lt;- starwars %&gt;% drop_na(height, mass) %&gt;% filter(mass &lt; 1000) # plot ggplot(data = filtered_starwars, mapping = aes(x = mass, y = height)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; Next, we’ll change the defaults in order to fit a linear model. We do this in the geom_smooth function, method = &quot;lm&quot;. We can additionally specify a function, so we can change how we fit the data. Currently, our formula is y ~ x, which is a regular linear model. We could, however, fit a quadratic function to the data by using y ~ poly(x, 2). The function poly(x, 2) calculates orthogonal polynomials to a certain degree. The first being linear, second quadratic (think curved lines with 1 inflection point), the third cubic (think curved lines with 2 inflection points), etc.. For now, we’ll just fit the linear model. ggplot(data = filtered_starwars, mapping = aes(x = mass, y = height)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = y ~ x) Alternatively, if we have a fitted model saved from our analyses, we can add the fitted line directly using the stat_summary function. However, this requires some insight into the models fitted, so we’ll save this for later classes. Finally, we’ll look at how we can combine these smoothed fits with averages of scores. This is often useful if you’re looking at timecourse data and you want to summarise performance across a whole range of participants. We’ll also look at adding a few of the graphical flourishes from above to make the plot look really nice. Next, we’ll look at making continuous data look a little better. For this, we’ll use another inbuilt data set from R, the Orange data set which shows the growth of orange trees by circumference (don’t ask). The reason I’ve chosen this data set is that, like INPUT CONTINUOUS DATA # code to go here 3.5 Combining Plots Finally, while it’s all well and good plotting one model at a time, often if you’re writing a paper you want several plots together to save space. You could do this by hand in certain image editing packages, but luckily there’s a package for this on CRAN; cowplot. First, we need to install and load the package. Intall this by running the commented out code below. After that, load cowplot each time you want to use it. So far, we’ve just been running the code to produce plots, but we haven’t saved them in our environment. But, in order to stich two plots together, we need to save our plots as objects so they’re available to the R environment. If we assign our plotting code to an object, then every time we run the name of the object, we’ll get the output of that code back. For keeping several plots in my environemnt at once, I often save them to an object. In this instance, we’ll save a plot of just the points to the name point_plot. That’s because we want to overlay a linear fit and a quadratic fit on this plot separately, but we don’t want to type the full code out twice. Instead, we fit the plot of points to point_plot, then add a linear or quadratic fit by adding the geom_smooth argument to point_plot and saving both under new names. We can do this like so: # create a plot of points point_plot &lt;- ggplot(data = filtered_starwars, mapping = aes(x = mass, y = height) ) + geom_point() # create a linear plot by adding the fit to the plot of points linear_plot &lt;- point_plot + geom_smooth(method = &#39;lm&#39;, formula = y ~ x) When you do this, you won’t automatically see the plot once you run the code unless you run the object assigned to the plot. Let’s try this for a quadratic fit of the same data. # create a quadratic plot by adding the fit to the plot of points quadratic_plot &lt;- point_plot + geom_smooth(method = &#39;lm&#39;, formula = y ~ poly(x, 2)) # fit quadratic # return the plot quadratic_plot You can see that we’ve got the quadratic fit and the 95% confidence interval around this fit from the above code. Why does the plot look different to the base plots in ggplot2? cowplot loads some defaults for all plots outputted by ggplot to save you on typing out your own theme arguments. Now, we can combine these two plots into a single plot using the new functionalities from cowplot. combined_plots &lt;- plot_grid(linear_plot, quadratic_plot, labels = c(&quot;A&quot;, &quot;B&quot;), # label plots align = &quot;h&quot; # align axes ) combined_plots What if we want a title for our plots? We first have to define our title with a combination of ggdraw() and draw_label(). Inside draw_label() we used a new function, paste() to paste two strings of text together (with a space between the two strings). We could simply input the entire string, but we broke it down into two bits so we don’t exceed the 80 character width limit from our style guide! We further specify a bold fontface in the draw_label() command outside our pasted title. Finally, we display the plot by creating a plot_grid() of our plot and the title in this order so the title displays underneath the plot. We specify that we just want 1 column so the plot and title are stacked together, and we specify the relative heights of the title and plot separately so that the title is smaller than the plot so it doesn’t take up an equal amount of space as the plot. # create title title &lt;- ggdraw() + draw_label(paste(&quot;Linear (A) and Quadratic&quot;, &quot;(B) fits of Height and Weight&quot; ), fontface = &quot;bold&quot; ) # print plot as a grid combined_plots &lt;- plot_grid(combined_plots, title, ncol = 1, rel_heights = c(1, 0.1) ) combined_plots You can find further information on adding joint titles, annotations, etc. in this article by Claus O. Wilke. 3.6 Saving Plots Finally, if you’ve went to the work of producing a nice plot, then it’ll be useful to know how to save it somewhere. To save our combined plot of the linear and quadratic fits, we’ll use ggsave(). You can name your plot anything you like, but remember to add a file extension at the end. I’ve used .png as this format suffers from fewer artifacts when comapred to JPEG, and it’s a pretty common filetype. ggsave(filename = &quot;outputs/starwars_mass_by_height.png&quot;, plot = combined_plots ) You can do a whole bunch of other things with ggplot, like adding vertical and horizontal lines (often useful for identifying chance performance in participants), and annotating plots directly (useful for adding statistics, or commenting on interesting sections of the data). We’ll cover these in the exercises, however, as these are just additional flavour to our plots! 3.7 Exercises Try out the exercises below, we’ll cover these in the class with the solutions uploaded at the beginning of each session in a separate downloadable link. Try to solve these questions before resorting to the solutions. I’ll be there to help out where necessary. "],
["data-manipulation.html", "Chapter 4 Data Manipulation", " Chapter 4 Data Manipulation How to get your data in an acceptable format. Content to follow. "],
["summarising-data.html", "Chapter 5 Summarising Data", " Chapter 5 Summarising Data How to calculate summaries from your data. Content to follow. "],
["traditional-tests.html", "Chapter 6 Traditional Tests", " Chapter 6 Traditional Tests Basic parametric and non-parametric analyses. Content to follow. "],
["simulation-and-calculating-power.html", "Chapter 7 Simulation and Calculating Power", " Chapter 7 Simulation and Calculating Power How to simualte data and perform power analyses. Content to follow. "],
["mixed-effects-models-1.html", "Chapter 8 Mixed Effects Models 1", " Chapter 8 Mixed Effects Models 1 Lesson One of Linear/Hierarchical Mixed Effects Modelling. Content to follow. "],
["mixed-effects-models-2.html", "Chapter 9 Mixed Effects Models 2", " Chapter 9 Mixed Effects Models 2 Basic non-parametric tests. Content to follow. "],
["creating-reproducible-documents.html", "Chapter 10 Creating Reproducible Documents 10.1 The Final Lesson", " Chapter 10 Creating Reproducible Documents How to create Rmarkdown documents for reproducible documentation of analyses. Content to follow. 10.1 The Final Lesson Congratulations, you’ve made it! "],
["references.html", "References", " References "]
]
